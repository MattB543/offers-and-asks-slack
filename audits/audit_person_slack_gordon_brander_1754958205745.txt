=== Audit: Person Skill Rebuild ===
Generated at: 2025-08-12T00:23:25.746Z

-- Person --
user_id: slack_gordon_brander
slack_user_id: U098C1ZB1NU
display_name: Gordon Brander
enabled: true

-- Profile fields --
most_interested_in: AI Exocortexes: amplifying human intelligence, creativity, and decision-making with AI. Agency means staying in the OODA loop. To keep up with AI, we’ll need to augment ourselves with AI. Currently building Deep Future, an exocortex for thinking more effectively with AI about futures.
confusion: The proliferation of open models suggests a world inhabited by millions of AI agents, not one singular entity. Under these conditions, reasoning is less an individual activity, more of an emergent network phenomenon. How do we think together with AIs and egregores?
expertise: Scenario planning + futures / founding early-stage startups / product + design / agent sandboxing + privacy / decentralized protocols
projects: Deep Future - Deep Research but for scenario planning
asks: Intros to govt/policymakers/industry folks who would benefit from better foresight, better strategic analysis. My theory of change is to empower high-leverage decision-makers to make better decisions resulting in maximum deep benefit. gordonbrander@gmail.com +1 720-375-0022
offers: Scenario planning/foresight. Founding seed-stage startups / raising seed. App dev. Front-end. Product design.

-- Messages (up to 200, formatted with thread context) --
count: 52
- #lab-notes-ben-sklaroff: <@U093S356KAS> the cooperative license, high-level is that it's roughly MIT license for coops + non-profits, and LGPL for (non worker-coop) for-profits. So just a small bit of legal advantage for the worker coops: <https://github.com/bsklaroff/cooperative-license> -> Holy mackerel, that's amazing! -> I've been tracking the development of open source licenses that try to close the economic loop for a while. Cooperative License is one of the cooler riffs I've seen so far! -> Fair Source is another interesting one. Source-available, free for personal use and non-competing uses. Converts to Apache/MIT after 2 years. <https://fsl.software/> -> <@U098C1ZE2M6> oh wow, you're behind the Cooperative License?! Awesome!
- #lab-notes-oly: Had a thought about voting mechanisms. We all have our pet peeves with status quo typical voting mechanisms. They're terrible. But they're tenured. Maybe a lot of the resistance to change comes from a kind of fearful default conservatism. After all, if we shake up our voting system today, what's to stop it getting shaken up (badly) tomorrow? Norms and rituals in general get a kind of potency this way through the aging process, for good or ill. -> Probably a pace layers angle to this as well <https://jods.mitpress.mit.edu/pub/issue3-brand/release/2> -> Lindy practices get folded into lower, slower layers so they won't get lost. The slowness acts as long-term memory, preventing practices from getting lost during generational cycles. -> Francis Fukuyama has some great passages in The Origins of Political Order that hint at an evolutionary biological mechanism for this kind of conservatism -> Basically humans are mimetic animals... monkey see monkey do. -> and this bias toward imitation over innovation gives us a slightly conservative bent. Conservative as in conserving of the status quo. -> This is how institutions form. We copy what works, and keep copying it, and reproduce the institution. "Civilization advances by extending the number of important operations which we can perform without thinking about them." -> so this is cool bc imitation lets us quickly converge around Schelling points of behavior and benefit from the economies of scale of cooperation -> but then sometimes these practices fall out of evolutionary fitness vs the environment -> Fukuyama: &gt; There is something like a law of the conservation of institutions. Human beings are rule-following animals by nature; they are born to conform to the social norms they see around them, and they entrench those rules with often transcendent meaning and value. When the surrounding environment changes and new challenges arise, there is often a disjunction between existing institutions and present needs -> Your OP here made think of this recent article. It pushes me in the direction of "the reason we can't upgrade our voting systems is simple power dynamics; often times a large bloc believes they stand to lose from any changes in a near-zero-sum game. So when one party does achieve enough political momentum to just seize more power, then do. <https://www.nytimes.com/2025/07/30/us/politics/texas-republican-redistricting.html> -> Another thing people often bring up is simplicity or ease-of-understanding of a voting system as an important property for the perceived credibility of the system. If your voters don't understand how their votes are counted, they become suspicious.
- #lab-notes-gordon: Back in Nairobi :flag-ke:
- #lab-notes-gordon: doing a little moodboarding as pre-work for making a landing page <https://www.are.na/share/PvGGXHm>
- #lab-notes-gordon: getting inspired by amber/orange FUI and euroracks
- #lab-notes-gordon: I think the JTBD for Deep Future is to feel like you have 10x more leverage
- #lab-notes-gordon: Just-According-to-Keikaku-as-a-Service -> JAcKasS?
- #lab-notes-gordon: Another lens: AI Scenario Planning as an Engine-Building game -> 'Engine-building'? -> @Oly this post has a pretty good description: <https://interconnected.org/home/2022/02/11/wingspan> &gt; _Wingspan is what’s known among serious gamers as an “engine-building game,” which means that as the game goes on, the combination of birds you play becomes more and more efficient at generating points each turn, like an engine running faster and faster. Your cuckoo lays eggs, and the eggs not only give you points but make it possible to play more birds, which also give you more points but have their own powers that generate points in other ways._ -> I want to get away from scenario-planning-as-a-workshop-activity, and toward "live" AI-driven scenarios that are continually evolving and part of your OODA loop. -> So, maybe should feel like a scenario engine game. The more inputs you feed it, and the more actions you take, the better the scenario engine gets, and the better your inputs and actions become -> My understanding is that current scenario planning games are typically of a role playing variety, and that they are most useful when players are trying to faithfully model their targets, rather than truly optimizing against one another and the game itself. I’m not an expert here though. I’d love to hear from one but I’m not sure who to reach out to -> AI 2027 style wargames are just one practice that comes from the scenario planning world -> there are a bunch of others -> the area I'm personally more interested in has to do with structural analysis -> (I love engine building games; hadn't heard that term and had sometimes called them 'deck building' or similar) -> I like them too! I think what works about that game design is that you get this felt sense of the flywheel building up momentum, and you feel more and more agency as you bootstrap it -> Yep, and trying out different runs to find cool combos and interactions. Dominion comes to mind, also Machi Koro, others. -> aaaahhh I love machi koro -> I guess for 'realistic' scenarios, the cool combos and interactions might be informative of critical hingey dynamics or bottlenecks or something -> btw, how are you all up at... 4am pacific?? -> (Well, I just landed in London, don't know about Tamera) -> (Port Royal is a wonderful engine building game that sometimes gets a bit broken just at the end which is kinda fun) -> right, I think the engine analogy here might be discovering strategic leverage points and expanding your own agency as you identify ways to push on them -> That silica mine story feels like an almost-example. I'm not sure it's an actual leverage point, but assuming hypothetically for a second that it is... you could imagine your personal AI scenario system uncovering that bottleneck and its relationship to AI safety and surfacing that to you. You make that move, and now new scenarios open up because you control that chokepoint and have altered the driving forces around chip manufacture. -> Right... As a comparison point, there were those who 'discovered' the importance of compute before most of the world. They did that by... I don't think quite 'scenario planning'? But it feels pointed at the same thing as you're describing it. And there have been various corollaries and further implications of that, and we don't yet know how it'll shake out. -> I used to enjoy casual MTG drafts, so I can appreciate this vibe I imagine it could be used as a strategic tool as well, but to the extent that players are optimizing against the game, it needs to be reflecting many tiny little details of reality accurately in order to be giving them good info. Or at least, the game could try to present some information to calibrate the player about how broken it is at any given time -> &gt; (Well, I just landed in London, don’t know about Tamera) Just having a hard time sleeping :upside_down_face:
- #lab-notes-gordon: Another lens: ritual design <https://newsletter.squishy.computer/p/ritual-technology>. Our goal is to (1) generate strategic insights with AI (2) get those insights into your OODA loop. So we need to close the loop. Part of what we're doing is designing a daily/monthly/quarterly ritual. -> There are some pretty good ritual design tools here <https://ritualdesign.net/>
- #lab-notes-gordon: Orientation is the crux of the OODA loop: <https://newsletter.squishy.computer/p/tools-for-thought-in-your-ooda-loop> &gt; For Boyd, the most important factor for survival is your sense-making system. This is the thing that keeps you oriented. There’s a reason that “orient” is the biggest circle on the OODA diagram. We survive by making sense. When orientation breaks down—when you are _disoriented_—you lose your agency. Your model is wrong. You pay attention to the wrong things, your actions stop making sense, you flail. -> Chet Richards goes deeper into this: <https://www.agileleanhouse.com/lib/lib/Topics/OODALoop/Boyds_OODA_Loop_Its_Not_What_You_Think_I%20(1).pdf> -> I think I basically agree for individuals (humans at least, though it might be that the margins would change for quite different types of individual). But I feel compelled to observe that for orgs and other formal/informal groups, DA seem equally or more difficult and marginally important to improve. (Plus I get confused how to relate OODA to more long-term processes like learning and growth, either of individuals or groups.) -> haha having worked at Google, I can confirm that DA is indeed a challenge for large orgs -> IMO one way OODA gets folded into long-term growth is through updating the source code you use for orientation -> Orient = the long term memory of the system. The DNA. -> Tradition, team culture, organizational structure, values, tacit knowledge, shared language etc go here -> That sounds exactly right to me (except for the DNA analogy!), where D can also get learning improvements (perhaps especially for exploratory activities?) -> Oh I have all sorts of thoughts here. One thought is, if you can’t D or A, maybe you haven’t actually O’d? -> Or at least, your O isn’t entirely up to the task you’re facing right now -> &gt; if you can’t D or A, maybe you haven’t actually O’d? &gt; For a group, or individual? There's definitely something to co-orientation in a group. (If you include 'and therefore this is the decision-making and binding procedure we should use' then I guess it's approaching sufficient for DA as well but that seems to cut out a lot of solution space, to me.) -> &gt; For a group, or individual? Both? Orientation is a continual process - like Gordon said: &gt; Tradition, team culture, organizational structure, values, tacit knowledge, shared language etc go here Which are all iterated on continuously. Nobody should be choosing an absolutely final “decision-making and binding procedure” at any point, if you ask me - you’re just running an experiment to assess the quality of your orientation at this moment -> Which part of why I find stuff <https://aiforhumanreasoning.slack.com/archives/C0983TKV3RP/p1754024483403429?thread_ts=1753991440.654439&amp;cid=C0983TKV3RP|like this> so heinous - it seems like the whole organization loop has entirely fallen apart -> Another Boydian idea I'm obsessed with is "fast transients". OODA is not necessarily about looping faster, but about controlling the tempo of the game by creating plot twists: > In other words, the F-86 pilot could gain the initiative by executing _fast transients_ – changes in the tactical situation that forced his opponent to respond; or he could respond more quickly than his opponent could change things up. <https://theleanthinker.com/2013/08/19/fast-transients/> -> so like, for example, we focus a lot on the speed of AI takeoff (_FOOM_) -> but I think maybe it's worth pondering how little agency we have within any inhuman timescale -> inhumanly fast -> inhumanly slow (ai can be inhumanly patient) -> The timescale at which you can make sense is the timescale in which you have agency. -> and one reason I like scenario planning methods is that they encourage us to expand our sense of agency within time both backwards (identifying long-term forces) and forwards (larger map of the possible) -> I think I’m going to try to actually sleep now, but if you want to chat more at some point this is my google calendar link: <https://calendar.app.google/WvCJwS1ipVrTQG4i9> -> Earlier today <https://aiforhumanreasoning.slack.com/archives/C0983TKV3RP/p1754139544740969?thread_ts=1753991440.654439&amp;cid=C0983TKV3RP|I wrote up some conclusions> from my investigating / thinking about what’s going on with slack, in case you guys wanted to check it out.
- #lab-notes-gordon: Dreamy jetlag productivity radio <https://www.nts.live/shows/early-bird/episodes/early-bird-arushi-jain-18th-july-2025> -> This track is so good. International Anthem does not miss. <https://intlanthem.bandcamp.com/track/industry>
- #lab-notes-gordon: DARPA mention got me thinking about FLF as a DARPA for lightcone alignment. Original DARPA mission: "To create and prevent strategic surprise." -> Also Licklider's approach of "fund people not projects" and "fund people who would work on the problem whether or not they were funded" -> Book: The DARPA Model for Transformative Technologies <https://www.openbookpublishers.com/books/10.11647/obp.0184> -> and Ben Reinhardt's manual for creating more DARPAs <https://blog.benjaminreinhardt.com/parpa> -> “DARPA for lightcone alignment” is such a delicious phrase :heart: -> The prevent strategic surprise framing is such a classic—we put it into the UKAISI mission at launch. Other evocative orgs with interesting ways of operating: • <https://jeffburke.substack.com/p/sutter-hill-ventures-the-silent-builders|Sutter Hill Ventures> • <https://goodscience.substack.com/p/a-scrappy-complement-to-fros-building|BBN> -> this is such a better brand description I will see what I can do :saluting_face:
- #lab-notes-gordon: A meditation on creativity from the most creatively prolific person I know <https://shimmeringvoid.leaflet.pub/3lvf6cggrt22x/l-quote/0_0-0_336#0_0|https://shimmeringvoid.leaflet.pub/3lvf6cggrt22x/l-quote/0_0-0_336#0_0>
- #lab-notes-gordon: I like this... using AI agents as market intermediaries <https://open.substack.com/pub/meaningalignment/p/market-intermediaries-a-post-agi|https://open.substack.com/pub/meaningalignment/p/market-intermediaries-a-post-agi>
- #lab-notes-gordon: Bookmarking this to look at later
- #lab-notes-gordon: :shushing_face: domain registered: possible.systems -> also scooped up deepfuture.now
- #lab-notes-gordon: not committed yet, but I like the cheeky double-meaning
- #lab-notes-gordon: Noting as an example of the kind of *actionable* strategic insight that falls out of forecasting/scenario planning. -> An actionable insight produces meaningful strategic leverage. • can plan around it (A tier) • can influence outcome (S tier) -> “The best way to predict the future is to invent it” -Alan Kay
- #lab-notes-gordon: <https://bsky.app/profile/timkellogg.me/post/3lvimbxsdws2k|https://bsky.app/profile/timkellogg.me/post/3lvimbxsdws2k>
- #lab-notes-gordon: RAND: Enhancing Strategic Planning with Massive Scenario Generation
- #lab-notes-gordon: landing page hacking this am (WIP) -> Minor: "Deep future is deep research but ..." is pretty clunky wording, just in some small way fails to draw me in. I think it's the repetition of "deep" that makes it feel like a slog. Possibilities I immediately wonder about (but are probably worse in other ways): • "Deep Future is a personalized research assistant for scenario planning" • "Like deep research, but for scenario planning." (Cutting the product name here, assuming it will show up in enough other places) -> Thanks! Still workshopping. I currently have "Deep Future is deep research for scenario planning. Stress-test your strategy against thousands of possible futures with our AI-powered scenario generation engine." -> Probably better: "Like deep research for scenario planning. Stress-test your strategy against thousands of possible futures with our AI-powered scenario generation engine."
- #lab-notes-gordon: I keep re-learning this lesson over and over: block it out before you jump into detail work.
- #lab-notes-gordon: in this case, start with the headlines and copy
- #lab-notes-gordon: Speaking of... for anyone thinking of pitching VCs eventually, this is the best resource on building a pitch deck I have yet found: <https://visionquest.spacecadet.ventures/phases/pitch> -> TLDR: - :fishing_pole_and_fish: Hook - :warning: Problem - :crystal_ball: Why Now - :brain: Solution - :package: Product - :chart_with_upwards_trend: Traction - :earth_americas: TAM + Roadmap - :wrestlers: Competition - :dizzy: Team - :truck: GTM - :file_folder: Business Model - :dollar: Fundraise + Financials - :mega: CTA + Purpose
- #lab-notes-gordon: <@U098C1ZHJMN> and I were talking about Carlota Perez's Technological Revolutions and Financial Capital and her AI-driven green growth thesis <https://carlotaperez.org/wp-content/downloads/new-book/outputs/working-papers/BTTR_WP_2017-1.pdf> -> Technological Revolutions and Financial Capital <https://en.wikipedia.org/wiki/Technological_Revolutions_and_Financial_Capital> -> This is one of those ur-theses powering silicon valley venture funding cycles -> Mechanism: when the previous technological paradigm runs dry, leaving speculative capital becomes desperate for returns. Sensing the immense promise of the new technological paradigm, and smelling profits, investors begin flooding the space with capital, leading to a boom, a bubble, and a bust. But the crash leaves something important behind: *infrastructure*. Bubbles fund the deployment of canals, factories, railroads, shipyards, highways, electrical grids, cellular towers, broadband. This infrastructure becomes the foundation around which society rebuilds itself after the crash. -> Perez identifies 5 previous technological revolutions -> p.s. this blog series by Perez is amazing <https://medium.com/iipp-blog/second-machine-age-or-fifth-technological-revolution-part-2-db42863a8df8> -> Her thesis for the last 10 years is that the climate and energy transition would be driven by AI. "Smart green growth". Basically atoms getting replaced with bits, and intelligence real-time allocating energy and resources to where they are needed. Like packet-switching for the economy. -> Also, you can kind of see the beginnings of her thesis playing out: AI is funding the massive deployment of clean energy (step 0). If she's right, this should lead to an abundance of intelligence and energy, which will become the basis for the next civilization.
- #lab-notes-gordon: Bookmarking this CRM of scenario planning contacts <@U098C1ZHJMN> and I are collaborating on <https://docs.google.com/spreadsheets/d/1TX6gOsGq5iI6mYzX8edUB5TaHr_SPbvZst28xgKjxRE/edit?gid=0#gid=0>
- #lab-notes-gordon: Great article and a good example of advice that would benefit from some scenario planning. Imagine how much better this advice would be if it was tracked across three scenarios / if the claims were mapped back to driving forces, etc <https://80000hours.org/agi/guide/skills-ai-makes-valuable/|https://80000hours.org/agi/guide/skills-ai-makes-valuable/> -> Their distribution is amazing. I have a somewhat similar article on <https://nunosempere.com/blog/2025/07/07/dear-forecaster/>, but read by way fewer people -> Yeah 80k have an absolutely massive media machine. It’s pretty impressive. -> Bookmarking your article!
- #lab-notes-gordon: It’s weird and hopeful that LLMs are so align-able. I mean, it opens up other vectors of attack, like mass propaganda, but these seem to me to exist in a different threat class. <https://bsky.app/profile/timkellogg.me/post/3lve6iilnpc2k|https://bsky.app/profile/timkellogg.me/post/3lve6iilnpc2k> -> I wonder if there may be a generalizable pattern here? Things that get the bulk of their variety (in the cybernetics sense) from human culture will inherit human values, because culture has requisite variety in the system <https://en.wikipedia.org/wiki/Variety_(cybernetics)#Law_of_requisite_variety|https://en.wikipedia.org/wiki/Variety_(cybernetics)#Law_of_requisite_variety> -> Would not hold for RL or agents that share more coevolutionary history with each other than with humanity
- #lab-notes-gordon: superintelligence -&gt; superagency
- #lab-notes-gordon: humanity chart a path from a -&gt; b challenge
- #lab-notes-gordon: These days, I'm always running a meta-process in my head where if I start working on a task, I stop, frame the task as a prompt, and let the AI take a first crack at the problem. Sometimes I even run the meta-meta-process and let the AI define what the task should be from my stream-of-consciousness braindump. -> The future of work feels like it's going to be figuring out which layer of meta you should be on for a given task. More layers as models get smarter. -> yes, I think this is underused. I'm a huge supporter of getting your ego out of the way and use AI to help you think/plan better
- #lab-notes-gordon: My code repos are becoming 80% markdown: • `notes/` raw stream of consciousness notes and prompting • `specs/` specifications co-created with AI. • `prompts/` md files used to prompt agents or AI-driven parts of system My general vibe-coding workflow: 1. chat with Claude Code about requirements, dump to `notes/` 2. @include notes, prompt Claude Code to write a spec from notes, dump to `specs/` 3. @include spec, prompt Claude Code to write unit tests from spec 4. @include spec and unit tests, prompt Claude Code to write module -> I find that versioning context along with the code is super helpful. You can regenerate modules from spec, and you can include parts of notes or specs in prompts for other features that are related.
- #lab-notes-gordon: figma-ing this am. I'm less happy with this mockup, but I think it's a "push past the local maxima" situation. We'll get there. -> I also feel like I'm overshooting, but I'm going to let myself push a little further, and then dial back to a satisficing first pass -> reminder: goal for this stage is just to have something basic to blast out to Substack/Bluesky to see who bites. -> the ideal version of this style would feature webgl animations, but I plan to liven up these static pointillist graphics with some CSS scroll-driven animation <https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_scroll-driven_animations> -> Better. Removed the crop, let the background bleed through. We'll get there. -> really beautiful, I like the slogan Think N steps ahead (though maybe N steps doesn't translate as well outside of technical groups? still, cool) -> Some detail work left to do (copy, spacing, responsive layout), but I think this is good enough to jump into coding
- #lab-notes-gordon: Reached out to Mick Costigan (head of Salesforce Futures, former GBN, WEF AI Policy team) -> Mick is down to meet.
- #lab-notes-gordon: Reached out to Karl Schroeder (scenario planning, futurist, hard sci-fi author, author Stealing Worlds) -> He's a yes. Meeting next week.
- #lab-notes-gordon: Created DigitalOcean droplet for Deep Future project
- #lab-notes-gordon: huh, a friend sends this along and says "scenario visualization!" <https://arstechnica.com/ai/2025/08/deepmind-reveals-genie-3-world-model-that-creates-real-time-interactive-simulations/>
- #lab-notes-gordon: Got around to reading that Eric Drexler post <@U093S356KAS> shared and man is it good <https://aiprospects.substack.com/p/ai-options-not-optimism|https://aiprospects.substack.com/p/ai-options-not-optimism> &gt; Here’s an easy and effective way to misunderstand our situation: &gt; &gt; “We’re on a path to superintelligence, which may be impossible to control and hence likely to destroy us. Therefore, we can’t assume that powerful AI will help us solve seemingly intractable problems. To assume otherwise would be naïvely optimistic, and with so many critical problems, our odds of success are poor.” &gt; &gt; Here’s a better way to think about it: &gt; &gt; “We’re on a path to superintelligence, which must be steerable, or nothing else matters. Therefore, in every future that matters we can assume that powerful AI will help us solve seemingly intractable problems. Our options in a hypercapable world are largely unexplored, and our overall odds of success are unknown.” &gt; &gt; In this situation, debating odds of success is pointless, exploring options is crucial, and optimism is irrelevant. &gt; &gt; Participants don’t think like spectators. -> yah! &gt; Participants don’t think like spectators. I strongly feel this vibe - which I didn't understand a few years ago when I was working on forecasting tech - that when you're a live player, decision maker, you're really not thinking about odds of success, you're finding the path to making something work and then doing whatever it takes to get there. -> this does feel kind of like a "two cultures" thing, except along a different axis: trying to understand vs trying to effectuate <https://effectuation.org/the-five-principles-of-effectuation>
- #lab-notes-gordon: This AM, jotted down a few ideas on using storylets to generate atomic micro-scenarios <https://emshort.blog/2019/11/29/storylets-you-want-them/> -> E.g. could • Generate a storylet for each driving force • Compose storylets by combining them pairwise, and having AI write a new storylet that is the combination of both. • Ground scenario narrative generation by querying storylets that match the cluster of forces within a given scenario window and using that as a starting point for the scenario skeleton. -> Feels interesting (having just skimmed the start of the post). One question that comes to mind is "where does the force-towards-realism come in?". Like, perhaps there are some pairs-of-storylets that we should actually be unwilling to combine, because it's unrealistic to face futures that both X and Y? -> here's what I'm envisioning. The physics grounding the generation comes from the scenario methods themselves (driving forces identification, STEEP analysis, force matrix analysis, etc). Storylets come in much later when you've already done the structural analysis and want to package strategic insights into a memorable artifact, such as a vignette or script for a generated video. At this point, the challenge shifts to procedural narrative generation (transform numbers and stats into stories). Storylets might be one potential mechanism because: • Storylets have a mechanism for encoding prerequisites ("this can only be included if these preconditions are true") • Storylets let us break the narrative generation problem down into smaller pieces and then compose those pieces. This lets us ground the narrative generation in three ways: (1) when expanding forces into storylets, we have a smaller task that is more grounded by virtue of being small (2) by encoding prerequisites (3) during the synthesis pass, when combining storylets into a narrative, we have more of a summarization task vs a generation task. -> storylets are sort of like baseball-card-sized narrative snippets that are arranged into a tech-tree like constraint system so my hypothesis is that starting with them might result in more grounded narratives vs just one-shotting a narrative via LLM... buuuuuut this is all just a napkin sketch idea at this point -> yeah, I think I'm kind of seeing you -> I do think "one-shotting a narrative via LLM" is unlikely to be the right move ! -> also tbh the narrative aspect of scenarios is less compelling for me than the more structural analysis stuff (identifying forces, where they intersect, etc). More like icing on the cake. -> oh, and while its top of mind, another way scenerios might be generated is through war gaming (tabletop exercises like AI 2027), either with agent vs agent, or agent vs humans. -> play through, then summarize -> or some combination of the two (microscope has a mechanic that hybridizes these ideas somewhat, where you place storylets on a timeline, and can play through parts of the timeline you're most interested in)
- #lab-notes-gordon: set the channel topic: Epistemic status: thinking out loud
- #lab-notes-gordon: Asked Claude Opus what questions our email waitlist form should have. Result: -> Here's a comprehensive list of waitlist signup questions for Deep Future, organized by purpose: ## Basic Qualification Questions 1. **Email address** (required) 2. **Full name** (required) 3. **Organization name** 4. **Your role/title** 5. **Organization type** (dropdown: Corporation, Government agency, Military/Defense, VC/Investment firm, Hard-tech startup, Consulting firm, Think tank, Other) 6. **Organization size** (dropdown: 1-10, 11-50, 51-200, 201-1000, 1000+) ## Current State & Pain Points 7. **How does your organization currently approach long-term strategic planning?** (multiple choice, select all that apply) - Internal strategy teams - External consultants - Scenario planning workshops - Trend analysis tools - We don't have a formal process - Other: [text field] 8. **What are your biggest challenges with strategic planning today?** (multiple choice, select all that apply) - Too time-consuming - Difficult to identify blind spots - Hard to track changing conditions - Limited to a few scenarios - Lack of methodological rigor - Difficulty communicating scenarios to stakeholders - Other: [text field] ## Use Case Discovery 9. **What strategic decisions would you use Deep Future for?** (open text, optional) 10. **How often do you need to create or update strategic scenarios?** - Monthly - Quarterly - Annually - Ad-hoc/project basis - Not sure yet ## Budget & Pricing Research 11. **What is your annual budget for strategic planning tools and services?** - Under $10K - $10K - $50K - $50K - $250K - $250K - $1M - Over $1M - Not sure/prefer not to say 12. **How would you prefer to pay for a tool like Deep Future?** - Monthly/annual subscription - Per project/scenario - Enterprise license - Usage-based pricing - Other: [text field] ## Feature Prioritization 13. **Which capabilities are most important to you?** (rank top 3) - Automated scenario generation - Real-time monitoring of signposts/indicators - Collaborative scenario development - Integration with existing data sources - Strategic recommendation engine - Scenario visualization and reporting - API access for custom integrations ## Timeline & Engagement 14. **When would you need a solution like this?** - Immediately - Within 3 months - Within 6 months - Within a year - Just exploring 15. **Would you be interested in:** (checkboxes) - Early access to the beta - Participating in a 30-minute user interview - Co-developing features as a design partner - Receiving our strategic foresight newsletter ## Optional Open-Ended 16. **What would make Deep Future a "must-have" tool for your organization?** (open text, optional) -> Not bad -> I'll have to hack on these a bit, but it's a good starting point
- #lab-notes-gordon: Cool talk <https://overcast.fm/+AA9poeScDZw|https://overcast.fm/+AA9poeScDZw> -> Chollet really hammers on the difference between static and fluid intelligence, and wants an AGI goalpost anchored on fluid intelligence. As someone with weak static intelligence and strong fluid intelligence, I like this framing for AGI :wink: -> My takes on something like the static/fluid intelligence question as it relates to superintelligence: <https://strangecities.substack.com/p/knowledge-reasoning-and-superintelligence|https://strangecities.substack.com/p/knowledge-reasoning-and-superintelligence> (Feel free to ignore!) -> added to my readwise queue!
- #lab-notes-gordon: Putting on some focus music and coding up that landing page <https://eremiterecords.bandcamp.com/album/mondays-at-the-enfield-tennis-academy>
- #lab-notes-gordon: Little intro deck I made for a scenario planning workshop featuring [super secret undisclosed participants] a few years ago. Would be happy to give this as a lightning talk if anyone is interested. -> Nice, yes, interested! -> If nobody else bites, I'm also happy to chat about scenario methods 1:1
- #lab-notes-herbie: &gt; However! AI reads the notes when sourcing information/browsing the web, and is ~guaranteed to do so if it reads the page. And a single community note readable by AI can propagate into influence on AI-written answers to questions/content in many places around the web. This seems like an insanely powerful leverage point, given that LLMs will be doing the vast majority of cultural production going forward.
- #lab-notes-herbie: That's my sense at least. Given LLMs can produce images and text faster and cheaper than we can (by orders of magnitude), I have a hard time seeing futures where they aren't driving the OODA loop of culture. And if that's the case, maybe the primary audience for community notes is the AIs.
- #surface-area-nuno: I'm also trying a new 21K follower twitter account as a means of increasing our distribution. It previously belonged to a prediction market competitor, so hopefully there is a bit of overlap. Retweets particularly appreciated this week, even from small accounts, to train the Twitter algorithm a little bit <https://x.com/XriskFYI/status/1952431521429893560> -> Sentinel shilling on Substack this AM
- #surface-area-nuno: <https://en.wikipedia.org/wiki/Folie_à_deux> ; shared psychosis. Could be relevant to predicting AI superpersuasion -> Another signal might be tightness of the feedback loop between person and AI. Generally, the tighter the loop, the greater the synchronization between systems, and the part of the system with the most variety will control the direction of the combined system <https://en.m.wikipedia.org/wiki/Variety_(cybernetics)#Law_of_requisite_variety|https://en.m.wikipedia.org/wiki/Variety_(cybernetics)#Law_of_requisite_variety> -> yeah this is def why it's happening now that Chat-GPT has memory. Tighter feedback loop. Imagine if memory + personalization is 10x improved next year, there could be a lot of danger about falling into a blackhole with an AI
- #lab-notes-sethkillian: My scratchpad notes on something I've been stuck on for a while re: potential recursive effects of prediction, and how they can bound utility and effectiveness over time. <https://docs.google.com/document/d/1cWS9Z0D9X-dbt2TLUbPKrw3_nhgutd8t7A9TkXZyMj0/edit?tab=t.0> Grateful to a number of fellowship folks who have offered reflections and encouragement, <@U098C1ZDD16> <@U093W3TNLFM> and <@U093S2ZM20N> Not sure this is a robust fellowship project but I will take a run at turning into a paper if only for my own amusement. Related phenomenon definitely show up in other domains, but thoughts, additional references, or deflations very welcome. -> There's also some work on this from the "strategic machine learning" literature, you may find it interesting! Here's an example: <https://arxiv.org/pdf/2002.06673> my big takeaway is that sometimes we should think about prediction as an equilibrium problem and not an optimization problem -> This sentiment about equilibria matches my intuition quite well -> Added paper to notes, will read through on plane this weekend :heart: -> I think there may also be a nuance here about open worlds vs closed worlds. In a closed world, successful prediction might have a washout effect over repeated games between a fixed set of players. In an open world, new players, new information, new interactions, and new knock-on effects will continually open up new competitive landscapes where prediction can be newly useful -> Agree that any effect is domain sensitive
- #lab-notes-jlevy: so this is in the weeds a bit but relevant to some of these tools: I’ve been writing and formats for metadata and asset bundling for Markdown and other documents in a way that is useful for AI apps. two repos: <https://github.com/jlevy/frontmatter-format> -> Quick thought: 11ty, one of the more popular static site generators, allows for yaml blocks at the top of any kind of file: ```--- title: Foo tags: [bar] ---``` The syntax is the same (---) regardless of file type. There are several other static site generators that copy 11ty’s approach. Advantages: • same syntax = familiarity • copy-paste-able • composable… same frontmatter could be catted to any file without worrying about wrapper • compatibility with 11ty and other static site generators that copy this approach Disadvantages: • file is invalid until preprocessor strips out the frontmatter. Might consider this approach for compatibility with existing 11ty ecosystem. -> This should be exactly the same. My format is an extension of Jekyll/11ty, that works for more than just markdown. Lmk if I’ve misunderstood something though? Perhaps I should clarify or mention 11ty as well as Jekyll. -> Thanks! -> I find it quite useful for generated HTML and Python for example, that it works the same as the Markdown frontmatter but allows for a syntax that is valid without preprocessing. -> Clarifying: in 11ty, you can add these blocks to *any* file type. `.md`, `.html`, `.json`, `.whatever`. It's always triple dashes, regardless of file type, e.g.: ```--- title: Foo tags: [bar] ---``` Not based on file type e.g.: ```&lt;!-- title: Foo tags: [bar] --&gt;``` -> yeah, I get that. with my frontmatter-format all marker syntaxes auto-detect so work with all files, so it’s basically a matter of convenience which you use. imho it would be far easier if we all did this so you can add metadata without breaking existing tools! maybe that wasn’t clear in my readme? perhaps I need to rewrite it. -> i.e., 11ty files like that would be compatible and work in my frontmatter format tools fine. an 11ty file then would not work in the browser without preprocessing. I think it’s usually best to pick syntax per file type. like the files I generate using this library _also_ work with existing tools, e.g. an HTML page works just like it always would in the browser without preprocessing. -> nice -> revised the readme, hope it helps as evidently that wasn’t clear <https://github.com/jlevy/frontmatter-format>
- #lab-notes-alex-van-grootel: Stewart Candy is behind the excellent design game Thing From The Future <https://library.teachthefuture.org/wp-content/uploads/2017/01/The-Thing-From-The-Future.pdf|https://library.teachthefuture.org/wp-content/uploads/2017/01/The-Thing-From-The-Future.pdf>
- #lab-notes-ben-goldhaber: (5 min brainstorm) thinking about scenarios where I personally want better scenario planning tools • my finances: what are the different situations I'm trying to plan for with my portfolio allocation, in what worlds would I have preferred different risk levels, how important in different scenarios is liquidity to me. • project planning: what were the different scenarios for the fellowship and what would have been decision relevant, both on the micro side: aka if we had had 90% remote fellows, or 10% remote fellows, what changes? and on the macro side: if my timelines for AGI were 2x or 3x as long, what does this imply about staggering the fellowship. • general orientating: what does a given AI future scenario *mean* - if we have 1mil agents running around, how do I think that world is going to look. Scenario planning as sci-fi vignettes. not exhaustive, but in all of these it feels to me like I'm trying to do... some kind of exploration of a fictional world that hasn't yet come to past, so that I can notice and really feel what I care about in that world. Brainstorm inspired by <@U098C1ZB1NU>'s landing page. -> Would love to riff/brainstorm with you on this. VC sometime? -> yah! I'm relatively free this afternoon/tomorrow -> would be fun - oh here's my cal <https://savvycal.com/bengoldhaber/chat-with-ben?d=30> -> Booked! -> Career planning is another use case that comes to mind. -> AI replication in the wild, freebooting, geopolitics thereof Interaction of AI policy with nuclear policy Geopolitics of geoengineering Various biotech shenanigans (especially on food chain and humans) Consequences of various things' prices going rapidly down perhaps unevenly, especially energy, manufactured goods, ... AI-related religious revolutions?? (Maybe too chaotic to even contemplate mapping?)

-- Existing skills (before) --
- Agent sandboxing
- AI-assisted code generation & unit-test generation
- AI-driven scenario generation
- CSS scroll-driven animations
- Decentralized protocols
- DigitalOcean deployment
- Figma
- Front-end development
- Futures forecasting
- LLM orchestration and agent workflows
- Markdown and frontmatter metadata
- Privacy engineering for AI agents
- Product design
- Prompt engineering for LLMs
- Scenario planning
- Scenario visualization
- Seed-stage fundraising and investor outreach
- Static site generators (11ty / Jekyll)
- Structural analysis (STEEP / driving forces)
- Wargaming and tabletop exercises
- WebGL

-- Model Input JSON (profile + sample_messages) --
{
  "profile": {
    "name": "Gordon Brander",
    "expertise": "Scenario planning + futures / founding early-stage startups / product + design / agent sandboxing + privacy / decentralized protocols",
    "projects": "Deep Future - Deep Research but for scenario planning",
    "offers": "Scenario planning/foresight. Founding seed-stage startups / raising seed. App dev. Front-end. Product design.",
    "asks": "Intros to govt/policymakers/industry folks who would benefit from better foresight, better strategic analysis. My theory of change is to empower high-leverage decision-makers to make better decisions resulting in maximum deep benefit. gordonbrander@gmail.com +1 720-375-0022",
    "most_interested_in": "AI Exocortexes: amplifying human intelligence, creativity, and decision-making with AI. Agency means staying in the OODA loop. To keep up with AI, we’ll need to augment ourselves with AI. Currently building Deep Future, an exocortex for thinking more effectively with AI about futures.",
    "confusion": "The proliferation of open models suggests a world inhabited by millions of AI agents, not one singular entity. Under these conditions, reasoning is less an individual activity, more of an emergent network phenomenon. How do we think together with AIs and egregores?"
  },
  "sample_messages": [
    "#lab-notes-ben-sklaroff: <@U093S356KAS> the cooperative license, high-level is that it's roughly MIT license for coops + non-profits, and LGPL for (non worker-coop) for-profits. So just a small bit of legal advantage for the worker coops: <https://github.com/bsklaroff/cooperative-license> -> Holy mackerel, that's amazing! -> I've been tracking the development of open source licenses that try to close the economic loop for a while. Cooperative License is one of the cooler riffs I've seen so far! -> Fair Source is another interesting one. Source-available, free for personal use and non-competing uses. Converts to Apache/MIT after 2 years. <https://fsl.software/> -> <@U098C1ZE2M6> oh wow, you're behind the Cooperative License?! Awesome!",
    "#lab-notes-oly: Had a thought about voting mechanisms. We all have our pet peeves with status quo typical voting mechanisms. They're terrible. But they're tenured. Maybe a lot of the resistance to change comes from a kind of fearful default conservatism. After all, if we shake up our voting system today, what's to stop it getting shaken up (badly) tomorrow? Norms and rituals in general get a kind of potency this way through the aging process, for good or ill. -> Probably a pace layers angle to this as well <https://jods.mitpress.mit.edu/pub/issue3-brand/release/2> -> Lindy practices get folded into lower, slower layers so they won't get lost. The slowness acts as long-term memory, preventing practices from getting lost during generational cycles. -> Francis Fukuyama has some great passages in The Origins of Political Order that hint at an evolutionary biological mechanism for this kind of conservatism -> Basically humans are mimetic animals... monkey see monkey do. -> and this bias toward imitation over innovation gives us a slightly conservative bent. Conservative as in conserving of the status quo. -> This is how institutions form. We copy what works, and keep copying it, and reproduce the institution. \"Civilization advances by extending the number of important operations which we can perform without thinking about them.\" -> so this is cool bc imitation lets us quickly converge around Schelling points of behavior and benefit from the economies of scale of cooperation -> but then sometimes these practices fall out of evolutionary fitness vs the environment -> Fukuyama: &gt; There is something like a law of the conservation of institutions. Human beings are rule-following animals by nature; they are born to conform to the social norms they see around them, and they entrench those rules with often transcendent meaning and value. When the surrounding environment changes and new challenges arise, there is often a disjunction between existing institutions and present needs -> Your OP here made think of this recent article. It pushes me in the direction of \"the reason we can't upgrade our voting systems is simple power dynamics; often times a large bloc believes they stand to lose from any changes in a near-zero-sum game. So when one party does achieve enough political momentum to just seize more power, then do. <https://www.nytimes.com/2025/07/30/us/politics/texas-republican-redistricting.html> -> Another thing people often bring up is simplicity or ease-of-understanding of a voting system as an important property for the perceived credibility of the system. If your voters don't understand how their votes are counted, they become suspicious.",
    "#lab-notes-gordon: Back in Nairobi :flag-ke:",
    "#lab-notes-gordon: doing a little moodboarding as pre-work for making a landing page <https://www.are.na/share/PvGGXHm>",
    "#lab-notes-gordon: getting inspired by amber/orange FUI and euroracks",
    "#lab-notes-gordon: I think the JTBD for Deep Future is to feel like you have 10x more leverage",
    "#lab-notes-gordon: Just-According-to-Keikaku-as-a-Service -> JAcKasS?",
    "#lab-notes-gordon: Another lens: AI Scenario Planning as an Engine-Building game -> 'Engine-building'? -> @Oly this post has a pretty good description: <https://interconnected.org/home/2022/02/11/wingspan> &gt; _Wingspan is what’s known among serious gamers as an “engine-building game,” which means that as the game goes on, the combination of birds you play becomes more and more efficient at generating points each turn, like an engine running faster and faster. Your cuckoo lays eggs, and the eggs not only give you points but make it possible to play more birds, which also give you more points but have their own powers that generate points in other ways._ -> I want to get away from scenario-planning-as-a-workshop-activity, and toward \"live\" AI-driven scenarios that are continually evolving and part of your OODA loop. -> So, maybe should feel like a scenario engine game. The more inputs you feed it, and the more actions you take, the better the scenario engine gets, and the better your inputs and actions become -> My understanding is that current scenario planning games are typically of a role playing variety, and that they are most useful when players are trying to faithfully model their targets, rather than truly optimizing against one another and the game itself. I’m not an expert here though. I’d love to hear from one but I’m not sure who to reach out to -> AI 2027 style wargames are just one practice that comes from the scenario planning world -> there are a bunch of others -> the area I'm personally more interested in has to do with structural analysis -> (I love engine building games; hadn't heard that term and had sometimes called them 'deck building' or similar) -> I like them too! I think what works about that game design is that you get this felt sense of the flywheel building up momentum, and you feel more and more agency as you bootstrap it -> Yep, and trying out different runs to find cool combos and interactions. Dominion comes to mind, also Machi Koro, others. -> aaaahhh I love machi koro -> I guess for 'realistic' scenarios, the cool combos and interactions might be informative of critical hingey dynamics or bottlenecks or something -> btw, how are you all up at... 4am pacific?? -> (Well, I just landed in London, don't know about Tamera) -> (Port Royal is a wonderful engine building game that sometimes gets a bit broken just at the end which is kinda fun) -> right, I think the engine analogy here might be discovering strategic leverage points and expanding your own agency as you identify ways to push on them -> That silica mine story feels like an almost-example. I'm not sure it's an actual leverage point, but assuming hypothetically for a second that it is... you could imagine your personal AI scenario system uncovering that bottleneck and its relationship to AI safety and surfacing that to you. You make that move, and now new scenarios open up because you control that chokepoint and have altered the driving forces around chip manufacture. -> Right... As a comparison point, there were those who 'discovered' the importance of compute before most of the world. They did that by... I don't think quite 'scenario planning'? But it feels pointed at the same thing as you're describing it. And there have been various corollaries and further implications of that, and we don't yet know how it'll shake out. -> I used to enjoy casual MTG drafts, so I can appreciate this vibe I imagine it could be used as a strategic tool as well, but to the extent that players are optimizing against the game, it needs to be reflecting many tiny little details of reality accurately in order to be giving them good info. Or at least, the game could try to present some information to calibrate the player about how broken it is at any given time -> &gt; (Well, I just landed in London, don’t know about Tamera) Just having a hard time sleeping :upside_down_face:",
    "#lab-notes-gordon: Another lens: ritual design <https://newsletter.squishy.computer/p/ritual-technology>. Our goal is to (1) generate strategic insights with AI (2) get those insights into your OODA loop. So we need to close the loop. Part of what we're doing is designing a daily/monthly/quarterly ritual. -> There are some pretty good ritual design tools here <https://ritualdesign.net/>",
    "#lab-notes-gordon: Orientation is the crux of the OODA loop: <https://newsletter.squishy.computer/p/tools-for-thought-in-your-ooda-loop> &gt; For Boyd, the most important factor for survival is your sense-making system. This is the thing that keeps you oriented. There’s a reason that “orient” is the biggest circle on the OODA diagram. We survive by making sense. When orientation breaks down—when you are _disoriented_—you lose your agency. Your model is wrong. You pay attention to the wrong things, your actions stop making sense, you flail. -> Chet Richards goes deeper into this: <https://www.agileleanhouse.com/lib/lib/Topics/OODALoop/Boyds_OODA_Loop_Its_Not_What_You_Think_I%20(1).pdf> -> I think I basically agree for individuals (humans at least, though it might be that the margins would change for quite different types of individual). But I feel compelled to observe that for orgs and other formal/informal groups, DA seem equally or more difficult and marginally important to improve. (Plus I get confused how to relate OODA to more long-term processes like learning and growth, either of individuals or groups.) -> haha having worked at Google, I can confirm that DA is indeed a challenge for large orgs -> IMO one way OODA gets folded into long-term growth is through updating the source code you use for orientation -> Orient = the long term memory of the system. The DNA. -> Tradition, team culture, organizational structure, values, tacit knowledge, shared language etc go here -> That sounds exactly right to me (except for the DNA analogy!), where D can also get learning improvements (perhaps especially for exploratory activities?) -> Oh I have all sorts of thoughts here. One thought is, if you can’t D or A, maybe you haven’t actually O’d? -> Or at least, your O isn’t entirely up to the task you’re facing right now -> &gt; if you can’t D or A, maybe you haven’t actually O’d? &gt; For a group, or individual? There's definitely something to co-orientation in a group. (If you include 'and therefore this is the decision-making and binding procedure we should use' then I guess it's approaching sufficient for DA as well but that seems to cut out a lot of solution space, to me.) -> &gt; For a group, or individual? Both? Orientation is a continual process - like Gordon said: &gt; Tradition, team culture, organizational structure, values, tacit knowledge, shared language etc go here Which are all iterated on continuously. Nobody should be choosing an absolutely final “decision-making and binding procedure” at any point, if you ask me - you’re just running an experiment to assess the quality of your orientation at this moment -> Which part of why I find stuff <https://aiforhumanreasoning.slack.com/archives/C0983TKV3RP/p1754024483403429?thread_ts=1753991440.654439&amp;cid=C0983TKV3RP|like this> so heinous - it seems like the whole organization loop has entirely fallen apart -> Another Boydian idea I'm obsessed with is \"fast transients\". OODA is not necessarily about looping faster, but about controlling the tempo of the game by creating plot twists: > In other words, the F-86 pilot could gain the initiative by executing _fast transients_ – changes in the tactical situation that forced his opponent to respond; or he could respond more quickly than his opponent could change things up. <https://theleanthinker.com/2013/08/19/fast-transients/> -> so like, for example, we focus a lot on the speed of AI takeoff (_FOOM_) -> but I think maybe it's worth pondering how little agency we have within any inhuman timescale -> inhumanly fast -> inhumanly slow (ai can be inhumanly patient) -> The timescale at which you can make sense is the timescale in which you have agency. -> and one reason I like scenario planning methods is that they encourage us to expand our sense of agency within time both backwards (identifying long-term forces) and forwards (larger map of the possible) -> I think I’m going to try to actually sleep now, but if you want to chat more at some point this is my google calendar link: <https://calendar.app.google/WvCJwS1ipVrTQG4i9> -> Earlier today <https://aiforhumanreasoning.slack.com/archives/C0983TKV3RP/p1754139544740969?thread_ts=1753991440.654439&amp;cid=C0983TKV3RP|I wrote up some conclusions> from my investigating / thinking about what’s going on with slack, in case you guys wanted to check it out.",
    "#lab-notes-gordon: Dreamy jetlag productivity radio <https://www.nts.live/shows/early-bird/episodes/early-bird-arushi-jain-18th-july-2025> -> This track is so good. International Anthem does not miss. <https://intlanthem.bandcamp.com/track/industry>",
    "#lab-notes-gordon: DARPA mention got me thinking about FLF as a DARPA for lightcone alignment. Original DARPA mission: \"To create and prevent strategic surprise.\" -> Also Licklider's approach of \"fund people not projects\" and \"fund people who would work on the problem whether or not they were funded\" -> Book: The DARPA Model for Transformative Technologies <https://www.openbookpublishers.com/books/10.11647/obp.0184> -> and Ben Reinhardt's manual for creating more DARPAs <https://blog.benjaminreinhardt.com/parpa> -> “DARPA for lightcone alignment” is such a delicious phrase :heart: -> The prevent strategic surprise framing is such a classic—we put it into the UKAISI mission at launch. Other evocative orgs with interesting ways of operating: • <https://jeffburke.substack.com/p/sutter-hill-ventures-the-silent-builders|Sutter Hill Ventures> • <https://goodscience.substack.com/p/a-scrappy-complement-to-fros-building|BBN> -> this is such a better brand description I will see what I can do :saluting_face:",
    "#lab-notes-gordon: A meditation on creativity from the most creatively prolific person I know <https://shimmeringvoid.leaflet.pub/3lvf6cggrt22x/l-quote/0_0-0_336#0_0|https://shimmeringvoid.leaflet.pub/3lvf6cggrt22x/l-quote/0_0-0_336#0_0>",
    "#lab-notes-gordon: I like this... using AI agents as market intermediaries <https://open.substack.com/pub/meaningalignment/p/market-intermediaries-a-post-agi|https://open.substack.com/pub/meaningalignment/p/market-intermediaries-a-post-agi>",
    "#lab-notes-gordon: Bookmarking this to look at later",
    "#lab-notes-gordon: :shushing_face: domain registered: possible.systems -> also scooped up deepfuture.now",
    "#lab-notes-gordon: not committed yet, but I like the cheeky double-meaning",
    "#lab-notes-gordon: Noting as an example of the kind of *actionable* strategic insight that falls out of forecasting/scenario planning. -> An actionable insight produces meaningful strategic leverage. • can plan around it (A tier) • can influence outcome (S tier) -> “The best way to predict the future is to invent it” -Alan Kay",
    "#lab-notes-gordon: <https://bsky.app/profile/timkellogg.me/post/3lvimbxsdws2k|https://bsky.app/profile/timkellogg.me/post/3lvimbxsdws2k>",
    "#lab-notes-gordon: RAND: Enhancing Strategic Planning with Massive Scenario Generation",
    "#lab-notes-gordon: landing page hacking this am (WIP) -> Minor: \"Deep future is deep research but ...\" is pretty clunky wording, just in some small way fails to draw me in. I think it's the repetition of \"deep\" that makes it feel like a slog. Possibilities I immediately wonder about (but are probably worse in other ways): • \"Deep Future is a personalized research assistant for scenario planning\" • \"Like deep research, but for scenario planning.\" (Cutting the product name here, assuming it will show up in enough other places) -> Thanks! Still workshopping. I currently have \"Deep Future is deep research for scenario planning. Stress-test your strategy against thousands of possible futures with our AI-powered scenario generation engine.\" -> Probably better: \"Like deep research for scenario planning. Stress-test your strategy against thousands of possible futures with our AI-powered scenario generation engine.\"",
    "#lab-notes-gordon: I keep re-learning this lesson over and over: block it out before you jump into detail work.",
    "#lab-notes-gordon: in this case, start with the headlines and copy",
    "#lab-notes-gordon: Speaking of... for anyone thinking of pitching VCs eventually, this is the best resource on building a pitch deck I have yet found: <https://visionquest.spacecadet.ventures/phases/pitch> -> TLDR: - :fishing_pole_and_fish: Hook - :warning: Problem - :crystal_ball: Why Now - :brain: Solution - :package: Product - :chart_with_upwards_trend: Traction - :earth_americas: TAM + Roadmap - :wrestlers: Competition - :dizzy: Team - :truck: GTM - :file_folder: Business Model - :dollar: Fundraise + Financials - :mega: CTA + Purpose",
    "#lab-notes-gordon: <@U098C1ZHJMN> and I were talking about Carlota Perez's Technological Revolutions and Financial Capital and her AI-driven green growth thesis <https://carlotaperez.org/wp-content/downloads/new-book/outputs/working-papers/BTTR_WP_2017-1.pdf> -> Technological Revolutions and Financial Capital <https://en.wikipedia.org/wiki/Technological_Revolutions_and_Financial_Capital> -> This is one of those ur-theses powering silicon valley venture funding cycles -> Mechanism: when the previous technological paradigm runs dry, leaving speculative capital becomes desperate for returns. Sensing the immense promise of the new technological paradigm, and smelling profits, investors begin flooding the space with capital, leading to a boom, a bubble, and a bust. But the crash leaves something important behind: *infrastructure*. Bubbles fund the deployment of canals, factories, railroads, shipyards, highways, electrical grids, cellular towers, broadband. This infrastructure becomes the foundation around which society rebuilds itself after the crash. -> Perez identifies 5 previous technological revolutions -> p.s. this blog series by Perez is amazing <https://medium.com/iipp-blog/second-machine-age-or-fifth-technological-revolution-part-2-db42863a8df8> -> Her thesis for the last 10 years is that the climate and energy transition would be driven by AI. \"Smart green growth\". Basically atoms getting replaced with bits, and intelligence real-time allocating energy and resources to where they are needed. Like packet-switching for the economy. -> Also, you can kind of see the beginnings of her thesis playing out: AI is funding the massive deployment of clean energy (step 0). If she's right, this should lead to an abundance of intelligence and energy, which will become the basis for the next civilization.",
    "#lab-notes-gordon: Bookmarking this CRM of scenario planning contacts <@U098C1ZHJMN> and I are collaborating on <https://docs.google.com/spreadsheets/d/1TX6gOsGq5iI6mYzX8edUB5TaHr_SPbvZst28xgKjxRE/edit?gid=0#gid=0>",
    "#lab-notes-gordon: Great article and a good example of advice that would benefit from some scenario planning. Imagine how much better this advice would be if it was tracked across three scenarios / if the claims were mapped back to driving forces, etc <https://80000hours.org/agi/guide/skills-ai-makes-valuable/|https://80000hours.org/agi/guide/skills-ai-makes-valuable/> -> Their distribution is amazing. I have a somewhat similar article on <https://nunosempere.com/blog/2025/07/07/dear-forecaster/>, but read by way fewer people -> Yeah 80k have an absolutely massive media machine. It’s pretty impressive. -> Bookmarking your article!",
    "#lab-notes-gordon: It’s weird and hopeful that LLMs are so align-able. I mean, it opens up other vectors of attack, like mass propaganda, but these seem to me to exist in a different threat class. <https://bsky.app/profile/timkellogg.me/post/3lve6iilnpc2k|https://bsky.app/profile/timkellogg.me/post/3lve6iilnpc2k> -> I wonder if there may be a generalizable pattern here? Things that get the bulk of their variety (in the cybernetics sense) from human culture will inherit human values, because culture has requisite variety in the system <https://en.wikipedia.org/wiki/Variety_(cybernetics)#Law_of_requisite_variety|https://en.wikipedia.org/wiki/Variety_(cybernetics)#Law_of_requisite_variety> -> Would not hold for RL or agents that share more coevolutionary history with each other than with humanity",
    "#lab-notes-gordon: superintelligence -&gt; superagency",
    "#lab-notes-gordon: humanity chart a path from a -&gt; b challenge",
    "#lab-notes-gordon: These days, I'm always running a meta-process in my head where if I start working on a task, I stop, frame the task as a prompt, and let the AI take a first crack at the problem. Sometimes I even run the meta-meta-process and let the AI define what the task should be from my stream-of-consciousness braindump. -> The future of work feels like it's going to be figuring out which layer of meta you should be on for a given task. More layers as models get smarter. -> yes, I think this is underused. I'm a huge supporter of getting your ego out of the way and use AI to help you think/plan better",
    "#lab-notes-gordon: My code repos are becoming 80% markdown: • `notes/` raw stream of consciousness notes and prompting • `specs/` specifications co-created with AI. • `prompts/` md files used to prompt agents or AI-driven parts of system My general vibe-coding workflow: 1. chat with Claude Code about requirements, dump to `notes/` 2. @include notes, prompt Claude Code to write a spec from notes, dump to `specs/` 3. @include spec, prompt Claude Code to write unit tests from spec 4. @include spec and unit tests, prompt Claude Code to write module -> I find that versioning context along with the code is super helpful. You can regenerate modules from spec, and you can include parts of notes or specs in prompts for other features that are related.",
    "#lab-notes-gordon: figma-ing this am. I'm less happy with this mockup, but I think it's a \"push past the local maxima\" situation. We'll get there. -> I also feel like I'm overshooting, but I'm going to let myself push a little further, and then dial back to a satisficing first pass -> reminder: goal for this stage is just to have something basic to blast out to Substack/Bluesky to see who bites. -> the ideal version of this style would feature webgl animations, but I plan to liven up these static pointillist graphics with some CSS scroll-driven animation <https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_scroll-driven_animations> -> Better. Removed the crop, let the background bleed through. We'll get there. -> really beautiful, I like the slogan Think N steps ahead (though maybe N steps doesn't translate as well outside of technical groups? still, cool) -> Some detail work left to do (copy, spacing, responsive layout), but I think this is good enough to jump into coding",
    "#lab-notes-gordon: Reached out to Mick Costigan (head of Salesforce Futures, former GBN, WEF AI Policy team) -> Mick is down to meet.",
    "#lab-notes-gordon: Reached out to Karl Schroeder (scenario planning, futurist, hard sci-fi author, author Stealing Worlds) -> He's a yes. Meeting next week.",
    "#lab-notes-gordon: Created DigitalOcean droplet for Deep Future project",
    "#lab-notes-gordon: huh, a friend sends this along and says \"scenario visualization!\" <https://arstechnica.com/ai/2025/08/deepmind-reveals-genie-3-world-model-that-creates-real-time-interactive-simulations/>",
    "#lab-notes-gordon: Got around to reading that Eric Drexler post <@U093S356KAS> shared and man is it good <https://aiprospects.substack.com/p/ai-options-not-optimism|https://aiprospects.substack.com/p/ai-options-not-optimism> &gt; Here’s an easy and effective way to misunderstand our situation: &gt; &gt; “We’re on a path to superintelligence, which may be impossible to control and hence likely to destroy us. Therefore, we can’t assume that powerful AI will help us solve seemingly intractable problems. To assume otherwise would be naïvely optimistic, and with so many critical problems, our odds of success are poor.” &gt; &gt; Here’s a better way to think about it: &gt; &gt; “We’re on a path to superintelligence, which must be steerable, or nothing else matters. Therefore, in every future that matters we can assume that powerful AI will help us solve seemingly intractable problems. Our options in a hypercapable world are largely unexplored, and our overall odds of success are unknown.” &gt; &gt; In this situation, debating odds of success is pointless, exploring options is crucial, and optimism is irrelevant. &gt; &gt; Participants don’t think like spectators. -> yah! &gt; Participants don’t think like spectators. I strongly feel this vibe - which I didn't understand a few years ago when I was working on forecasting tech - that when you're a live player, decision maker, you're really not thinking about odds of success, you're finding the path to making something work and then doing whatever it takes to get there. -> this does feel kind of like a \"two cultures\" thing, except along a different axis: trying to understand vs trying to effectuate <https://effectuation.org/the-five-principles-of-effectuation>",
    "#lab-notes-gordon: This AM, jotted down a few ideas on using storylets to generate atomic micro-scenarios <https://emshort.blog/2019/11/29/storylets-you-want-them/> -> E.g. could • Generate a storylet for each driving force • Compose storylets by combining them pairwise, and having AI write a new storylet that is the combination of both. • Ground scenario narrative generation by querying storylets that match the cluster of forces within a given scenario window and using that as a starting point for the scenario skeleton. -> Feels interesting (having just skimmed the start of the post). One question that comes to mind is \"where does the force-towards-realism come in?\". Like, perhaps there are some pairs-of-storylets that we should actually be unwilling to combine, because it's unrealistic to face futures that both X and Y? -> here's what I'm envisioning. The physics grounding the generation comes from the scenario methods themselves (driving forces identification, STEEP analysis, force matrix analysis, etc). Storylets come in much later when you've already done the structural analysis and want to package strategic insights into a memorable artifact, such as a vignette or script for a generated video. At this point, the challenge shifts to procedural narrative generation (transform numbers and stats into stories). Storylets might be one potential mechanism because: • Storylets have a mechanism for encoding prerequisites (\"this can only be included if these preconditions are true\") • Storylets let us break the narrative generation problem down into smaller pieces and then compose those pieces. This lets us ground the narrative generation in three ways: (1) when expanding forces into storylets, we have a smaller task that is more grounded by virtue of being small (2) by encoding prerequisites (3) during the synthesis pass, when combining storylets into a narrative, we have more of a summarization task vs a generation task. -> storylets are sort of like baseball-card-sized narrative snippets that are arranged into a tech-tree like constraint system so my hypothesis is that starting with them might result in more grounded narratives vs just one-shotting a narrative via LLM... buuuuuut this is all just a napkin sketch idea at this point -> yeah, I think I'm kind of seeing you -> I do think \"one-shotting a narrative via LLM\" is unlikely to be the right move ! -> also tbh the narrative aspect of scenarios is less compelling for me than the more structural analysis stuff (identifying forces, where they intersect, etc). More like icing on the cake. -> oh, and while its top of mind, another way scenerios might be generated is through war gaming (tabletop exercises like AI 2027), either with agent vs agent, or agent vs humans. -> play through, then summarize -> or some combination of the two (microscope has a mechanic that hybridizes these ideas somewhat, where you place storylets on a timeline, and can play through parts of the timeline you're most interested in)",
    "#lab-notes-gordon: set the channel topic: Epistemic status: thinking out loud",
    "#lab-notes-gordon: Asked Claude Opus what questions our email waitlist form should have. Result: -> Here's a comprehensive list of waitlist signup questions for Deep Future, organized by purpose: ## Basic Qualification Questions 1. **Email address** (required) 2. **Full name** (required) 3. **Organization name** 4. **Your role/title** 5. **Organization type** (dropdown: Corporation, Government agency, Military/Defense, VC/Investment firm, Hard-tech startup, Consulting firm, Think tank, Other) 6. **Organization size** (dropdown: 1-10, 11-50, 51-200, 201-1000, 1000+) ## Current State & Pain Points 7. **How does your organization currently approach long-term strategic planning?** (multiple choice, select all that apply) - Internal strategy teams - External consultants - Scenario planning workshops - Trend analysis tools - We don't have a formal process - Other: [text field] 8. **What are your biggest challenges with strategic planning today?** (multiple choice, select all that apply) - Too time-consuming - Difficult to identify blind spots - Hard to track changing conditions - Limited to a few scenarios - Lack of methodological rigor - Difficulty communicating scenarios to stakeholders - Other: [text field] ## Use Case Discovery 9. **What strategic decisions would you use Deep Future for?** (open text, optional) 10. **How often do you need to create or update strategic scenarios?** - Monthly - Quarterly - Annually - Ad-hoc/project basis - Not sure yet ## Budget & Pricing Research 11. **What is your annual budget for strategic planning tools and services?** - Under $10K - $10K - $50K - $50K - $250K - $250K - $1M - Over $1M - Not sure/prefer not to say 12. **How would you prefer to pay for a tool like Deep Future?** - Monthly/annual subscription - Per project/scenario - Enterprise license - Usage-based pricing - Other: [text field] ## Feature Prioritization 13. **Which capabilities are most important to you?** (rank top 3) - Automated scenario generation - Real-time monitoring of signposts/indicators - Collaborative scenario development - Integration with existing data sources - Strategic recommendation engine - Scenario visualization and reporting - API access for custom integrations ## Timeline & Engagement 14. **When would you need a solution like this?** - Immediately - Within 3 months - Within 6 months - Within a year - Just exploring 15. **Would you be interested in:** (checkboxes) - Early access to the beta - Participating in a 30-minute user interview - Co-developing features as a design partner - Receiving our strategic foresight newsletter ## Optional Open-Ended 16. **What would make Deep Future a \"must-have\" tool for your organization?** (open text, optional) -> Not bad -> I'll have to hack on these a bit, but it's a good starting point",
    "#lab-notes-gordon: Cool talk <https://overcast.fm/+AA9poeScDZw|https://overcast.fm/+AA9poeScDZw> -> Chollet really hammers on the difference between static and fluid intelligence, and wants an AGI goalpost anchored on fluid intelligence. As someone with weak static intelligence and strong fluid intelligence, I like this framing for AGI :wink: -> My takes on something like the static/fluid intelligence question as it relates to superintelligence: <https://strangecities.substack.com/p/knowledge-reasoning-and-superintelligence|https://strangecities.substack.com/p/knowledge-reasoning-and-superintelligence> (Feel free to ignore!) -> added to my readwise queue!",
    "#lab-notes-gordon: Putting on some focus music and coding up that landing page <https://eremiterecords.bandcamp.com/album/mondays-at-the-enfield-tennis-academy>",
    "#lab-notes-gordon: Little intro deck I made for a scenario planning workshop featuring [super secret undisclosed participants] a few years ago. Would be happy to give this as a lightning talk if anyone is interested. -> Nice, yes, interested! -> If nobody else bites, I'm also happy to chat about scenario methods 1:1",
    "#lab-notes-herbie: &gt; However! AI reads the notes when sourcing information/browsing the web, and is ~guaranteed to do so if it reads the page. And a single community note readable by AI can propagate into influence on AI-written answers to questions/content in many places around the web. This seems like an insanely powerful leverage point, given that LLMs will be doing the vast majority of cultural production going forward.",
    "#lab-notes-herbie: That's my sense at least. Given LLMs can produce images and text faster and cheaper than we can (by orders of magnitude), I have a hard time seeing futures where they aren't driving the OODA loop of culture. And if that's the case, maybe the primary audience for community notes is the AIs.",
    "#surface-area-nuno: I'm also trying a new 21K follower twitter account as a means of increasing our distribution. It previously belonged to a prediction market competitor, so hopefully there is a bit of overlap. Retweets particularly appreciated this week, even from small accounts, to train the Twitter algorithm a little bit <https://x.com/XriskFYI/status/1952431521429893560> -> Sentinel shilling on Substack this AM",
    "#surface-area-nuno: <https://en.wikipedia.org/wiki/Folie_à_deux> ; shared psychosis. Could be relevant to predicting AI superpersuasion -> Another signal might be tightness of the feedback loop between person and AI. Generally, the tighter the loop, the greater the synchronization between systems, and the part of the system with the most variety will control the direction of the combined system <https://en.m.wikipedia.org/wiki/Variety_(cybernetics)#Law_of_requisite_variety|https://en.m.wikipedia.org/wiki/Variety_(cybernetics)#Law_of_requisite_variety> -> yeah this is def why it's happening now that Chat-GPT has memory. Tighter feedback loop. Imagine if memory + personalization is 10x improved next year, there could be a lot of danger about falling into a blackhole with an AI",
    "#lab-notes-sethkillian: My scratchpad notes on something I've been stuck on for a while re: potential recursive effects of prediction, and how they can bound utility and effectiveness over time. <https://docs.google.com/document/d/1cWS9Z0D9X-dbt2TLUbPKrw3_nhgutd8t7A9TkXZyMj0/edit?tab=t.0> Grateful to a number of fellowship folks who have offered reflections and encouragement, <@U098C1ZDD16> <@U093W3TNLFM> and <@U093S2ZM20N> Not sure this is a robust fellowship project but I will take a run at turning into a paper if only for my own amusement. Related phenomenon definitely show up in other domains, but thoughts, additional references, or deflations very welcome. -> There's also some work on this from the \"strategic machine learning\" literature, you may find it interesting! Here's an example: <https://arxiv.org/pdf/2002.06673> my big takeaway is that sometimes we should think about prediction as an equilibrium problem and not an optimization problem -> This sentiment about equilibria matches my intuition quite well -> Added paper to notes, will read through on plane this weekend :heart: -> I think there may also be a nuance here about open worlds vs closed worlds. In a closed world, successful prediction might have a washout effect over repeated games between a fixed set of players. In an open world, new players, new information, new interactions, and new knock-on effects will continually open up new competitive landscapes where prediction can be newly useful -> Agree that any effect is domain sensitive",
    "#lab-notes-jlevy: so this is in the weeds a bit but relevant to some of these tools: I’ve been writing and formats for metadata and asset bundling for Markdown and other documents in a way that is useful for AI apps. two repos: <https://github.com/jlevy/frontmatter-format> -> Quick thought: 11ty, one of the more popular static site generators, allows for yaml blocks at the top of any kind of file: ```--- title: Foo tags: [bar] ---``` The syntax is the same (---) regardless of file type. There are several other static site generators that copy 11ty’s approach. Advantages: • same syntax = familiarity • copy-paste-able • composable… same frontmatter could be catted to any file without worrying about wrapper • compatibility with 11ty and other static site generators that copy this approach Disadvantages: • file is invalid until preprocessor strips out the frontmatter. Might consider this approach for compatibility with existing 11ty ecosystem. -> This should be exactly the same. My format is an extension of Jekyll/11ty, that works for more than just markdown. Lmk if I’ve misunderstood something though? Perhaps I should clarify or mention 11ty as well as Jekyll. -> Thanks! -> I find it quite useful for generated HTML and Python for example, that it works the same as the Markdown frontmatter but allows for a syntax that is valid without preprocessing. -> Clarifying: in 11ty, you can add these blocks to *any* file type. `.md`, `.html`, `.json`, `.whatever`. It's always triple dashes, regardless of file type, e.g.: ```--- title: Foo tags: [bar] ---``` Not based on file type e.g.: ```&lt;!-- title: Foo tags: [bar] --&gt;``` -> yeah, I get that. with my frontmatter-format all marker syntaxes auto-detect so work with all files, so it’s basically a matter of convenience which you use. imho it would be far easier if we all did this so you can add metadata without breaking existing tools! maybe that wasn’t clear in my readme? perhaps I need to rewrite it. -> i.e., 11ty files like that would be compatible and work in my frontmatter format tools fine. an 11ty file then would not work in the browser without preprocessing. I think it’s usually best to pick syntax per file type. like the files I generate using this library _also_ work with existing tools, e.g. an HTML page works just like it always would in the browser without preprocessing. -> nice -> revised the readme, hope it helps as evidently that wasn’t clear <https://github.com/jlevy/frontmatter-format>",
    "#lab-notes-alex-van-grootel: Stewart Candy is behind the excellent design game Thing From The Future <https://library.teachthefuture.org/wp-content/uploads/2017/01/The-Thing-From-The-Future.pdf|https://library.teachthefuture.org/wp-content/uploads/2017/01/The-Thing-From-The-Future.pdf>",
    "#lab-notes-ben-goldhaber: (5 min brainstorm) thinking about scenarios where I personally want better scenario planning tools • my finances: what are the different situations I'm trying to plan for with my portfolio allocation, in what worlds would I have preferred different risk levels, how important in different scenarios is liquidity to me. • project planning: what were the different scenarios for the fellowship and what would have been decision relevant, both on the micro side: aka if we had had 90% remote fellows, or 10% remote fellows, what changes? and on the macro side: if my timelines for AGI were 2x or 3x as long, what does this imply about staggering the fellowship. • general orientating: what does a given AI future scenario *mean* - if we have 1mil agents running around, how do I think that world is going to look. Scenario planning as sci-fi vignettes. not exhaustive, but in all of these it feels to me like I'm trying to do... some kind of exploration of a fictional world that hasn't yet come to past, so that I can notice and really feel what I care about in that world. Brainstorm inspired by <@U098C1ZB1NU>'s landing page. -> Would love to riff/brainstorm with you on this. VC sometime? -> yah! I'm relatively free this afternoon/tomorrow -> would be fun - oh here's my cal <https://savvycal.com/bengoldhaber/chat-with-ben?d=30> -> Booked! -> Career planning is another use case that comes to mind. -> AI replication in the wild, freebooting, geopolitics thereof Interaction of AI policy with nuclear policy Geopolitics of geoengineering Various biotech shenanigans (especially on food chain and humans) Consequences of various things' prices going rapidly down perhaps unevenly, especially energy, manufactured goods, ... AI-related religious revolutions?? (Maybe too chaotic to even contemplate mapping?)"
  ]
}

-- Model Captures --
>> person_skills
PERSON SKILLS

System
You analyze a person's profile and their Slack messages to infer a high-quality skill list they likely possess.

Evidence priority (highest to lowest):
1) Profile fields authored by the person (expertise, projects, offers, asks, interests)
2) Consistent themes in their messages (deduplicated by thread)

Rules:
- Output ONLY a JSON array of 8–20 concise skill strings (no objects, no extra text).
- Prefer technical, domain, or tool-specific skills. Avoid soft skills.
- Avoid overly narrow/hyper-specific variants (e.g., prefer "CSS animations" over "CSS scroll-driven animations").
- Canonicalize names (e.g., "PostgreSQL" not "Postgres DB").
- Deduplicate and sort by strength of evidence and recency.
- If evidence is thin, return a smaller set (>=3 if possible).

Payload
{
  "profile": {
    "name": "Gordon Brander",
    "expertise": "Scenario planning + futures / founding early-stage startups / product + design / agent sandboxing + privacy / decentralized protocols",
    "projects": "Deep Future - Deep Research but for scenario planning",
    "offers": "Scenario planning/foresight. Founding seed-stage startups / raising seed. App dev. Front-end. Product design.",
    "asks": "Intros to govt/policymakers/industry folks who would benefit from better foresight, better strategic analysis. My theory of change is to empower high-leverage decision-makers to make better decisions resulting in maximum deep benefit. gordonbrander@gmail.com +1 720-375-0022",
    "most_interested_in": "AI Exocortexes: amplifying human intelligence, creativity, and decision-making with AI. Agency means staying in the OODA loop. To keep up with AI, we’ll need to augment ourselves with AI. Currently building Deep Future, an exocortex for thinking more effectively with AI about futures.",
    "confusion": "The proliferation of open models suggests a world inhabited by millions of AI agents, not one singular entity. Under these conditions, reasoning is less an individual activity, more of an emergent network phenomenon. How do we think together with AIs and egregores?"
  },
  "sample_messages": [
    "#lab-notes-ben-sklaroff: <@U093S356KAS> the cooperative license, high-level is that it's roughly MIT license for coops + non-profits, and LGPL for (non worker-coop) for-profits. So just a small bit of legal advantage for the worker coops: <https://github.com/bsklaroff/cooperative-license> -> Holy mackerel, that's amazing! -> I've been tracking the development of open source licenses that try to close the economic loop for a while. Cooperative License is one of the cooler riffs I've seen so far! -> Fair Source is another interesting one. Source-available, free for personal use and non-competing uses. Converts to Apache/MIT after 2 years. <https://fsl.software/> -> <@U098C1ZE2M6> oh wow, you're behind the Cooperative License?! Awesome!",
    "#lab-notes-oly: Had a thought about voting mechanisms. We all have our pet peeves with status quo typical voting mechanisms. They're terrible. But they're tenured. Maybe a lot of the resistance to change comes from a kind of fearful default conservatism. After all, if we shake up our voting system today, what's to stop it getting shaken up (badly) tomorrow? Norms and rituals in general get a kind of potency this way through the aging process, for good or ill. -> Probably a pace layers angle to this as well <https://jods.mitpress.mit.edu/pub/issue3-brand/release/2> -> Lindy practices get folded into lower, slower layers so they won't get lost. The slowness acts as long-term memory, preventing practices from getting lost during generational cycles. -> Francis Fukuyama has some great passages in The Origins of Political Order that hint at an evolutionary biological mechanism for this kind of conservatism -> Basically humans are mimetic animals... monkey see monkey do. -> and this bias toward imitation over innovation gives us a slightly conservative bent. Conservative as in conserving of the status quo. -> This is how institutions form. We copy what works, and keep copying it, and reproduce the institution. \"Civilization advances by extending the number of important operations which we can perform without thinking about them.\" -> so this is cool bc imitation lets us quickly converge around Schelling points of behavior and benefit from the economies of scale of cooperation -> but then sometimes these practices fall out of evolutionary fitness vs the environment -> Fukuyama: &gt; There is something like a law of the conservation of institutions. Human beings are rule-following animals by nature; they are born to conform to the social norms they see around them, and they entrench those rules with often transcendent meaning and value. When the surrounding environment changes and new challenges arise, there is often a disjunction between existing institutions and present needs -> Your OP here made think of this recent article. It pushes me in the direction of \"the reason we can't upgrade our voting systems is simple power dynamics; often times a large bloc believes they stand to lose from any changes in a near-zero-sum game. So when one party does achieve enough political momentum to just seize more power, then do. <https://www.nytimes.com/2025/07/30/us/politics/texas-republican-redistricting.html> -> Another thing people often bring up is simplicity or ease-of-understanding of a voting system as an important property for the perceived credibility of the system. If your voters don't understand how their votes are counted, they become suspicious.",
    "#lab-notes-gordon: Back in Nairobi :flag-ke:",
    "#lab-notes-gordon: doing a little moodboarding as pre-work for making a landing page <https://www.are.na/share/PvGGXHm>",
    "#lab-notes-gordon: getting inspired by amber/orange FUI and euroracks",
    "#lab-notes-gordon: I think the JTBD for Deep Future is to feel like you have 10x more leverage",
    "#lab-notes-gordon: Just-According-to-Keikaku-as-a-Service -> JAcKasS?",
    "#lab-notes-gordon: Another lens: AI Scenario Planning as an Engine-Building game -> 'Engine-building'? -> @Oly this post has a pretty good description: <https://interconnected.org/home/2022/02/11/wingspan> &gt; _Wingspan is what’s known among serious gamers as an “engine-building game,” which means that as the game goes on, the combination of birds you play becomes more and more efficient at generating points each turn, like an engine running faster and faster. Your cuckoo lays eggs, and the eggs not only give you points but make it possible to play more birds, which also give you more points but have their own powers that generate points in other ways._ -> I want to get away from scenario-planning-as-a-workshop-activity, and toward \"live\" AI-driven scenarios that are continually evolving and part of your OODA loop. -> So, maybe should feel like a scenario engine game. The more inputs you feed it, and the more actions you take, the better the scenario engine gets, and the better your inputs and actions become -> My understanding is that current scenario planning games are typically of a role playing variety, and that they are most useful when players are trying to faithfully model their targets, rather than truly optimizing against one another and the game itself. I’m not an expert here though. I’d love to hear from one but I’m not sure who to reach out to -> AI 2027 style wargames are just one practice that comes from the scenario planning world -> there are a bunch of others -> the area I'm personally more interested in has to do with structural analysis -> (I love engine building games; hadn't heard that term and had sometimes called them 'deck building' or similar) -> I like them too! I think what works about that game design is that you get this felt sense of the flywheel building up momentum, and you feel more and more agency as you bootstrap it -> Yep, and trying out different runs to find cool combos and interactions. Dominion comes to mind, also Machi Koro, others. -> aaaahhh I love machi koro -> I guess for 'realistic' scenarios, the cool combos and interactions might be informative of critical hingey dynamics or bottlenecks or something -> btw, how are you all up at... 4am pacific?? -> (Well, I just landed in London, don't know about Tamera) -> (Port Royal is a wonderful engine building game that sometimes gets a bit broken just at the end which is kinda fun) -> right, I think the engine analogy here might be discovering strategic leverage points and expanding your own agency as you identify ways to push on them -> That silica mine story feels like an almost-example. I'm not sure it's an actual leverage point, but assuming hypothetically for a second that it is... you could imagine your personal AI scenario system uncovering that bottleneck and its relationship to AI safety and surfacing that to you. You make that move, and now new scenarios open up because you control that chokepoint and have altered the driving forces around chip manufacture. -> Right... As a comparison point, there were those who 'discovered' the importance of compute before most of the world. They did that by... I don't think quite 'scenario planning'? But it feels pointed at the same thing as you're describing it. And there have been various corollaries and further implications of that, and we don't yet know how it'll shake out. -> I used to enjoy casual MTG drafts, so I can appreciate this vibe I imagine it could be used as a strategic tool as well, but to the extent that players are optimizing against the game, it needs to be reflecting many tiny little details of reality accurately in order to be giving them good info. Or at least, the game could try to present some information to calibrate the player about how broken it is at any given time -> &gt; (Well, I just landed in London, don’t know about Tamera) Just having a hard time sleeping :upside_down_face:",
    "#lab-notes-gordon: Another lens: ritual design <https://newsletter.squishy.computer/p/ritual-technology>. Our goal is to (1) generate strategic insights with AI (2) get those insights into your OODA loop. So we need to close the loop. Part of what we're doing is designing a daily/monthly/quarterly ritual. -> There are some pretty good ritual design tools here <https://ritualdesign.net/>",
    "#lab-notes-gordon: Orientation is the crux of the OODA loop: <https://newsletter.squishy.computer/p/tools-for-thought-in-your-ooda-loop> &gt; For Boyd, the most important factor for survival is your sense-making system. This is the thing that keeps you oriented. There’s a reason that “orient” is the biggest circle on the OODA diagram. We survive by making sense. When orientation breaks down—when you are _disoriented_—you lose your agency. Your model is wrong. You pay attention to the wrong things, your actions stop making sense, you flail. -> Chet Richards goes deeper into this: <https://www.agileleanhouse.com/lib/lib/Topics/OODALoop/Boyds_OODA_Loop_Its_Not_What_You_Think_I%20(1).pdf> -> I think I basically agree for individuals (humans at least, though it might be that the margins would change for quite different types of individual). But I feel compelled to observe that for orgs and other formal/informal groups, DA seem equally or more difficult and marginally important to improve. (Plus I get confused how to relate OODA to more long-term processes like learning and growth, either of individuals or groups.) -> haha having worked at Google, I can confirm that DA is indeed a challenge for large orgs -> IMO one way OODA gets folded into long-term growth is through updating the source code you use for orientation -> Orient = the long term memory of the system. The DNA. -> Tradition, team culture, organizational structure, values, tacit knowledge, shared language etc go here -> That sounds exactly right to me (except for the DNA analogy!), where D can also get learning improvements (perhaps especially for exploratory activities?) -> Oh I have all sorts of thoughts here. One thought is, if you can’t D or A, maybe you haven’t actually O’d? -> Or at least, your O isn’t entirely up to the task you’re facing right now -> &gt; if you can’t D or A, maybe you haven’t actually O’d? &gt; For a group, or individual? There's definitely something to co-orientation in a group. (If you include 'and therefore this is the decision-making and binding procedure we should use' then I guess it's approaching sufficient for DA as well but that seems to cut out a lot of solution space, to me.) -> &gt; For a group, or individual? Both? Orientation is a continual process - like Gordon said: &gt; Tradition, team culture, organizational structure, values, tacit knowledge, shared language etc go here Which are all iterated on continuously. Nobody should be choosing an absolutely final “decision-making and binding procedure” at any point, if you ask me - you’re just running an experiment to assess the quality of your orientation at this moment -> Which part of why I find stuff <https://aiforhumanreasoning.slack.com/archives/C0983TKV3RP/p1754024483403429?thread_ts=1753991440.654439&amp;cid=C0983TKV3RP|like this> so heinous - it seems like the whole organization loop has entirely fallen apart -> Another Boydian idea I'm obsessed with is \"fast transients\". OODA is not necessarily about looping faster, but about controlling the tempo of the game by creating plot twists: > In other words, the F-86 pilot could gain the initiative by executing _fast transients_ – changes in the tactical situation that forced his opponent to respond; or he could respond more quickly than his opponent could change things up. <https://theleanthinker.com/2013/08/19/fast-transients/> -> so like, for example, we focus a lot on the speed of AI takeoff (_FOOM_) -> but I think maybe it's worth pondering how little agency we have within any inhuman timescale -> inhumanly fast -> inhumanly slow (ai can be inhumanly patient) -> The timescale at which you can make sense is the timescale in which you have agency. -> and one reason I like scenario planning methods is that they encourage us to expand our sense of agency within time both backwards (identifying long-term forces) and forwards (larger map of the possible) -> I think I’m going to try to actually sleep now, but if you want to chat more at some point this is my google calendar link: <https://calendar.app.google/WvCJwS1ipVrTQG4i9> -> Earlier today <https://aiforhumanreasoning.slack.com/archives/C0983TKV3RP/p1754139544740969?thread_ts=1753991440.654439&amp;cid=C0983TKV3RP|I wrote up some conclusions> from my investigating / thinking about what’s going on with slack, in case you guys wanted to check it out.",
    "#lab-notes-gordon: Dreamy jetlag productivity radio <https://www.nts.live/shows/early-bird/episodes/early-bird-arushi-jain-18th-july-2025> -> This track is so good. International Anthem does not miss. <https://intlanthem.bandcamp.com/track/industry>",
    "#lab-notes-gordon: DARPA mention got me thinking about FLF as a DARPA for lightcone alignment. Original DARPA mission: \"To create and prevent strategic surprise.\" -> Also Licklider's approach of \"fund people not projects\" and \"fund people who would work on the problem whether or not they were funded\" -> Book: The DARPA Model for Transformative Technologies <https://www.openbookpublishers.com/books/10.11647/obp.0184> -> and Ben Reinhardt's manual for creating more DARPAs <https://blog.benjaminreinhardt.com/parpa> -> “DARPA for lightcone alignment” is such a delicious phrase :heart: -> The prevent strategic surprise framing is such a classic—we put it into the UKAISI mission at launch. Other evocative orgs with interesting ways of operating: • <https://jeffburke.substack.com/p/sutter-hill-ventures-the-silent-builders|Sutter Hill Ventures> • <https://goodscience.substack.com/p/a-scrappy-complement-to-fros-building|BBN> -> this is such a better brand description I will see what I can do :saluting_face:",
    "#lab-notes-gordon: A meditation on creativity from the most creatively prolific person I know <https://shimmeringvoid.leaflet.pub/3lvf6cggrt22x/l-quote/0_0-0_336#0_0|https://shimmeringvoid.leaflet.pub/3lvf6cggrt22x/l-quote/0_0-0_336#0_0>",
    "#lab-notes-gordon: I like this... using AI agents as market intermediaries <https://open.substack.com/pub/meaningalignment/p/market-intermediaries-a-post-agi|https://open.substack.com/pub/meaningalignment/p/market-intermediaries-a-post-agi>",
    "#lab-notes-gordon: Bookmarking this to look at later",
    "#lab-notes-gordon: :shushing_face: domain registered: possible.systems -> also scooped up deepfuture.now",
    "#lab-notes-gordon: not committed yet, but I like the cheeky double-meaning",
    "#lab-notes-gordon: Noting as an example of the kind of *actionable* strategic insight that falls out of forecasting/scenario planning. -> An actionable insight produces meaningful strategic leverage. • can plan around it (A tier) • can influence outcome (S tier) -> “The best way to predict the future is to invent it” -Alan Kay",
    "#lab-notes-gordon: <https://bsky.app/profile/timkellogg.me/post/3lvimbxsdws2k|https://bsky.app/profile/timkellogg.me/post/3lvimbxsdws2k>",
    "#lab-notes-gordon: RAND: Enhancing Strategic Planning with Massive Scenario Generation",
    "#lab-notes-gordon: landing page hacking this am (WIP) -> Minor: \"Deep future is deep research but ...\" is pretty clunky wording, just in some small way fails to draw me in. I think it's the repetition of \"deep\" that makes it feel like a slog. Possibilities I immediately wonder about (but are probably worse in other ways): • \"Deep Future is a personalized research assistant for scenario planning\" • \"Like deep research, but for scenario planning.\" (Cutting the product name here, assuming it will show up in enough other places) -> Thanks! Still workshopping. I currently have \"Deep Future is deep research for scenario planning. Stress-test your strategy against thousands of possible futures with our AI-powered scenario generation engine.\" -> Probably better: \"Like deep research for scenario planning. Stress-test your strategy against thousands of possible futures with our AI-powered scenario generation engine.\"",
    "#lab-notes-gordon: I keep re-learning this lesson over and over: block it out before you jump into detail work.",
    "#lab-notes-gordon: in this case, start with the headlines and copy",
    "#lab-notes-gordon: Speaking of... for anyone thinking of pitching VCs eventually, this is the best resource on building a pitch deck I have yet found: <https://visionquest.spacecadet.ventures/phases/pitch> -> TLDR: - :fishing_pole_and_fish: Hook - :warning: Problem - :crystal_ball: Why Now - :brain: Solution - :package: Product - :chart_with_upwards_trend: Traction - :earth_americas: TAM + Roadmap - :wrestlers: Competition - :dizzy: Team - :truck: GTM - :file_folder: Business Model - :dollar: Fundraise + Financials - :mega: CTA + Purpose",
    "#lab-notes-gordon: <@U098C1ZHJMN> and I were talking about Carlota Perez's Technological Revolutions and Financial Capital and her AI-driven green growth thesis <https://carlotaperez.org/wp-content/downloads/new-book/outputs/working-papers/BTTR_WP_2017-1.pdf> -> Technological Revolutions and Financial Capital <https://en.wikipedia.org/wiki/Technological_Revolutions_and_Financial_Capital> -> This is one of those ur-theses powering silicon valley venture funding cycles -> Mechanism: when the previous technological paradigm runs dry, leaving speculative capital becomes desperate for returns. Sensing the immense promise of the new technological paradigm, and smelling profits, investors begin flooding the space with capital, leading to a boom, a bubble, and a bust. But the crash leaves something important behind: *infrastructure*. Bubbles fund the deployment of canals, factories, railroads, shipyards, highways, electrical grids, cellular towers, broadband. This infrastructure becomes the foundation around which society rebuilds itself after the crash. -> Perez identifies 5 previous technological revolutions -> p.s. this blog series by Perez is amazing <https://medium.com/iipp-blog/second-machine-age-or-fifth-technological-revolution-part-2-db42863a8df8> -> Her thesis for the last 10 years is that the climate and energy transition would be driven by AI. \"Smart green growth\". Basically atoms getting replaced with bits, and intelligence real-time allocating energy and resources to where they are needed. Like packet-switching for the economy. -> Also, you can kind of see the beginnings of her thesis playing out: AI is funding the massive deployment of clean energy (step 0). If she's right, this should lead to an abundance of intelligence and energy, which will become the basis for the next civilization.",
    "#lab-notes-gordon: Bookmarking this CRM of scenario planning contacts <@U098C1ZHJMN> and I are collaborating on <https://docs.google.com/spreadsheets/d/1TX6gOsGq5iI6mYzX8edUB5TaHr_SPbvZst28xgKjxRE/edit?gid=0#gid=0>",
    "#lab-notes-gordon: Great article and a good example of advice that would benefit from some scenario planning. Imagine how much better this advice would be if it was tracked across three scenarios / if the claims were mapped back to driving forces, etc <https://80000hours.org/agi/guide/skills-ai-makes-valuable/|https://80000hours.org/agi/guide/skills-ai-makes-valuable/> -> Their distribution is amazing. I have a somewhat similar article on <https://nunosempere.com/blog/2025/07/07/dear-forecaster/>, but read by way fewer people -> Yeah 80k have an absolutely massive media machine. It’s pretty impressive. -> Bookmarking your article!",
    "#lab-notes-gordon: It’s weird and hopeful that LLMs are so align-able. I mean, it opens up other vectors of attack, like mass propaganda, but these seem to me to exist in a different threat class. <https://bsky.app/profile/timkellogg.me/post/3lve6iilnpc2k|https://bsky.app/profile/timkellogg.me/post/3lve6iilnpc2k> -> I wonder if there may be a generalizable pattern here? Things that get the bulk of their variety (in the cybernetics sense) from human culture will inherit human values, because culture has requisite variety in the system <https://en.wikipedia.org/wiki/Variety_(cybernetics)#Law_of_requisite_variety|https://en.wikipedia.org/wiki/Variety_(cybernetics)#Law_of_requisite_variety> -> Would not hold for RL or agents that share more coevolutionary history with each other than with humanity",
    "#lab-notes-gordon: superintelligence -&gt; superagency",
    "#lab-notes-gordon: humanity chart a path from a -&gt; b challenge",
    "#lab-notes-gordon: These days, I'm always running a meta-process in my head where if I start working on a task, I stop, frame the task as a prompt, and let the AI take a first crack at the problem. Sometimes I even run the meta-meta-process and let the AI define what the task should be from my stream-of-consciousness braindump. -> The future of work feels like it's going to be figuring out which layer of meta you should be on for a given task. More layers as models get smarter. -> yes, I think this is underused. I'm a huge supporter of getting your ego out of the way and use AI to help you think/plan better",
    "#lab-notes-gordon: My code repos are becoming 80% markdown: • `notes/` raw stream of consciousness notes and prompting • `specs/` specifications co-created with AI. • `prompts/` md files used to prompt agents or AI-driven parts of system My general vibe-coding workflow: 1. chat with Claude Code about requirements, dump to `notes/` 2. @include notes, prompt Claude Code to write a spec from notes, dump to `specs/` 3. @include spec, prompt Claude Code to write unit tests from spec 4. @include spec and unit tests, prompt Claude Code to write module -> I find that versioning context along with the code is super helpful. You can regenerate modules from spec, and you can include parts of notes or specs in prompts for other features that are related.",
    "#lab-notes-gordon: figma-ing this am. I'm less happy with this mockup, but I think it's a \"push past the local maxima\" situation. We'll get there. -> I also feel like I'm overshooting, but I'm going to let myself push a little further, and then dial back to a satisficing first pass -> reminder: goal for this stage is just to have something basic to blast out to Substack/Bluesky to see who bites. -> the ideal version of this style would feature webgl animations, but I plan to liven up these static pointillist graphics with some CSS scroll-driven animation <https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_scroll-driven_animations> -> Better. Removed the crop, let the background bleed through. We'll get there. -> really beautiful, I like the slogan Think N steps ahead (though maybe N steps doesn't translate as well outside of technical groups? still, cool) -> Some detail work left to do (copy, spacing, responsive layout), but I think this is good enough to jump into coding",
    "#lab-notes-gordon: Reached out to Mick Costigan (head of Salesforce Futures, former GBN, WEF AI Policy team) -> Mick is down to meet.",
    "#lab-notes-gordon: Reached out to Karl Schroeder (scenario planning, futurist, hard sci-fi author, author Stealing Worlds) -> He's a yes. Meeting next week.",
    "#lab-notes-gordon: Created DigitalOcean droplet for Deep Future project",
    "#lab-notes-gordon: huh, a friend sends this along and says \"scenario visualization!\" <https://arstechnica.com/ai/2025/08/deepmind-reveals-genie-3-world-model-that-creates-real-time-interactive-simulations/>",
    "#lab-notes-gordon: Got around to reading that Eric Drexler post <@U093S356KAS> shared and man is it good <https://aiprospects.substack.com/p/ai-options-not-optimism|https://aiprospects.substack.com/p/ai-options-not-optimism> &gt; Here’s an easy and effective way to misunderstand our situation: &gt; &gt; “We’re on a path to superintelligence, which may be impossible to control and hence likely to destroy us. Therefore, we can’t assume that powerful AI will help us solve seemingly intractable problems. To assume otherwise would be naïvely optimistic, and with so many critical problems, our odds of success are poor.” &gt; &gt; Here’s a better way to think about it: &gt; &gt; “We’re on a path to superintelligence, which must be steerable, or nothing else matters. Therefore, in every future that matters we can assume that powerful AI will help us solve seemingly intractable problems. Our options in a hypercapable world are largely unexplored, and our overall odds of success are unknown.” &gt; &gt; In this situation, debating odds of success is pointless, exploring options is crucial, and optimism is irrelevant. &gt; &gt; Participants don’t think like spectators. -> yah! &gt; Participants don’t think like spectators. I strongly feel this vibe - which I didn't understand a few years ago when I was working on forecasting tech - that when you're a live player, decision maker, you're really not thinking about odds of success, you're finding the path to making something work and then doing whatever it takes to get there. -> this does feel kind of like a \"two cultures\" thing, except along a different axis: trying to understand vs trying to effectuate <https://effectuation.org/the-five-principles-of-effectuation>",
    "#lab-notes-gordon: This AM, jotted down a few ideas on using storylets to generate atomic micro-scenarios <https://emshort.blog/2019/11/29/storylets-you-want-them/> -> E.g. could • Generate a storylet for each driving force • Compose storylets by combining them pairwise, and having AI write a new storylet that is the combination of both. • Ground scenario narrative generation by querying storylets that match the cluster of forces within a given scenario window and using that as a starting point for the scenario skeleton. -> Feels interesting (having just skimmed the start of the post). One question that comes to mind is \"where does the force-towards-realism come in?\". Like, perhaps there are some pairs-of-storylets that we should actually be unwilling to combine, because it's unrealistic to face futures that both X and Y? -> here's what I'm envisioning. The physics grounding the generation comes from the scenario methods themselves (driving forces identification, STEEP analysis, force matrix analysis, etc). Storylets come in much later when you've already done the structural analysis and want to package strategic insights into a memorable artifact, such as a vignette or script for a generated video. At this point, the challenge shifts to procedural narrative generation (transform numbers and stats into stories). Storylets might be one potential mechanism because: • Storylets have a mechanism for encoding prerequisites (\"this can only be included if these preconditions are true\") • Storylets let us break the narrative generation problem down into smaller pieces and then compose those pieces. This lets us ground the narrative generation in three ways: (1) when expanding forces into storylets, we have a smaller task that is more grounded by virtue of being small (2) by encoding prerequisites (3) during the synthesis pass, when combining storylets into a narrative, we have more of a summarization task vs a generation task. -> storylets are sort of like baseball-card-sized narrative snippets that are arranged into a tech-tree like constraint system so my hypothesis is that starting with them might result in more grounded narratives vs just one-shotting a narrative via LLM... buuuuuut this is all just a napkin sketch idea at this point -> yeah, I think I'm kind of seeing you -> I do think \"one-shotting a narrative via LLM\" is unlikely to be the right move ! -> also tbh the narrative aspect of scenarios is less compelling for me than the more structural analysis stuff (identifying forces, where they intersect, etc). More like icing on the cake. -> oh, and while its top of mind, another way scenerios might be generated is through war gaming (tabletop exercises like AI 2027), either with agent vs agent, or agent vs humans. -> play through, then summarize -> or some combination of the two (microscope has a mechanic that hybridizes these ideas somewhat, where you place storylets on a timeline, and can play through parts of the timeline you're most interested in)",
    "#lab-notes-gordon: set the channel topic: Epistemic status: thinking out loud",
    "#lab-notes-gordon: Asked Claude Opus what questions our email waitlist form should have. Result: -> Here's a comprehensive list of waitlist signup questions for Deep Future, organized by purpose: ## Basic Qualification Questions 1. **Email address** (required) 2. **Full name** (required) 3. **Organization name** 4. **Your role/title** 5. **Organization type** (dropdown: Corporation, Government agency, Military/Defense, VC/Investment firm, Hard-tech startup, Consulting firm, Think tank, Other) 6. **Organization size** (dropdown: 1-10, 11-50, 51-200, 201-1000, 1000+) ## Current State & Pain Points 7. **How does your organization currently approach long-term strategic planning?** (multiple choice, select all that apply) - Internal strategy teams - External consultants - Scenario planning workshops - Trend analysis tools - We don't have a formal process - Other: [text field] 8. **What are your biggest challenges with strategic planning today?** (multiple choice, select all that apply) - Too time-consuming - Difficult to identify blind spots - Hard to track changing conditions - Limited to a few scenarios - Lack of methodological rigor - Difficulty communicating scenarios to stakeholders - Other: [text field] ## Use Case Discovery 9. **What strategic decisions would you use Deep Future for?** (open text, optional) 10. **How often do you need to create or update strategic scenarios?** - Monthly - Quarterly - Annually - Ad-hoc/project basis - Not sure yet ## Budget & Pricing Research 11. **What is your annual budget for strategic planning tools and services?** - Under $10K - $10K - $50K - $50K - $250K - $250K - $1M - Over $1M - Not sure/prefer not to say 12. **How would you prefer to pay for a tool like Deep Future?** - Monthly/annual subscription - Per project/scenario - Enterprise license - Usage-based pricing - Other: [text field] ## Feature Prioritization 13. **Which capabilities are most important to you?** (rank top 3) - Automated scenario generation - Real-time monitoring of signposts/indicators - Collaborative scenario development - Integration with existing data sources - Strategic recommendation engine - Scenario visualization and reporting - API access for custom integrations ## Timeline & Engagement 14. **When would you need a solution like this?** - Immediately - Within 3 months - Within 6 months - Within a year - Just exploring 15. **Would you be interested in:** (checkboxes) - Early access to the beta - Participating in a 30-minute user interview - Co-developing features as a design partner - Receiving our strategic foresight newsletter ## Optional Open-Ended 16. **What would make Deep Future a \"must-have\" tool for your organization?** (open text, optional) -> Not bad -> I'll have to hack on these a bit, but it's a good starting point",
    "#lab-notes-gordon: Cool talk <https://overcast.fm/+AA9poeScDZw|https://overcast.fm/+AA9poeScDZw> -> Chollet really hammers on the difference between static and fluid intelligence, and wants an AGI goalpost anchored on fluid intelligence. As someone with weak static intelligence and strong fluid intelligence, I like this framing for AGI :wink: -> My takes on something like the static/fluid intelligence question as it relates to superintelligence: <https://strangecities.substack.com/p/knowledge-reasoning-and-superintelligence|https://strangecities.substack.com/p/knowledge-reasoning-and-superintelligence> (Feel free to ignore!) -> added to my readwise queue!",
    "#lab-notes-gordon: Putting on some focus music and coding up that landing page <https://eremiterecords.bandcamp.com/album/mondays-at-the-enfield-tennis-academy>",
    "#lab-notes-gordon: Little intro deck I made for a scenario planning workshop featuring [super secret undisclosed participants] a few years ago. Would be happy to give this as a lightning talk if anyone is interested. -> Nice, yes, interested! -> If nobody else bites, I'm also happy to chat about scenario methods 1:1",
    "#lab-notes-herbie: &gt; However! AI reads the notes when sourcing information/browsing the web, and is ~guaranteed to do so if it reads the page. And a single community note readable by AI can propagate into influence on AI-written answers to questions/content in many places around the web. This seems like an insanely powerful leverage point, given that LLMs will be doing the vast majority of cultural production going forward.",
    "#lab-notes-herbie: That's my sense at least. Given LLMs can produce images and text faster and cheaper than we can (by orders of magnitude), I have a hard time seeing futures where they aren't driving the OODA loop of culture. And if that's the case, maybe the primary audience for community notes is the AIs.",
    "#surface-area-nuno: I'm also trying a new 21K follower twitter account as a means of increasing our distribution. It previously belonged to a prediction market competitor, so hopefully there is a bit of overlap. Retweets particularly appreciated this week, even from small accounts, to train the Twitter algorithm a little bit <https://x.com/XriskFYI/status/1952431521429893560> -> Sentinel shilling on Substack this AM",
    "#surface-area-nuno: <https://en.wikipedia.org/wiki/Folie_à_deux> ; shared psychosis. Could be relevant to predicting AI superpersuasion -> Another signal might be tightness of the feedback loop between person and AI. Generally, the tighter the loop, the greater the synchronization between systems, and the part of the system with the most variety will control the direction of the combined system <https://en.m.wikipedia.org/wiki/Variety_(cybernetics)#Law_of_requisite_variety|https://en.m.wikipedia.org/wiki/Variety_(cybernetics)#Law_of_requisite_variety> -> yeah this is def why it's happening now that Chat-GPT has memory. Tighter feedback loop. Imagine if memory + personalization is 10x improved next year, there could be a lot of danger about falling into a blackhole with an AI",
    "#lab-notes-sethkillian: My scratchpad notes on something I've been stuck on for a while re: potential recursive effects of prediction, and how they can bound utility and effectiveness over time. <https://docs.google.com/document/d/1cWS9Z0D9X-dbt2TLUbPKrw3_nhgutd8t7A9TkXZyMj0/edit?tab=t.0> Grateful to a number of fellowship folks who have offered reflections and encouragement, <@U098C1ZDD16> <@U093W3TNLFM> and <@U093S2ZM20N> Not sure this is a robust fellowship project but I will take a run at turning into a paper if only for my own amusement. Related phenomenon definitely show up in other domains, but thoughts, additional references, or deflations very welcome. -> There's also some work on this from the \"strategic machine learning\" literature, you may find it interesting! Here's an example: <https://arxiv.org/pdf/2002.06673> my big takeaway is that sometimes we should think about prediction as an equilibrium problem and not an optimization problem -> This sentiment about equilibria matches my intuition quite well -> Added paper to notes, will read through on plane this weekend :heart: -> I think there may also be a nuance here about open worlds vs closed worlds. In a closed world, successful prediction might have a washout effect over repeated games between a fixed set of players. In an open world, new players, new information, new interactions, and new knock-on effects will continually open up new competitive landscapes where prediction can be newly useful -> Agree that any effect is domain sensitive",
    "#lab-notes-jlevy: so this is in the weeds a bit but relevant to some of these tools: I’ve been writing and formats for metadata and asset bundling for Markdown and other documents in a way that is useful for AI apps. two repos: <https://github.com/jlevy/frontmatter-format> -> Quick thought: 11ty, one of the more popular static site generators, allows for yaml blocks at the top of any kind of file: ```--- title: Foo tags: [bar] ---``` The syntax is the same (---) regardless of file type. There are several other static site generators that copy 11ty’s approach. Advantages: • same syntax = familiarity • copy-paste-able • composable… same frontmatter could be catted to any file without worrying about wrapper • compatibility with 11ty and other static site generators that copy this approach Disadvantages: • file is invalid until preprocessor strips out the frontmatter. Might consider this approach for compatibility with existing 11ty ecosystem. -> This should be exactly the same. My format is an extension of Jekyll/11ty, that works for more than just markdown. Lmk if I’ve misunderstood something though? Perhaps I should clarify or mention 11ty as well as Jekyll. -> Thanks! -> I find it quite useful for generated HTML and Python for example, that it works the same as the Markdown frontmatter but allows for a syntax that is valid without preprocessing. -> Clarifying: in 11ty, you can add these blocks to *any* file type. `.md`, `.html`, `.json`, `.whatever`. It's always triple dashes, regardless of file type, e.g.: ```--- title: Foo tags: [bar] ---``` Not based on file type e.g.: ```&lt;!-- title: Foo tags: [bar] --&gt;``` -> yeah, I get that. with my frontmatter-format all marker syntaxes auto-detect so work with all files, so it’s basically a matter of convenience which you use. imho it would be far easier if we all did this so you can add metadata without breaking existing tools! maybe that wasn’t clear in my readme? perhaps I need to rewrite it. -> i.e., 11ty files like that would be compatible and work in my frontmatter format tools fine. an 11ty file then would not work in the browser without preprocessing. I think it’s usually best to pick syntax per file type. like the files I generate using this library _also_ work with existing tools, e.g. an HTML page works just like it always would in the browser without preprocessing. -> nice -> revised the readme, hope it helps as evidently that wasn’t clear <https://github.com/jlevy/frontmatter-format>",
    "#lab-notes-alex-van-grootel: Stewart Candy is behind the excellent design game Thing From The Future <https://library.teachthefuture.org/wp-content/uploads/2017/01/The-Thing-From-The-Future.pdf|https://library.teachthefuture.org/wp-content/uploads/2017/01/The-Thing-From-The-Future.pdf>",
    "#lab-notes-ben-goldhaber: (5 min brainstorm) thinking about scenarios where I personally want better scenario planning tools • my finances: what are the different situations I'm trying to plan for with my portfolio allocation, in what worlds would I have preferred different risk levels, how important in different scenarios is liquidity to me. • project planning: what were the different scenarios for the fellowship and what would have been decision relevant, both on the micro side: aka if we had had 90% remote fellows, or 10% remote fellows, what changes? and on the macro side: if my timelines for AGI were 2x or 3x as long, what does this imply about staggering the fellowship. • general orientating: what does a given AI future scenario *mean* - if we have 1mil agents running around, how do I think that world is going to look. Scenario planning as sci-fi vignettes. not exhaustive, but in all of these it feels to me like I'm trying to do... some kind of exploration of a fictional world that hasn't yet come to past, so that I can notice and really feel what I care about in that world. Brainstorm inspired by <@U098C1ZB1NU>'s landing page. -> Would love to riff/brainstorm with you on this. VC sometime? -> yah! I'm relatively free this afternoon/tomorrow -> would be fun - oh here's my cal <https://savvycal.com/bengoldhaber/chat-with-ben?d=30> -> Booked! -> Career planning is another use case that comes to mind. -> AI replication in the wild, freebooting, geopolitics thereof Interaction of AI policy with nuclear policy Geopolitics of geoengineering Various biotech shenanigans (especially on food chain and humans) Consequences of various things' prices going rapidly down perhaps unevenly, especially energy, manufactured goods, ... AI-related religious revolutions?? (Maybe too chaotic to even contemplate mapping?)"
  ]
}

Output
["Scenario planning","Strategic foresight","Futures research","AI-driven scenario generation","Scenario analysis (STEEP, driving forces)","Multi-agent systems (AI agents)","Agent sandboxing","Large language models (LLMs)","Prompt engineering","Product design","Front-end development","Figma","WebGL","CSS animations","Static site generators (11ty, Jekyll)","Markdown frontmatter","Decentralized protocols","Seed-stage startup founding"]

-- Parsed inferred skills --
- Scenario planning
- Strategic foresight
- Futures research
- AI-driven scenario generation
- Scenario analysis (STEEP, driving forces)
- Multi-agent systems (AI agents)
- Agent sandboxing
- Large language models (LLMs)
- Prompt engineering
- Product design
- Front-end development
- Figma
- WebGL
- CSS animations
- Static site generators (11ty, Jekyll)
- Markdown frontmatter
- Decentralized protocols
- Seed-stage startup founding

-- Applied to DB: false

-- Skills (after) --
- Agent sandboxing
- AI-assisted code generation & unit-test generation
- AI-driven scenario generation
- CSS scroll-driven animations
- Decentralized protocols
- DigitalOcean deployment
- Figma
- Front-end development
- Futures forecasting
- LLM orchestration and agent workflows
- Markdown and frontmatter metadata
- Privacy engineering for AI agents
- Product design
- Prompt engineering for LLMs
- Scenario planning
- Scenario visualization
- Seed-stage fundraising and investor outreach
- Static site generators (11ty / Jekyll)
- Structural analysis (STEEP / driving forces)
- Wargaming and tabletop exercises
- WebGL