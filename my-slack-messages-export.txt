<flf_fellowship_slack>

===== Tue, Jul 29, 2025 =====

--- #general ---
Alexander G: Comment under here <!channel>
  └ Alexander G: I am writing a PRD on a reality gaming engine.
  └ Anand S: @Ben Sklaroff and I are working on a v1 for multi-agent coordination. Idea: groupchat integrated with Claude that helps you plan when to meet to hangout. Claude is called and then it explodes into 1:1 chats with each individual in the groupchat. Synchronously updates its internal understanding of the best time to meet while it coordinates with everybody. Hoping to keep it well constrained enough that we can finish the prototype by lunch. more coders or anyone else to help flesh out the idea welcome.
  └ Kai S: @Rob Gordon and I are working on roadmap for simulations and a simple MVP of an LLM-ABM for emergent dynamics in trading
  └ [ME] Matthew B: The Distillation Toolbox We have built a doc that lists sources, links, ideas, etc. related to: > We cover distillation in the broad sense: Searching from sources of all kinds, extracting information, and finding true pieces of information in useful ways for specific use cases. https://docs.google.com/document/d/1Re4CgEUWppjHhRqEJYixj5inEs2oNHH6cTk3BKyVZ78/edit?pli=1&tab=t.1gqrwq6luvgk
  └ Paul F: @Paul de Font-Reaulx @Maximilian Kroner Dale @Luke Hewitt and @Steven Isley are working on designing an evaluation standard for how LLMs affect people's views as compared to how they are affected by deliberative polling, and an experiment testing that by comparing the relative effects
  └ Steven I: In the US, proposed federal rules must go through a rule-making process where the public is invited to submit comments. All this data is publicly available. Exploring how to build a tool that would help regulators understand the feedback, update the rule, and share out the changes.
  └ Joshua L: @Agita Pasaribu @Jamie Joyce @Nuño Sempere @Alejandro Botas we were exploring many aspects of information distillation https://aiforhumanreasoning.slack.com/archives/C0943H5NLRG/p1753816687573819
  └ Emma K: I'm turning my notes on a bunch of ways AI could augment the policy process (from deliberation, design, implementation, adaptation etc) into a set of concrete projects, and roadmapping one or two of the most tractable/exciting -- if anyone wants to help with evaluating ideas/roadmapping them I'm in the corridor next to the main whiteboard. Will also share a rough doc at some point.
[ME] Matthew B: agent-based human simulator company: https://aaru.com/


===== Thu, Jul 31, 2025 =====

--- #general ---
Kathleen F: Hi everyone! I hope everyone’s travels are going well, that you’re all getting some appropriate rest after an intense few days, and that you’re excited to keep the progress going! Just some notes on the next steps regarding fellowship programming, x-posted from email (see thread):
  └ Kathleen F: We’ve put together a programming overview document (https://docs.google.com/document/d/1ezZiO1uXUa8aTmJfdZMTe0dRut8HVk0jzukz6W-LNDE/edit?usp=sharing) that outlines what we’re offering and our approach. Most importantly, we see this first month as a collaborative experiment and expect to adapt based on your feedback. We realize we may have communicated about the 9:30am sessions during the retreat in a way that made them sound more required than we intended. To clarify: the fellowship programming is intended to support your progress. While we strongly encourage participation (especially the first week as we all calibrate), we recognize you know your working styles and constraints best.
  └ Kathleen F: Here’s a visual of the cohort activity scheduling
  └ [ME] Matthew B: Can we (should we?) Create a meta-fellowship channel or something, to talk about process, etc.?
  └ Kathleen F: We are planning to run these sessions at 9:30 and 1:30 PT starting tomorrow, but are expecting many people to miss these sessions due to travel / rest. Just want to keep the momentum going!
  └ Kathleen F: good idea, thanks Matt
  └ Kathleen F: I created <#C098WPQGRUZ|> for this purpose
Nathan Y: Here is a 2 minute poll for the stuff I’m doing about community notes. Would value people’s votes. https://viewpoints.xyz/polls/10x-community-notes
  └ Josh J: I feel like I’d value an optional “weigh my opinion low / high / very high” for people to use when appropriate. Someone might eg “know the answer” but that won’t show up in this poll (of course they optimally message you about that, but there’s also versions less extreme than knowing).
  └ Nathan Y: You’d expect one question which sort of says something about how much you feel like you know about this topic
  └ Nathan Y: Almost like a demographic question?

--- #lab-notes-alyssia ---
Alyssia J: Feeling excited! First time I’ve ever made a lab notes channel like this so it feels cool. Working on scoping a few projects to ship quickly in the next week or two-ish, starting with getting our community notes bot live on X! Other areas I’m looking to scope in next: • An experiment in coordination/epistemics for specs, h/t @user and his DARPA excitement! • Epistemic evals (epistemic arena of sorts? a quick scoped eval in consistency implemented in inspect?) • Something in forecasting (need to map open problems in this area!) • Exploring what a semantic search/indexing API for helping people build misinformation identification tools like our X bot might look like

--- #lab-notes-tamera ---
Tamera L: (I see that most people are using their lab notes channels for mostly work stuff (very reasonable!) so I might move personal thoughts to another channel later on) I haven’t used slack in almost a year, and man, there are a bunch of little ways in which it has become noticeably worse to me in that time. It is so perplexing to me when successful software companies can’t seem to stop themselves from making their product worse with successive new updates - definitely an open question in my mind why this happens
  └ Ben G: I have this same question!
  └ Ben G: Why change the UI??
  └ Tamera L: I tried to make a canvas just now and the formatting was all over the place. Like nobody QA’d it at all before pushing to prod
  └ Tamera L: Or even like, looked at it, or used it
  └ Tamera L: Did slack stop using slack internally?
  └ Tamera L: Also, this message showed up all on its own in my individual DMs. I know it’s harmless / meaningless, but also, it’s just a fabricated record of something that I didn’t actually do. Like, why are you doing things and pretending to be me, slack? I did not ask for that
  └ Tamera L: Bad for epistemic hygiene if you ask me
  └ [ME] Matthew B: I think it's a few things, but mostly principal agent problems The dev that can convince everyone internally to "improve" the UI gets a raise/status/whatever And it might be hard or subtle to actually know the truth if it's an improvement It only takes one charismatic connected dev to push crap through. And their manager is also incentivised to make the project seem like a success, etc. Multiply that by thousands of people and its basically impossible not to trend towards slop Also tech debt, decision debt, etc.
  └ Tamera L: Sure, I can believe that in the abstract. It just seems to me like some other feedback loop would take over once things got so obviously bad on the frontend that even a nonexpert would notice and complain
  └ Tamera L: Thanks for your analysis!
  └ [ME] Matthew B: People could def notice, but if you don't have the status/connections/whatever to go against the charismatic guy that pushed the update, then its not in your personal interest to say anything
  └ [ME] Matthew B: People in this fellowship are like 3x more disagreeable than the average tech employee, lol
  └ [ME] Matthew B: (In a good way)
  └ Tamera L: Seems like an honest-to-god emperor’s new clothes situation. I didn’t think that kind of thing actually happened in real life, to be honest
  └ Tamera L: Thank you again for your analysis. It’s hard to know what kind of bubbles we truly live in
  └ Seth K: Internal inventive gradients at large orgs heavily and inevitably favor performative work (in particular, adding stuff)


===== Fri, Aug 1, 2025 =====

--- #announcements ---
Kathleen F: Optional lightning talks are starting soon in a conference room on floor 3 (or in gather.town (https://app.gather.town/app/4ZfRZorTe9gHg6nD/RSP-library))

--- #general ---
  └ Alex B: Is it a safe assumption that demo day will be the final week, i.e. week of 13-Oct?
  └ Josh J: And/or an optional comment box at the end
  └ [ME] Matthew B: Or what if there was a slider on each question with 3 options: • Low confidence, med confidence, high confidence Defaults to med but you could change it for each question. If you switch it to high confidence then future questions default to that (maybe you're an expert) --- And is there a benefit from having strongly agree? 5 point scale, or not really? Just two thumbs down, one thumbs down, neutral, one thumbs up, two thumbs up. Wouldn't take up more space cause you're removing the text in the buttons unless you think that's necessary Obviously don't want to make it a more complicated tool if the value-add isn't there for these features Tagging in @Rob Gordon

--- #lab-notes-jlevy ---
Joshua L: thanks @user @user @user @user @user @user @user! if you have further thoughts do drop here or in the doc and I’ll review [pray]
  └ Kathleen F: piggy-backing off this list of pings… Do any of you have feedback / ideas about the use of Google Docs as a collaboration tool during this meeting? Or other ideas about how the group discussion went? Feel free to reply here or DM me [slightlysmilingface]
  └ Paul F: I thought it was good, not sure if there are other better options, happy to try new things if people have ideas.
  └ Kathleen F: One of my thoughts is that Google Meet might work better than gather.town once we finish the “checkin” part of the meeting and break into smaller groups (although we didnt’ have enough people to break into multiple groups today)
  └ Kathleen F: Seems like Matt had some issues with gather.town for example
  └ Kathleen F: Another of my thoughts: It might be helpful to try the “hand/finger” moderation system (people write in the chat “hand” if they want to make a new point, and “finger” if they want to say something that’s a subtopic on the current discussion; the moderator calls on the hands in order but calls on the fingers before moving to the next hand)
  └ Kathleen F: At any rate, I was happy to see the discussion today; it seemed fruitful!

--- #lab-notes-tamera ---
  └ Tamera L: This is the new canvas page btw. Like, that is not subtly bad. That is really obviously bad. And when I look at this, I think the whole story can’t just be about adding new features - they have to be letting quality standards slip, too, to get here. I am so curious about what’s going on in there haha
  └ Tamera L: I wonder if there’s any firm that specializes in rescuing companies from their own internal incentive problems like this. Surely investors should stand to benefit from that?
  └ Tamera L: I guess that’s like the Bobs from office space. Presumably some firms are better than others at that task - certainly the Bobs weren’t looked at with respect in that film. I wonder how pricing and prestige work in that industry
  └ Tamera L: Even without consultants, it’s so mind-blowing to me that it could even get to this point to begin with.
  └ [ME] Matthew B: Canvas is new enough it might still technically be in beta phase or something, idk Or slack is now a dead player because bought by Salesforce, so any truly good devs left?
  └ Tamera L: There was a decent version of canvas in slack a year ago. It was a sidebar thing, and it had some faults, but was basically solid


===== Sat, Aug 2, 2025 =====

--- #announcements ---
  └ [ME] Matthew B: did there happen to be notes / recordings?

--- #lab-notes-jlevy ---
  └ [ME] Matthew B: @user I'd prefer google meet or zoom just so we could invite an ai recorder to transcribe so the notes are complete and guaranteed. But that's just my opinion
  └ Kathleen F: thanks Matt!
  └ Joshua L: agree, I’d love to have audo recordings of meetings on specific topics. I’d use them with my “deep transcribe” tool [slightlysmilingface]

--- #lab-notes-tamera ---
  └ Tamera L: After further reflection, my main hypothesis is that solving these kinds of problems is typically the role of consulting firms like McKinsey, but it seems like they aren’t able to be effective in this case. I’d further conjecture that the hold up there is that those longstanding consulting firms are best adapted to help non-tech companies, and that the problems that tech companies have just aren’t in their wheelhouse. Consultants at firms like McKinsey aren’t typically software people - as far as I understand, they usually hire people right out of college, and look for generalists. That type of profile might not be the best at identifying the bottlenecks within the thorny intersections of software, people, and process at high-flying tech companies. This is just a hypothesis, but if it came down to it, I believe I would place a bet on it.
  └ Tamera L: And, as a side benefit, I feel like I finally understand what consulting firms actually do for the first time in my life. And that’s despite having been friends with consultants and speaking with them about their work!
  └ Tamera L: (To be clear, placing a bet on something doesn’t mean that I’m totally certain of it, or that I believe it’s the whole story. It’s just a sign of slightly elevated confidence in what is still ultimately a hypothesis)
  └ Tamera L: Anyway, if anyone wants to start a software consultancy with me, please let me know! [sparkles] (this is basically a joke, but I do think projects in that hypothetical space could be instructive in various ways)


===== Mon, Aug 4, 2025 =====

--- #announcements ---
  └ Kathleen F: unfortunately we failed to record this session due to technical difficulties, which should be resolved now

--- #eng ---
[ME] Matthew B: Alex Boone (Josh's friend) sent me this tweet about an open source task-specific auto RL pipeline thing (idk if it's good, but it's interesting!) https://x.com/saumyagandhi007/status/1950664506595758097

--- #general ---
  └ Maximilian D: Will the links to check-ins and breakouts be a Google Meets link or are we meeting in Gather.town? Wasn't clear to me from the programming doc
  └ Kathleen F: @Maximilian Kroner Dale For at least our breakout meetings, we’ll start in gather.town and then use google meet when we split into groups. We’re doing our lightning talks in gather.town for now as well although we might change to using google meet
[ME] Matthew B: I created a Prompts folder (https://drive.google.com/drive/u/0/folders/1OI_AccAViGS0K3SGFt98ZQ4PoBdAHxgg) in the shared Google Drive, please add to it!! • I added the first prompt - a high level overview of the whole FLF Fellowship / AI for Human Reasoning idea - 20k tokens • If you're comfortable, allow people to update the file directly so it can easily be expanded / improved. Otherwise, lock it for comments only and let people copy to fork if they want • An easy way to use the prompt is to download the doc as a markdown file to either attach it as a file or copy-paste the text (I think markdown helps focus the LLM attention) • I suggest wrapping your doc in descriptive XML tags so it's easier for AI to delineate between many prompts
Tamera L: Question for the organizers: can I use the compute stipend to pay for a fancier version of Claude Code to use during the fellowship? It’s $100-$200 per month depending on the tier
  └ [ME] Matthew B: I asked this during a Q&A and they said yes Claude Code counts as compute you can expense

--- #lab-notes-matt ---
[ME] Matthew B: Created my Canvas (https://aiforhumanreasoning.slack.com/docs/T093YGRN8R2/F098T4TJL2X) (inspired by @user's)
[ME] Matthew B: I'll put the plan that I wrote out during one of the kickoff events in this thread, although it's subject to change a lot
  └ [ME] Matthew B: *Project Ideas:* - Epistemic Evals - Read everything Lukas has written or linked to - Come back to this: https://docs.google.com/document/d/1bZKLW2Yaiw__VOPioL9b6oAWXomFQdvayV8Zny3CMc0/edit?tab=t.0#heading=h.603h3k4gas08 - Figure out the theory of change potential (to limit the search scope below) - evals are hot - maybe they're oversupplied - labs will be eval shopping? - Find eval aggregators, directories, lists - Understand the eval space broadly - Most popular evals - Longest lasting - Eval posting orgs - Common structures - Tips / warnings / gotchas / guides - Find posts of people discussing eval results, building evals, critiquing evals (Less Wrong, etc.) - Book a call with all of the people in the fellowship that are interested in this - Write my ideas/questions in the eval Slack - Once I gather the above, write a doc or book a call with Lukas to confirm my ideas or clear up my confusions and help me plan on MVP - How to get people to care? - Orgs (gov and not Gov) have calls for proposals for evals - Alignment Project - Open Phil - calls for proposals - Cause areas usually increase eval - Distill Toolkit - Have another discussion with the people interested in this topic sync and async to understand: - What is the impact story? - anyone using it for citations would be good? - What is the short term and long term goals? - What parts of the idea would we collab a lot on and share VS what parts of the project are going to splinter - How could this project best help other fellowship projects? - library manager for tools other groups are building? - Nathan - Getting stuff for prediction markets - What is out there already? Do research with the group to understand better the current playing field - What can we steal / copy / improve? - Existence proof of people looking to use tools like this - How does can this toolkit be set up or built out to also help the eval idea at the same time? - Automated evals for citation checking - Automated eval for finding logical flaws - Automated eval for finding biased language?? - STEP: Set up a call with Elicit - Are we a competitor? - https://aisafetyfeed.com/ - Show it off in the demo channel, explain it as a project to see if anyone else is interested in the general concept, get feedback, etc. - Keep it running in a basic sense until the build phase - It could be used as a test bed for distill toolkit, etc.
[ME] Matthew B: I think if we share/improve each other's prompts, (Google Drive folder here (https://drive.google.com/drive/folders/1OIAccAViGS0K3SGFt98ZQ4PoBdAHxgg?usp=drivelink)) we would get a lot of value out of our collective intelligence + AI
[ME] Matthew B: I'm going to work on a very simple Slack app that can help people with offers and asks • Gather the offers from everyone in the doc we originally wrote ◦ let people add / improve it manually? ◦ Scrape their website / resume to auto update it? • Allow people to search it with embedding + AI to find the people/offers that best fit their needs week to week ◦ possibly ping people on Monday morning asking them to share their asks. Automatically suggest people that can fulfill the ask ▪︎ post the asks automatically in a channel?

--- #lab-notes-niki ---
Niki D: Hi <!subteam^S0941SE8AG5>! I've made a thing I would really really like you to see / contribute to. It's an interactive map of existing projects in the AI for Human Reasoning space, which is intended to support you in discovering prior work you might not have known about. It's very rough, but in the spirit of working in public I'm releasing a public link now. I plan to continue to maintain and update/improve this map throughout the fellowship. It would be very helpful if you could share projects that I've missed, either by using the feedback form on the site, or by sharing that with me via slack. Hope you discover something new [heart] The Map: https://cute-cascaron-fcc2d6.netlify.app/ Codebase: https://github.com/nickkeesG/AI4HR_Map
  └ [ME] Matthew B: Would my AI Safety Feed count? https://aisafetyfeed.com/
  └ [ME] Matthew B: a filterable table view would be cool, if it's easy to build
  └ Maximilian D: this is awesome


===== Tue, Aug 5, 2025 =====

--- #general ---
  └ Niki D: Is the fancy version worth it?

--- #lab-notes-emma ---
Emma K: Update from the past weekend: TL;DR have a side project I plan to pursue + have tentatively positively updated on the ease of using local government as a testbed for AI x policy/govt ideas • Went to a tech for SF hackathon (my first hackathon!) • Won it, with a vibecoded website (we're truly in a golden era for “idea guys”) • The project was an open platform for govt, technologists, and civic actors to post ideas/requests for civic tech projects and get feedback/collaborators/advisors/end users (as well as for govt actions/policies to better leverage tech or collaborate better with the civic tech community). • Given a new more pro-tech mayor recently came in, there’s a lot of activity around tech x SF govt; I appear to have maybe successfully inserted myself into that/possibly preempted some of it with the platform. We’ve been invited to discuss it with the Mayor’s Office of Innovation, some of the Board of Supervisors, and one or two new/relaunching public-private initiatives. • Don’t think this will require more than a few hours a week from me to continue to pursue, and has some direct benefits for my fellowship work: ◦ Its a platform for coordination + collective intelligence, so a testbed for ideas relating to that. I have some ways I want to try using AI to help with that (though I don’t think AI is necessarily the core lever here). ◦ It’ll help refine my understanding of paths for adoption/distribution of AI4HR tech into government ◦ It’s offering good surface area expansion/networking, and in particular will get me relationships with local govt & civic tech realm that could help offer testbeds for my (or others’) AI4HR projects, where that’s useful (e.g., for public input style tools)
  └ Ben G: Congratulations!
  └ Ben W: wow, awesome!
  └ Oly S: • Went to first Hackathon • Won it [sunglasses]
  └ Oly S: Well, we are in 'explore' phase, so don't let me talk you into doing something you don't believe in but this sounds like exactly the sort of fellowship activity I'd encourage pursuing!
  └ Emma K: thats really good to hear! will definitely keep pursuing, just dont think doing so needs to close off also pursuing other ideas

--- #lab-notes-gordon ---
Gordon B: These days, I'm always running a meta-process in my head where if I start working on a task, I stop, frame the task as a prompt, and let the AI take a first crack at the problem. Sometimes I even run the meta-meta-process and let the AI define what the task should be from my stream-of-consciousness braindump.
  └ Gordon B: The future of work feels like it's going to be figuring out which layer of meta you should be on for a given task. More layers as models get smarter.

--- #lab-notes-martin ---
Martin C: TLDR: Reverse image searching a photo to find when it first appeared online is sometimes - but is usually not - the only piece of information needed for a community note. Reverse image searching would likely need to be part of a larger AI pipeline to make this into a useful bot. @user and I have been experimenting with what pieces of the pipeline are actually needed and which are easiest to build. What we're currently working on: Developing a reverse image search system for X Community Notes to help combat misinformation by identifying when old or misattributed images are being shared as current events Key challenges discovered so far: Cost constraints • Existing reverse image search APIs are expensive to use ($200 for 5000 images) ◦ Related: Could we potentially develop an image indexer within X itself to document when an image first shows up on X and its related content? This might be very useful for @user and I to potentially build as shared infra for other community note bot teams and the broader community notes ecosystem cc @user Not enough sample data in the community notes test database • When we first joined last week, there was a total of 20 tweets with images that community notes were being requested for • Now it's at 200, but we've found some issues • Very few of these actually yield useful community notes when reverse image searched ◦ (but is non-zero. Probably 1-5% on the upper end, perhaps this is still significant) To increase our success rate, we'd need to solve additional cases like (just brainstorming/calling them out, not saying that we would do them): • Is this image AI generated? • After reverse image searching, crawl and parse the websites to see if the image is still there • Check if the context on the website differs from the tweet (many cited sources won't even show the same image anymore) Difficult to identify candidates • From tweet text alone, it's hard to determine which images would benefit from reverse image searching (we would need an LLM to look at the image and provide context to what it shows) • Figuring out what exact prompt to give an LLM alongside the image, to help narrow down if this is a good candidate for reverse image searching is tricky (loads of edge cases)

--- #lab-notes-matt ---
[ME] Matthew B: Created <#C098NC0QBEK|> to chat with other people creating slack bot mvps

--- #lab-notes-niki ---
  └ Niki D: @user added

--- #lab-notes-tamera ---
Tamera L: I’m feeling in a bit of a (likely temporary) slump with the TTX project. I’m setting up the initial version now, and I think I may just be putting slightly too many constraints on myself: • Great architecture (I’m very happy with this part tbh) • New-to-me tech stack w/ partykit (which I am still learning) • Feel like I need to be creating something polished for a large group of people to provide feedback on (which feels hard to make go well at this stage) • Coding it up by myself, not on a team • Wanting to have a better automated coding workflow than I do / feeling like I’m moving more slowly than I should, or producing lower-quality work than I want to I think maybe the main issue is some implicit feeling of pressure to move faster and have more done than I do, which paradoxically makes it hard to make progress. I’ve typically found it easier to move fast on personal projects, where I don’t have the imagined board room of stakeholders looming over me in my mind [upsidedownface] any one of whom could have an objection or request which I didn’t happen to prioritize. Or working on a team, with established standards, and people to talk to when needed. I imagine I’ll move past this soon, but I thought it might be nice to share some of the lowlights as well as highlights. I’m also open to anybody’s reactions, thoughts, or advice here!
  └ Tamera L: I’m already getting a sense of what a better plan might be to move forward from here: don’t build this repo as though it will be the final perfect version. Just build it to be what it needs to be right now, and learn from that - it’s an MVP. As time goes on we’ll learn and have a better sense of how exactly to implement everything so that it’s as stable and scalable as it is in my dreams
  └ Owen C: When I've hit issues with momentum that rhyme with this, I've often found that at some level it's a kind of logjam where I have some explicit belief about what the next step "should" be, but another part of me doesn't fully believe in that and is unwilling to defer. Unjamming sometimes comes from by-myself making space for enough meta that I can tune into what I actually feel about next steps; also bringing someone in to talk things through with and bounce ideas off can help (partially just because it helps with social permissioning to spend time at the meta level, but also because other perspectives can help to move me out of a rut or directly help). Not sure how idiosyncratic this is! At least sometimes I think this advice has been helpful to others, but do ignore if it feels wrong-shaped. Also as I was writing it you said you've found a better plan already which is great. Guess I'll still share in case the general model helps any.
  └ Tamera L: Yeah this is helpful advice for me! I think writing about the stuckness was a similar permission for me to spend some time at the meta level. And your perspective does help!
  └ Alex B: One thing I didn't share in the vibecoding session earlier is that I feel frustrated with my overall rate of progress on projects, especially where I care at all about the quality or I'm building something complex enough that AI slop code will come back to bite me quickly. Like, in theory I have these multiple faster-than-me agents running around getting stuff done, and then at the end of the week I'm not even fully sure that I achieved more than I would have without AI. I also get stuck between "I need to optimize this workflow so I can move faster" and "I need to stop wasting time trying to optimize this workflow and just get stuff done". Reading your note and reflecting on my experience, I'd actually caution against going too hard on trying to improve your AI agent workflow. It's easy to sink a lot of time there, and the ecosystem is moving so fast that we'll all get better versions of these workflows for free before too long anyway.
  └ Tamera L: Thanks for that, that’s super helpful. Great reminder about how fast the ecosystem moves
  └ Ben S: similar to what you already mentioned, my advice would be to race to build the shittiest possible version that works end-to-end, and then go back and iterate afterwards. in my experience it's often not clear which parts of a product / codebase are most important to polish until the whole thing is strung together, even if hanging by a thread. also I think people here would probably be happy to give you feedback on an unpolished version

--- #lab-notes-timothy ---
Timothy T: Thinking a lot about what structures are most useful for Fellows. Currently considering eliminating the cohort-wide activities in the afternoon for the second half of the week (Starting Wednesday) and keeping the morning sessions (LT wed, Breakouts Thu, LT Fri)

--- #slack-bot-workshop ---
[ME] Matthew B: @Ben Sklaroff did you say something about building a Slack thing?
[ME] Matthew B: Here's my WIP for a Slack bot that helps automate offers & asks: https://github.com/MattB543/offers-and-asks-slack
  └ Ben S: this is cool! currently Pivotal doesn't store any long-term user-specific context -- it just tracks all the ongoing conversations in slack around a given topic (e.g. scheduling a specific event). you got me thinking about how to make that topic history a queryable source of user info (e.g. user skill sets), so that it could potentially serve this use case as well
  └ [ME] Matthew B: Do you have a channel or doc with your thoughts, ideas, plans, etc. For pivotal?
  └ Ben S: we're at <#C098TMQ9XT5|>, feel free to join!
  └ Ben S: I'm also planning on writing a weekly update email, which I'd be happy to forward to anyone who wants to track our progress
[ME] Matthew B: it's not working yet, lol, I had Claude Code try to spin up V1 late last night and I didn't get it to deploy yet
[ME] Matthew B: @Rob Gordon do you have the fellow data you used for this https://ai4hr-fellow-flashcards.vercel.app/ in a format you can easily share with me?
  └ Rob G: Yeah! It's in airtable. Do you want to DM me your e-mail address and I'll add you to the airtable. Then you should be able to create an API token and pull it (and/or just export it)
  └ [ME] Matthew B: sick - <mailto:matthewrbrooks94@gmail.com|matthewrbrooks94@gmail.com>
  └ [ME] Matthew B: I trust these crazy fellows w/ my email, lol
  └ Rob G: haha word. Just didn't want to assume!
  └ Rob G: Okay, just sent an invite! Also we can totally share with the whole retreat if people have other uses for it
Niki D: Ok next hurdle: Slack api rate limits. Anyone dealt with that already?
  └ [ME] Matthew B: ohhh crap... they just updated that I think https://api.slack.com/changelog/2025-05-terms-rate-limit-update-and-faq Btw, it takes months to get approved for the Slack marketplace I think you'll just have to work around it, like start your job at 5 AM and analyze the previous days messages very slowly
  └ Niki D: I think the methods I'm using are on Tier 3 https://api.slack.com/apis/rate-limits It says 50+ per minute so... that's ok I guess? Annoying for development though
Niki D: @Timothy Telleen-Lawton Can I get takes on whether I should do: Option 1: Rolling 7 day window, daily summary covers everything you need to know about the big picture of what people are working on Option 2: Daily report only tells you about the last 24 hours (and to see more info you previous reports, posted in the same channel) I'm leaning toward 1, cause I hate scrolling up in a channel to find old things? But I realize it might be really annoying to see the same stuff posted every day for 7 days.
  └ Niki D: My thinking is: For someone who checks every day they'd prefer 2, but for people who don't they might prefer 1?
  └ [ME] Matthew B: you could post a messages like these (and make them better obviously) "Tues 08/05/25 24 Hour Report" And then put the 24 hour report in a thread on that message And then every Monday morning you could also post "Mon 08/04/25 Weekly Report" And put the full previous week report in that thread? So it's very easy to scroll and click into different threads (not cluttered)
  └ Timothy T: Hmmm good question. If we only do one I agree Option1 is more valuable, in part because many people will have not posted in a given 24 hour period, and we want most people to be able to look at just one report.
  └ Timothy T: In theory we could do both or something like Matt's solution to try to meet both use-cases (people checking daily vs weekly).

--- #surface-area-luke ---
Luke H: [ai4hr mapping/deliberation] It'd be interesting to poll us all on a bunch of high-level questions (AI predictions, interest in various projects, theories of impact, etc), see what correlates with what, and then repeat the survey in a couple of months and see how everybody has shifted.

--- #surface-area-nathan ---
Nathan Y: A problem with community notes is not knowing which sources are trustworthy. Seems like an increasing problem as it becomes cheaper to put up a fake but official looking site
  └ Ben G: https://www.tracingwoodgrains.com/p/reliable-sources-how-wikipedia-admin I think this is true and it's like this article on how a wiki admin has abused the "reliable sources" aspect of wikipedia
  └ Ben G: or, maybe its the inverse of it! in that community notes doesn't have a canonical list, so instead you have maybe a proliferation of spam sites

--- #surface-area-nuno ---
  └ Gordon B: Another signal might be tightness of the feedback loop between person and AI. Generally, the tighter the loop, the greater the synchronization between systems, and the part of the system with the most variety will control the direction of the combined system https://en.m.wikipedia.org/wiki/Variety(cybernetics)#Lawofrequisitevariety (https://en.m.wikipedia.org/wiki/Variety(cybernetics)#Lawofrequisitevariety)


===== Wed, Aug 6, 2025 =====

--- #coordination-and-ai ---
Ben S: @Sofi (or anyone else) if you're interested in building a new pivotal workflow, take a look at this function: https://github.com/cooperativetech/pivotal/blob/main/src/anthropic-api.ts#L260 Basically the idea is that it takes as arguments the current topic summary (e.g. a topic is "let's schedule a meeting for next week"), and all historical slack messages relevant to that topic, as well as the latest message sent that the bot can see (which triggers the function call). It then returns a set of updates to the topic summary or set of users involved, as well as a list of next messages to send either in DMs or to the group. Pretty much all you have to do for a new workflow is to write a new one of these functions that corresponds to your desired workflow, the rest of topic tracking + slack bot minutiae should be handled by the backend. That said, it's kind of difficult to make sure the bot does the right thing given all the conversational context, so we're working on more testing + evaluation tools now, which should be generalizable to multiple types of workflows
  └ [ME] Matthew B: @Ben Sklaroff how did you build the Slack message scraping thing? @Niki Dupuis was hitting crazy rate limits on that. Like 1 request a minute or something
  └ Sofia V: Thanks! FYI we're likely building a simple custom react frontend
  └ Ben S: @Matt Brooks right now the app is not actively querying for chat history, it just uses the Events API to listen for incoming messages from channels it's a part of, and saves any messages relevant to active topics

--- #eng ---
[ME] Matthew B: Does anyone have a good/easy method to give Claude Code access to your DB? (maybe read only if you're scared) So it can check schema/ data/ etc. automatically?
  └ Alex B: Are you finding it struggles using the command line? Do you have examples in Claude.md?
  └ [ME] Matthew B: I've never really tried it tbh, it can run postgres queries directly in command line? Do you put your DB URL in claude.md and tell it to fire off queries? Is this somewhere in their documentation? Should be a standard flow, right?
  └ Alex B: Don't know which DB service you're using but yeah basically. Though personally I would use a local test DB for dev and not let ai near any DB I care about for now.
[ME] Matthew B: Does anyone use an auto PR reviewer / bug finder? Like: https://www.coderabbit.ai/

--- #fellowship-water-cooler ---
[ME] Matthew B: Roon - "one thing I am more sure of now is that total governance capacity of mankind is not ready for superintelligence" https://x.com/tszzl/status/1726018295449760072
[ME] Matthew B: https://x.com/jeffreyhuber/status/1953164961599152592 > what is the best LLM (and prompt) for getting high-quality critical feedback on ideas? the sycophancy is driving me nuts > Any model, but you need to say "my friend has this idea and I'm trying to talk him out of it"

--- #general ---
Elizabeth G: [sparkles][books]Book Recommendations[books][sparkles]:
  └ Elizabeth G: @Kathleen Finlinson @Blake Borgeson
  └ [ME] Matthew B: Blake's suggestion was "Reinventing Organizations" I think Kathleen's was "Tao Te Ching"?

--- #lab-notes-alyssia ---
  └ [ME] Matthew B: > Exploring what a semantic search/indexing API for helping people build misinformation identification tools like our X bot might look like Does this sounds like something that would be a part of the "Distillation Toolkit (https://docs.google.com/document/d/1Re4CgEUWppjHhRqEJYixj5inEs2oNHH6cTk3BKyVZ78/edit?pli=1&tab=t.1gqrwq6luvgk)" idea that is being talked about
  └ Alyssia J: I should check that out - could definitely be! There might be room for specializing in image here potentially

--- #lab-notes-emma ---
  └ [ME] Matthew B: "Its a platform for coordination + collective intelligence, so a testbed for ideas relating to that. I have some ways I want to try using AI to help with that (though I don’t think AI is necessarily the core lever here)." Does this sound similar to the small group agent-based decision making / coordination apps that <#C098TMQ9XT5|> and <#C097XRT3BSP|> are looking to build? It would be sick to build an awesome MVP and get it in front of SF gov people. Obviously don't want to hijack your idea/project, but I'm sure some people would love to help you build something if you already have a use-case/audience in mind (I would be happy to help)
  └ Emma K: @Matt Brooks maybe? here's the mockup website to give you a sense of where we're going with the project (SF OS) https://preview--idea-forge-nation.lovable.app/; if you think theres potential to integrate your work into the website, super excited to hear. (But also if your work's not an exact fit for integration into SF OS as a platform, but there's civic applications for it, then would encourage you to use SF OS (once its officially up) to post your work on there, to see if ppl adopt it/build upon it further!)
  └ [ME] Matthew B: ahh, now I fully get it, very slick for a hackathon, well done!

--- #lab-notes-gordon ---
  └ [ME] Matthew B: yes, I think this is underused. I'm a huge supporter of getting your ego out of the way and use AI to help you think/plan better

--- #lab-notes-kai ---
Kai S: One thing I have noticed is that it looks like a number of different groups are thinking about coordination projects that feel related: @user and @user are programming a Slack Bot for relatively granular decisions, @user and @user are aiming at automating the "2-hr meeting" level complexity of decision, and for deliberation my thinking was to look at more complex outputs (like contracts, code-bases etc) where the bottleneck is helping people focus their attention on which parts are most relevant to them. It feels like the basic piping behind the scenes will be quite similar for all of these. Without wanting to introduce additional complexity, I wonder if that means there would be scope for building out a joint back-end. Perhaps worth thinking about depending on how the different explorations go!
  └ Parker W: https://github.com/cooperativetech/pivotal Our github is public, see here
  └ [ME] Matthew B: @user if you can make it to breakouts in 20 mins we're aiming to have a breakout about this exact topic!
  └ Kai S: Hey, sorry another meeting ran over and I just missed this! What was the take-away? [slightlysmilingface]
  └ [ME] Matthew B: We talked a lot and typed a lot into this doc: https://docs.google.com/document/d/1R5nRqfHcXJtKfoylsMTh0jn43dKrth5mwZ9StZzCj3w/edit?tab=t.0 @user has the meeting recording (if it's possible, can you add a link to that to the top of the notes doc?) I think: • two groups will likely think somewhat separately and hack away for now • Potential for shared infra in the future • groups / ideas are definitely still in flux • short term product and long term impact goals aren't nailed down, but there are many thoughts/opinions in the air

--- #lab-notes-martin ---
  └ [ME] Matthew B: "Is this image AI generated?" is a tool that @user would highly benefit from, I believe
  └ Jay B: Thanks @user @user ! On my end in the near term I at least will be able to ramp up the number of candidate eligible posts In the longer run, I do very much like the idea of building an image indexer internal to generate candidate eligible posts for notes! Let me chat with other people at X and think about whether it might only make sense to do this internally at X or if there is a viable approach with external infra
  └ Alyssia J: @Jay Baxter nice! great to hear you also find it interesting. do keep us in the loop, both @Martin Ciesielski-Listwan and I would be keen on trying to help however we can (whichever way it might look like) because of how impactful we think it could be

--- #lab-notes-matt ---
[ME] Matthew B: My Slack App MVP "Offers and Asks" is live!! [rocket] [fire] [rocket] [fire] [rocket] [fire] Please test it out and give me feedback. Just <slack://app?team=T093YGRN8R2&id=A098T3WPQRH&tab=messages|go to the app> and DM it something like "I need help making an MVP quickly" and it will reply in a thread to your message with the 5 most likely people that can help you with that task based on their assumed skills, like so: (you can edit / add your skills from the app home tab)
  └ [ME] Matthew B: Lightning Talk Feedback: • Seth: create clusters around readings/content/sources ◦ surprising connections. 14 people all mentioned/cited this paper • Alex: there could be more connection happening and my direction is decent, he was thinking about this. There are likely other people that are feeling a bit lost. Quickly identify "this person might want to work on this thing, I could write up a spec and chat about it". You might want "potentially promising interactions" - instead of perfect matches we should find weird/interesting overlap in people.
[ME] Matthew B: Random thought... if OpenAI indeed buys itself out from under the non-profit, won't the non-profit have many billions of dollars, believe in relatively short timelines, and want to quickly fund projects that raise human reasoning/epistemics/governance/etc.? Off of that thought... the apparent bottleneck for OpenPhil is the ability to evaluate grants I believe they don't have the time/manpower to evaluate many hundreds of small grants (like <$10k or something) And reviewing large grants takes a bunch of time Wouldn't a personalized platform/agent/flow powered by GPT-5 be able to cut down on man hours by like 50% or something? Couldn't this "grant evaluation" thing be used for other impact-oriented funders? Maybe the OpenAI non-profit in the future? The idea would be to unlock many billions of dollars that want to support x-risk in the next few years but don't have a good way to evaluate people/projects Also, if a warning shot or more billionaires wake up to the AI risks over the next couple of years, the money that wants to flow into mitigating X-risk might like 5x but the bottleneck woudl still be grant evaluations / follow ups / etc. Does FLF have this sort of problem? Would they be interested in using a tool like this? @user
  └ Ben G: 1.) The situation you're pointing at - an influx of new philanthropic dollars into AI safety - is very real. Regardless of what happens with OAI, I'm betting on massive amounts of capital coming into the space from gvts, companies, and other foundations over the next few years. 2.) Improvements to grant evaluators would be a great use case for things like AI research tools, AI forecasters, etc. 3.) I'd be interested in using such a tool (how we leverage Uplift for FLF's core workflows is a recurring questions/topic!) 4.) However, note of caution: I often feel like the problem is not that we're bottlenecked on grant evaluation in a traditional sense: it's more that we're bottlenecked on ambitious fundable plans that have been de-risked to the point that we can put meaningful dollars into them. If this is of interest I can put you in contact with some of the creators of https://manifund.substack.com/p/announcing-manival
  └ Ben G: (I haven't seriously tried it, I'd be interested in evals around it - another area where epistemic evals would help...)
  └ [ME] Matthew B: > However, note of caution: I often feel like the problem is not that we're bottlenecked on grant evaluation in a traditional sense: it's more that we're bottlenecked on ambitious fundable plans that have been de-risked to the point that we can put meaningful dollars into them. Yeah, I agree with this, although it's probably a harder problem to solve and takes more time (upskilling, incubating, community growth, etc.) I actually hadn't seen Manival, that's super cool, I'll take a look
[ME] Matthew B: I had thoughts/ideas/questions around Fellowship Mapping & Coordination during this (good and bad) chaotic Explore phase I dropped my ideas into a doc and then talked them through with @user https://docs.google.com/document/d/1-Xc4C-1IJQnKkK32k3DzlLjUStADxxTnFnZrhMarBUs/edit?usp=sharing I think it would be interesting to follow up with @user about TownCrier (I need your data, lol) And talk to @user about a survey we can build and send out to gather some cool/specific info from fellows (fellowship leadership would be interested in collabing to get info they need too) Claude high level summary: > I'd like to build an AI-powered coordination system that analyzes fellowship activities (daily lab notes, project docs, offers/asks, etc.) to surface valuable connections and insights—helping fellows naturally form teams, coordinate, and avoid duplication. > > The goal is to preserve the creative chaos that sparks innovation while reducing the overwhelming chaos that causes confusion, by automatically detecting emerging project clusters, matching complementary skills, and tracking how thinking/uncertainty evolves throughout the program.
[ME] Matthew B: I created <#C099AMKKSR0|> to talk about stuff like this

--- #lab-notes-tamera ---
  └ [ME] Matthew B: Also, once you have a somewhat crappy but working MVP it's much easier with AI to build a cleaner/better version iterating in the right direction So I totally agree with "don’t build this repo as though it will be the final perfect version"
  └ Seth K: Fwiw just to echo sentiments here, “build what it needs to be right now” / MVP is always how games get built. So much so that whenever I see a fully comprehensive master plan of “here’s how it will go,” it’s a red flag of a team that’s about to learn expensive lessons. IMO it’s simply that many of the important questions don’t come into actionable focus until you encounter them in the context of everything else that you’ve built. The master plan is like going to Big Sur and plotting out which lanes you will use, etc. You can have a general sense ofc but excellent decisions about your lane depend on context that just doesn’t exist yet.

--- #lab-notes-timothy ---
  └ [ME] Matthew B: I have some thoughts/questions/ideas on this, would you like to book a quick 1 on 1 later today? Maybe 15 mins just after the breakout group if you're free?
  └ [ME] Matthew B: actually @user, I'll chat with Ben in gathertown after the breakout, feel free to join if you want!
  └ Timothy T: I'd love to join, but I have a meeting at 10:30, so will only make it if the breakout ends early.

--- #meta-fellowship-tools ---
[ME] Matthew B: I had thoughts/ideas/questions around Fellowship Mapping & Coordination during this (good and bad) chaotic Explore Phase I dropped my ideas into a doc and then talked them through with @Ben Goldhaber https://docs.google.com/document/d/1-Xc4C-1IJQnKkK32k3DzlLjUStADxxTnFnZrhMarBUs/edit?usp=sharing I think it would be interesting to follow up with @Niki Dupuis about TownCrier (I need your data, lol) And talk to @Luke Hewitt about a survey we can build and send out to gather some cool/specific info from fellows (fellowship leadership would be interested in collabing to get info they need too) Claude high level summary: > I'd like to build an AI-powered coordination system that analyzes fellowship activities (daily lab notes, project docs, offers/asks, etc.) to surface valuable connections and insights—helping fellows naturally form teams, coordinate, and avoid duplication. > The goal is to preserve the creative chaos that sparks innovation while reducing the overwhelming chaos that causes confusion, by automatically detecting emerging project clusters, matching complementary skills, and tracking how thinking/uncertainty evolves throughout the program.

--- #slack-bot-workshop ---
[ME] Matthew B: A cool v3 of your bot @Niki Dupuis is to scrape everything in every lab notes channel, save it to a DB, an let people search / query / chat with the DB
  └ [ME] Matthew B: actually, saving all of that data in a DB would help me a lot because I could use it to improve the skills/offers in my dataset (you could even use my DB and just create your own tables if you want)
[ME] Matthew B: lol, don't let your bot DM you and then re-trigger a flow based on receiving a DM or you will enter into an unending loop and send hundreds of messages [face_palm]
[ME] Matthew B: Anyway, after fixing that last bug, my Slack App MVP is live!! [rocket] [fire] [rocket] [fire] [rocket] [fire] Please test it out and give me feedback. Just <slack://app?team=T093YGRN8R2&id=A098T3WPQRH&tab=messages|go to the app> and DM it something like "I need help making an MVP quickly" and it will reply in a thread to your message with the 5 most likely people that can help you with that task based on their assumed skills, like so:
  └ Niki D: it works!
  └ Niki D: unsettling how fast that was though
  └ Niki D: like did it even make an api call?
  └ [ME] Matthew B: Yeah, kinda crazy, it has to make one gpt-4.1 call and then match the embedding. Quicker than I thought it would be though
  └ Ben S: the link didn't work for me in the browser, had to go here: https://app.slack.com/client/T093YGRN8R2/D099TCBTWRW
  └ [ME] Matthew B: Oh true, that deep link might only work in the slack app What kind of monster uses slack in the browser though? [laughing]
Niki D: ugh I'm not facing some kind of limit, where when I read from a channel it only lets me get max 15 messages (no matter how high I set the limit). Claude code thinks this is my fault somehow, tells me the limit should be 100
  └ Niki D: ah
  └ [ME] Matthew B: Might be worth it to use my db and run the job every hour to scrape 15 messages max on a recurring basis
  └ Niki D: Maybe we should chat tomorrow. I also realized that I'm being rate limited at 1 api request per minute [sob]
  └ Niki D: At least that's what the docs say, in practice I'm doing a bit better than that but still

--- #small-group-decision-accelerator ---
[ME] Matthew B: <!here> what if we created a breakout group in todays call for Pivotal and DecisionMate to chat through your theories of counterfactual impact, your MVP and longer term plans, idea use cases / customers, etc. to see what level of similarity there is or shared resources or whatever

--- #surface-area-luke ---
  └ [ME] Matthew B: I really like this idea, it would be really great to find divergent world/impact views or cruxes now, to maybe attempt to resolve them or make it more clear which projects are attempting what things
  └ Luke H: Cool if you have thoughts on particular questions you think would be good to include drop 'em here and I can make a poll later this week
  └ [ME] Matthew B: I think this could be pretty valuable/interesting but it might take a fair amount of thought/work to come up with really good questions, etc. I'll think about it some today and send a doc and we could book a call later this week to brainstorm or something
  └ Ben G: +1 to very valuable - @user @user and I have been thinking about what the best way is to get good surface area on where people are at in their project dleiberation, clustering. Happy to help

--- #surface-area-nathan ---
  └ Nathan Y: Yes I think it’s probably the inverse.
  └ [ME] Matthew B: I think "citation score/trustworthiness" is a great problem to work on, and might fall into the <#C097Z6QB9QD|> toolkit idea, @user Sooo many people/projects/flows could benefit from: • Verifying a citation is truly saying what is claimed it said (easy - this is the deep research analysis idea you have) • Fact checking the citation and giving it a quality/truthfulness score (could be hard but super valuable)

--- #surface-area-nuno ---
  └ [ME] Matthew B: yeah this is def why it's happening now that Chat-GPT has memory. Tighter feedback loop. Imagine if memory + personalization is 10x improved next year, there could be a lot of danger about falling into a blackhole with an AI


===== Thu, Aug 7, 2025 =====

--- #demos ---
[ME] Matthew B: [fire] [fire] [fire] Officially announcing my MVP Slack app @Offers and Asks ready to demo, please test it out and give me feedback!! [fire] [fire] [fire] [rocket] How to use: DM the bot with a request. Either a skill you're looking for in another fellow, or a task you need help with, or something you need help thinking about, anything like that The bot then: - [brain] Creates a list of concrete skills that you might need from your request using AI (e.g., “AI forecasting”, “Bayesian inference”). - [dna] Creates embeddings for the need and each skill to compare them semantically. - [magright] Searches a database for fellows whose skills best match each extracted skill. - [abacus] Merges matches across skills and keep each person’s top relevant skills. - [sportsmedal] Asks AI to re-rank the top candidates using additional context about each person and picks the top five. - [writinghand] Replies to your message in a thread with the top helpers and brief context. Example output below (I think it nailed it) I can very easily make a much better V2 with more data about the fellows (pending from @Niki Dupuis's bot), but I'm excited for ideas/feedback from IRL usage The code is open-sourced here (https://github.com/MattB543/offers-and-asks-slack) [rotatinglight] FYI, I'm saving requests and responses in my DB to iterate, track usage, etc. [rotating_light]
[ME] Matthew B: (I added all of the missing fellows into this channel. Feel free to leave/mute the channel if you'd like, but I think a lot of demo testing from fellow demos will help us think about our own tools better and help the creator iterate faster, so if we could create a culture of lots of testing each other's stuff I think we would all benefit! [heart])

--- #distillation ---
[ME] Matt B: Seems like Roast my Post has some distillation / agent techniques we might want to test / copy https://www.roastmypost.org/docs/LLE3WOnlk9WNzqb6/reader

--- #eng ---
  └ Steven I: Neon has hosted postgres that you can branch out a dev copy of. I’ve found it really useful. They also have an MCP, but I haven’t tried it.

--- #fellowship-water-cooler ---
  └ Vaughn T: so ... my fellowship project (as currently planned anyway) is to build a tool that changes the ux of interacting with [probably a mix of diff models] to learn how to do the critical thinking with the support of ai tools. imv asking a machine to do the critical thinking is precisely the wrong framing; better to ask the machine for information that helps the human do the critical thinking better/faster

--- #lab-notes-matt ---
  └ Ben W: I agree with Ben that it's possible that the area will get a lot more funding but OpenAI does not seem likely to make a foundation which "believes in relatively short timelines, and want to quickly fund projects that raise human reasoning/epistemics/governance/etc". They have been pretty explicit about the money going to mainstream art, health, etc. nonprofits, particularly those in california. A cynical reader may interpret this as an attempt to buy off the california AG who needs to approve their conversion.
  └ Ben W: > it's more that we're bottlenecked on ambitious fundable plans that have been de-risked to the point that we can put meaningful dollars into them +1, I've talked to some other funders who feel like there is not much to fund in the space which is very impactful. Although I think it's worth noting that there is kind of a chicken and egg problem where people don't want to start projects because they don't want to be tied to OP and then as a result there is nothing to fund
  └ [ME] Matthew B: > They have been pretty explicit about the money going to mainstream art, health, etc. nonprofits, particularly those in california. My own naive outside view is that they say this because if they say anything else it angers too many people/politicians/everyone I would expect the non-profit board to be pretty AGI-pilled and would want to reduce x-risk because there are a ton of other players that will build healthcare/education stuff automatically --- > +1, I've talked to some other funders who feel like there is not much to fund in the space which is very impactful. Although I think it's worth noting that there is kind of a chicken and egg problem where people don't want to start projects because they don't want to be tied to OP and then as a result there is nothing to fund I agree and think this is a massive bottleneck, but might be pretty hard to solve. Very open to ideas/projects that could push on this. It seems like one of the bigger potential levers you could push if you could find something tractable. Quick prompt: why isn't OpenPhil trying to ideate/iterate/explore this bottleneck? Maybe they are, but I haven't heard to much about it. I've heard they don't want to say this out loud because if they say "there's not enough good projects" it pisses off people that they deny funding AND it disincentives people starting projects AND if they try to open the door more to potential ideas they just get more crap
  └ Ben W: hmm, I feel like OP is working on this? Like they invest a lot of their time in RFPs and active grant making which they wouldn't need to do if they had a stream of impactful grants coming in
  └ Ben W: I guess they don't say "we are spending our time coming up with lists of projects you should do because y'all are too dumb to come up with it yourselves" but I think that's the subtext
  └ [ME] Matthew B: > they invest a lot of their time in RFPs and active grant making which they wouldn't need to do if they had a stream of impactful grants coming in This is one way - to kind of hand hold in a more top-down way, but that doesn't seem very scalable. If AI Safety money 10xs in the next 3 years, could that approach 10x? Could it generalize to other orgs/Gov? Could someone build an environment to fine-tune GPT-5 on all of OpenPhils documents/writing/RFPs so that once they identify a space they want to fund they can spin up a high quality RFP in 30 mins? But also, it's interesting (and sad) that none of the (few) bottom-up methods have really been pushed hard on, mostly they haven't worked well, or there hasn't been appetite
  └ Ben W: Oh yeah the AI assistant thing seems great, I was just commenting that I think the assistant should be something more like "causes other people to apply with good ideas" rather than "filters the applications for good ideas"
  └ [ME] Matthew B: Ah, yeah I totally agree, ideally it should do both, as both sides of the problem could be a huge bottleneck in the near future (and maybe even currently)

--- #lab-notes-niki ---
Niki D: @Tamera Lanham and I have been trying to brainstorm ways to take things like this paper: Scaffolding cooperation in human groups with deep reinforcement learning (https://www.nature.com/articles/s41562-023-01686-7.pdf), where AI facilitates people to move from a low trust equilibrium to a high trust equilibrium, and turn it into something PRACTICAL that people would ACTUALLY USE. We haven't really settled on anything yet, but one idea that seemed not terrible was trying to build a really good mutual aid network, where people freely ask for and offer services to each other. AI would act as a matchmaker, helping to set up positive connections. 1. @Matt Brooks is already working on AI matchmaking with @Offers and Asks, and so I'd really like to chat to Matt about this idea more. 2. On Mutual Aid, I had a quick look for any local mutual aid networks in SF, and found these: a. Sunset Neighborhood Help Group (https://www.facebook.com/groups/sunsethelpers/): Top video is people announcing a street cleanup, asking for volunteers to join them. Also the FB group claims to be AI enabled, with a bot called SunsetPal??? b. Haight Neighborhood Help Group (https://docs.google.com/forms/d/e/1FAIpQLSdVyyRhv2AxKHUoSXV-pvn6s1IXBfW9J8wjkR11FcQ-i5aWw/viewform) Google form for offers c. SF Bay Mutual Aid (https://www.sfmutualaid.com/) Links to an offers airtable and an asks airtable. d. East Bay Mutual Aid Volunteer Intake (https://docs.google.com/forms/d/e/1FAIpQLSdOZvKC2UuFXvoMDr6NsYABalu4X0W6r0gj0YenfiTcJ7Xdw/viewform) Google form for offers e. Cole Valley Cares (https://www.facebook.com/groups/colevalleycares/) FB group I might sign up for some of these just to see what happens, and get some context on them. My sense is that mutual aid had a MOMENT during the pandemic, and a lot of the mutual aid networks I found seemed specifically about helping people get through that. Would anyone else want to brainstorm about this too? (maybe @Paul de Font-Reaulx @Alex Bleakley @Sofi @Ben Sklaroff @Parker Whitfill?) If so I'll make a channel for this specifically.
  └ Niki D: I just asked @Offers and Asks to recommend people who might be interested and it returned @Joshua Levy @Rob Gordon @Gordon Brander @Elizabeth Garrett @Sofi , so let me know (or @Matt Brooks) if it got that right [laughing]
  └ Owen C: Sort of guessing that @Oly and I may be suckers for the mechanism design aspect of brainstorming here (though I don't think I've fully wrapped my head around the idea and may be mistaken about how big a part of the key challenges here the mechanism design part is)
  └ Alex B: Definitely up for brainstorming!
  └ Parker W: Yep! In general, I think using AI for matching/market making is quite interesting.
  └ Rob G: @Niki Dupuis It definitely feels like there could be some overlap between this and what I'm working on. Please invite me to any brainstorm that's set up for this!
  └ [ME] Matthew B: I would like to brainstorm as well
  └ Gordon B: I love this idea!
  └ Ben S: love this!! food not bombs is a good one in SF, they operate mostly on signal
  └ Gordon B: Throwing in a random cool sci-fi idea (from Beyond the Reach of Earth). One of the hypothetical technologies in that novel is a phone app called "We Work" (joke) which is an AI app that does just-in-time dispatch for group coordination. In the novel, they use it on a construction worksite. The AI uses phones and AR glasses to monitor progress, dispatching microtasks to participants ("move this toolbox over to this location"). Kind of like packet switching but for teamwork. The result is described in the book as a bit weird. Everybody coordinating wordlessly, with work happening seemingly at random, but progress happening very efficiently because there's a high-dimensional optimization system coordinating everything.
  └ Gordon B: Maybe a bit out there, but I like sci-fi vignettes like this as provocations. What I love about your ideas and this sci-fi idea is that they explore how AI can support forms of coordination that are underneath the Coasean floor https://wiki.p2pfoundation.net/CoaseanFloorand_Ceiling
  └ Niki D: I love that sci-fi example! I was personally inspired by DivLab (Division of Labor) which is this computer system these anarchists use in The Dispossessed (https://en.wikipedia.org/wiki/The_Dispossessed) to coordinate volunteer work.
  └ Emma K: Interested!
  └ Tamera L: @Gordon Brander thanks for that interesting link about the Cosean floor and ceiling - it’s nice to have a crisp concept to describe something like that
  └ Emma K: @Niki Dupuis @Tamera Lanham tossing out there that if you think this https://preview--idea-forge-nation.lovable.app/ (https://preview--idea-forge-nation.lovable.app/) would be a good test bed for your coordination ideas (once it’s up and running with an active community - recognise that might not correspond to your timeline), we would be v interested in integrating it! sounds potentially related to something we were hoping to integrate around matchmaking people to projects as volunteers or advisors
  └ Tamera L: Is this your hackathon project?
  └ Tamera L: It looks super polished!
  └ Emma K: Yep! That’s all thanks to loveable (nb not all the features totally work aha). Proper version is coming along here nicely though, thanks to another teammate: sfos.vercel.app (http://sfos.vercel.app)
  └ Tamera L: Nice! Yeah this seems like a great use case for matchmaking
  └ Tamera L: I also think this whole SF OS project seems like a great idea generally. I’d be interested to follow along with it, and help support it where I can. Is it still in post-hackathon development?

--- #slack-bot-workshop ---
TownCrier: Hi! I'm not ready to make summaries yet, but here's the raw data from today's lab-notes scrape [blush] If you'd like to experiment with it, be my guest!
  └ [ME] Matthew B: @Niki Dupuis if I create a route in my Offers and Asks app, can you POST all of the raw data to it, so I can save it and use it in my DB? Are you doing a scrape every few hours or so, to get around the limited history allowance?
  └ [ME] Matthew B: Oh, you're limited on 15 objects and 1 request per min, but you can set the oldest or latest params to control the timeframe
  └ Niki D: Right, so I grab 15 at a time, once a minute. I was planning to just have the script spin up once a day. But sure, I'm happy to post that data to you!
  └ [ME] Matthew B: oh, what if someone posts more than 15 messages in a day? also, I'm thinking I might want to scrape all messages from all public channels..... what do you think of that?
  └ Niki D: No, like I just scrape in batches of 15 until I've gone as far back as 7 days
  └ Niki D: So I'll get everything, it just slows things down
  └ [ME] Matthew B: I meant not even just lab notes channels, but all public channels (even new ones as they're created)
  └ Niki D: Well, I do need to add the bot manually to a channel to be able to scrape, so can't do new ones as they're created (afaik). But yeah? Could do channel summaries for all of them, not sure what the pros/cons are.
  └ Niki D: I could at least scrape everything and send it to you, even if TownCrier doesn't use all the channels
[ME] Matthew B: <!here> Niki and I are going to sync about Slack Bots at 1 PM PT, anyone is free to join

--- #surface-area-nuno ---
NunoSempere-cli: The decisionmate workflow reminds me of the delphi method; you might want to look into that! https://en.wikipedia.org/wiki/Delphi_method
  └ Nuno S: More thoughts: • Current demo shows users typing, but talking is much faster
  └ Nuno S: The interface doesn't spark a huge amount of joy to me
  └ Nuno S: Wouldn't bother about anonymity here, too complex
  └ Nuno S: Gradient descent/rapid iteration seems more meaningful to me than longer term backchaining here
  └ Nuno S: Nathan probably has some good thoughts here
  └ Nuno S: https://nathanpmyoung.substack.com/p/learnings-from-building-epistemic
  └ Gordon B: +1 I've used variations of Delphi before as part of scenario workshops with large numbers of participants.
  └ Alex B: > • Current demo shows users typing, but talking is much faster This is one of my biggest concerns. I agree that talking is much faster and where this will need to go. Have you seen good implementations of two-way voice interfaces? The things I've tried feel very clunky to me, even chatgpt's advanced voice mode. I'm hoping they will get better soon (e.g. ~6 months), but have uncertainty about that.


===== Fri, Aug 8, 2025 =====

--- #distillation ---
[ME] Matt B: Grain recording + transcript from today's call: https://grain.com/share/recording/f6e8b6e8-502b-4e75-a740-c37932a73ec2/biXrOqcO7uYRC7UQpUjsrwhROpwWzAwFHjPv6ceL Google Doc (https://docs.google.com/document/d/1ARuigNaR2i51h-8-XAeFt-ea8onuI2z0VqD-_Lams/edit?tab=t.0#heading=h.f4sj2hbsv5xc)
  └ Joshua L: thanks Matt!!
[ME] Matt B: AI likes the NotebookLM-like base layer + social + rev share idea, lol
  └ Joshua L: now we need gpt-5 to be really critical haha

--- #eng ---
  └ Joshua L: +1 re forkable neon dbs, a feature we needed for years.

--- #lab-notes-niki ---
  └ Emma K: Amazing! And yes it is - will post a more detailed update on it tomorrow in my lab notes channel

--- #meta-fellowship-tools ---
Luke H: Random idea: 1. Make a #poll channel, where where we can post questions and people answer with emoji-reactions. 2. Make a slackbot that comes up with a couple of interesting new polls every day that people might be split on (perhaps based on topics people have been discussing)
  └ [ME] Matthew B: I really like this idea! What if we started with a more official/detailed survey: • AI takes / timelines / etc. • AI for human reasoning takes / clusters / projects / focuses / impact stories? • A little bit of personal bio / skills / experiences (for the asks and offers bot and future bots/agents?) • Questions that FLF leadership wants to add for their sake Then once we analyze that survey we can find cruxes or interesting follow up questions and post them like once a day automatically in this poll channel
  └ Kathleen F: hey guys! this seems really helpful
  └ Kathleen F: How much momentum do y’all have for creating this? The FLF organizers are definitely interested in getting in on the action


===== Sat, Aug 9, 2025 =====

--- #distillation ---
[ME] Matt B: Owain Evans AI for Epistemics ideas: "Better evals for deep research tools and for other uses for LLMs/agents in epistemics" https://docs.google.com/document/d/1dRyqcGh_FC3PQm3Bgfh7TaBTG8ddoh2Oar0GGV9zUEo/edit?tab=t.0#heading=h.nu60kirmv6yz

--- #eng ---
Luke H: Curious to hear people's current setup for giving LLMs context on their lives (inspired by nuno's neat script in the linked thread). Also interested to hear how everybody feels about privacy concerns etc -- I'm excited by local models but I feel like I might be in the minority here!
  └ [ME] Matthew B: I think this is a very important/valuable topic and underdiscussed! LLMs get so much better with more quality relevant context, so I feel like even in this fellowship we're not saving/managing context as well as we should. For instance, ideally every call (with consent) should be recorded and have a notes doc and they should both be dropped into an LLM with a particular fellowship-related prompt to summarize it. Then we could have an every growing broad context about everything we're all talking about and working on I asked @Niki Dupuis to scrape every public Slack channel every day and I'm storing it in my Offers and Asks DB so I can improve the quality of the analysis of fellows and their skills --- But back to personal context. What I've done is: 1. taken the clearer thinking personality (https://www.clearerthinking.org/tools/the-ultimate-personality-test) test a. I asked for them to export all of my questions and answers, so I have all of those + the full report (Big Five, Enneagram, MBTI, etc. etc.) My report here (https://mattbrooks.xyz/personalityreport/) 2. taken this Saboteurs (Self-Sabotage) test (https://www.positiveintelligence.com/saboteurs/) 3. fed in my resume 4. then, used voice to ramble about my life in chronological order, talking about the different items that feel important to who I am, how I turned out, what I care about, etc. for about ~15 mins 5. created a twitter scraper (https://github.com/MattB543/analyze-twitter-profile) to parse my tweets, twitter likes and bookmarks and replies a. The output is 800k tokens I tossed into Gemini to summarize. That output is here (https://mattbrooks.xyz/twitteranalysis/) 6. Threw all of the above into GPT-4.5 and asked it what items I'm neglecting to share that would help it understand me better and did a few back and forths on that I'm trying to build up like 100k very high quality dense tokens about me in a way that would very much help AIs: 1. Negotiate on my behalf / share my preferences with others easier 2. Give me very specific advice / recommendations 3. Help me find biases or areas for growth (it's good at this! it can point out your shadows)
  └ [ME] Matthew B: --- I think this has really great potential to be a project for the fellowship - or a piece of a project. I think high quality personal context matters so much and has so many valuable use cases How could you trust an amazing negotiator bot to help you if the LLM doesn't fully understand you, your values, your context, history, personality etc.!?
  └ Alex B: Super interesting, thanks for sharing, Matt! Re personal context: It totally makes sense to me that you're experimenting with this. I'd be curious to hear any specific anecdotes about where this provided value for you. And curious to hear if you think this is "a reasonable thing you might want to try out" versus "everyone should absolutely make the time to do this".
  └ [ME] Matthew B: I don't think I've gotten a ton of high value out of doing this yet, but I do enjoy introspection / self-study anyway, so it's been generally fun / interesting I hope / expect the future value will come from plugging my extensive personal context into tools to personalize the output/flows So if I'm building some distillation modules I can say like "Pull out the information I'll find most interesting" Or "most confusing / most disagreeable" It would be basically impossible do that flow without great personal context, but will be trivial for me now Also, I plan to publish all of the personal context I'm willing to share publicly on my website and hopefully AI agents in the future can find it easily and use that info to interact with me and my agents better, etc.
  └ Alex B: > Also, I plan to publish all of the personal context I'm willing to share publicly on my website and hopefully AI agents in the future can find it easily and use that info to interact with me and my agents better, etc. This is an interesting idea, and potentially worth prioritizing while next-word-prediction pre-training is still a thing. Feels like there are pros and cons to an AI knowing a lot about who you are and how your mind works (thinking less about misalignment here, where there'd be bigger things to worry about, and more about other people or companies being better able to manipulate you or gain an information advantage). But more pros?
  └ [ME] Matthew B: Yeah, I totally understand there are pros and cons, but I think there are enough positive sum stuff I want to get into, and I can hopefully mitigate the cons with like AI assisted epistemics (like the agent @Luke Hewitt was talking about that helps spot bias or manipulation), so it seems likely very beneficial

--- #epistemic-evals ---
[ME] Matthew B: Gpt-5 bring a sycophant and not consistent: https://x.com/steveruizok/status/1954158353342492896 (https://x.com/steveruizok/status/1954158353342492896)

--- #fellowship-water-cooler ---
Blake B: I missed the boat on making a lab notes channel, and just did it. But I notice 1) I don't know if I can put it in the right section, and 2) I don't know that I can invite everyone to it so they can see it, without pinging a notification to everyone. Anyone know who knows the protocol?
  └ [ME] Matthew B: I think a Slack admin has to move it and set it up for you in the right way (@Niki Dupuis if you hard coded a list of lab note channels you might have to add this one)

--- #lab-notes-alex-van-grootel ---
Alexander G: Got this UX working yesterday - useful to get a sense of the sort of thing we're working on. I'm getting the ux populated with a more realistic run rather than this stub data. Obvious things to do in the future with various levels of priority: • there is much more that can be done on a single scenario analysis • add a multi-run workflow - where instead of setting up a single scenario, we do many at the same time. Both by branching of within a scenario and by changing the underlying assumptions of a run. ◦ that also adds a whole new dimension for analysis • benchmarking is prob pretty high prio in the next step. As in; does the backend actually result in realistic trajectories? In useful trajectories? Everything else is downstream of this • The way to ingest the action information is pretty bland. you just end up reading a bunch. Prob better ways to do this. • Adding a "pick your own stakeholder" game mode, where you play against the AIs as whatever stakeholder.
  └ Alexander G: feedback / hot takes welcome.

--- #lab-notes-matt ---
[ME] Matthew B: I think building a ton of personal context (or specific context for a task/project/whatever) is super valuable and underrated and dumped my brain on the subject here: https://aiforhumanreasoning.slack.com/archives/C097T56EKRT/p1754765337487239?thread_ts=1754706836.177899&cid=C097T56EKRT
[ME] Matthew B: On the Offers and Asks bot: 1. I'm going to create a V2 over the next few days that uses a lot more Slack context (messages and channels) to build a better bio for each fellow, creates a better ranking of the relevant fellows given your request AND explains why the AI thinks each fellow is specifically a good fit 2. @Niki Dupuis mentioned expanding and generalizing the idea a bit and test it with: a. Mox - to help people in the mox network find other people they can help / collab with / meet? b. Mutual Aide - help a particular mutual aide niche coordinate with some needs matching? (<#C099LFJ8GG5|>) c. I'm down to contribute to these two ideas but as of now I don't think I will push them forward myself (FYI @Niki Dupuis - let me know if you have any more specific ideas/plans and I'll see how much I'm excited to contribute) After I build V2 I'll see if it's actually being used / producing value and decide if there is anything else I want to add. But likely I'll pause there for now
[ME] Matthew B: --- Deeper Research MVP Idea: I want to run Deep Research on multiple platforms and intelligently combine / clean / analyze them (I selfishly want this product - which is good for an MVP) • Combine them into one report, noting which sections/text/links were the same and which were unique • Analyze the citations / links to ensure the content in the citation is accurate / seemingly truthful (@Joshua Levy is working on this idea) Problem: only OpenAI has a DeepResearch API (I believe) V1: • Explore and use what @Joshua Levy has built so far with • Export/copy the results from multi deep research runs manually see if I can produce an output I think is much better using LLM-assisted flows


===== Sun, Aug 10, 2025 =====

--- #eng ---
  └ [ME] Matthew B: Oh and I started but haven't finished these questions, as well: https://ggia.berkeley.edu/practice/36questionsforincreasingcloseness

--- #fellowship-water-cooler ---
  └ Ben G: updated!

--- #lab-notes-alex-van-grootel ---
  └ [ME] Matthew B: Random feedback (understanding that this is MVP V1) 1. there is going to be a ton of data / text / info to keep track of, spending time figuring out exactly what layouts / views / graphs are most helpful and how to easily hide / expand necessary info when needed will be important a. I felt like this was a limitation in the IRL AI 2027 - I didn't feel like I had access to much data at all, other than the small conversations I was in, or the post-it notes that were read 2. Final world state - it would be interesting to show how the hidden dynamics affected actions / outcomes a. Did the stakeholders figure out / guess / accommodate for these hidden dynamics? b. Did they miss key implications because of these dynamics 3. Very small UI/UX idea is to have the AI pick an icon / emoji for each bullet point, for like the key outcomes a. [handshake] Wyoming-Japan parternship b. [chartwithdownwards_trend] China's market share reduced c. etc.
  └ Alexander G: Thoughtful Matt! Thanks Yeah interesting especially about the dynamics analysis. There is something in there for me about really rewarding running tests and experiments in game, and updating with new info - in the way I would hope we would in reality
  └ [ME] Matthew B: Interesting to think that after you run it the first time with a group, they'll then understand the hidden dynamics so you'd either have to generate new hidden dynamics for the re-run or understand that they will now more correctly model the hidden stuff
  └ Alexander G: I think we’d need to remind people that it’s not whether the players understands the dynamics but whether the stakeholders they represent understand the dynamics

--- #surface-area-nuno ---
  └ [ME] Matthew B: it's easy to have a built-in audio-in feature in your app instead of typing, so people can just click the mic button and record their rambling and then it transcribes That would be the very easy V1, but idk how much people prefer talking to typing, but giving them the option seems nice


===== Mon, Aug 11, 2025 =====

--- #eng ---
[ME] Matt B: OpenAI created an automated prompt optimizer for GPT-5: https://platform.openai.com/chat/edit?models=gpt-5&optimize=true

--- #fellowship-water-cooler ---
  └ Jamie J: @Ben Goldhaber would you also set up mine so it is in the right place, please? Mine doesn't appear to be
  └ Ben G: @Jamie Joyce have you created the channel? I don't see it - but if so add it here/dm me and will add

--- #lab-notes-matt ---
[ME] Matt B: I communed with the universe this weekend ([mushroom]) I decided I needed to clarify my own map of the fellowship / idea space, so I mind dumped into Claude, added a bunch of Fellowship context, and then co-wrote this 13 page overview doc with Claude (https://docs.google.com/document/d/1Z4ObJB0RgIrCHIopINmMjtgmEDO5OsburR6M7zKkTyY/edit?usp=sharing) You can probably skim the whole thing in 15 minutes, I tried to format it to be very readable. Here is the outline: • 1) The Coordination Stack - Eight layers of coordination, the main bottlenecks, and why “good” coordination struggles to spread. • 2) The Pattern in Practice - What works and fails across six scales (individual → global), plus the common traits of successes vs. failures. • 3) The AI Breakthrough - How near-term AI can ease each bottleneck, concrete example use cases at every level. • 4) The Fellowship - Starting building the coordination layer now: recruit builders, ship prototypes, test in the wild, and scale what works. I aim to use this doc + generalized Fellowship context to build a large prompt / rubric to help me think through projects/ideas. We can also build a big doc someone can dump into an AI to understand what we're even doing here. Like to quickly onboard someone conceptually so that they can use their on the ground expertise to give us insights we need to make better decisions. I'm very open to comments, pushbacks, riffs, etc.
  └ Tamera L: I thought this was really nice, thanks for sharing it! I liked the overviews of coordination patterns and bottlenecks, and examples and counterexamples of what coordination looks like at those different scales. I thought it was a nice taxonomy.
  └ [ME] Matt B: Thank you! That's very kind
  └ Ben S: This was a great read! Might be worth publishing more broadly even, like on your blog
  └ [ME] Matt B: Thank you @Ben Sklaroff [pray] I'll post it on my website (why not) - but I don't have any viewers/readers/distribution, lol I'm trying to think how this could be used to help point out opportunities that are being missed, or red team ideas, or something
[ME] Matt B: --- I decided to write out the chain of events that brought me to this fellowship because it’s an interesting story about: • The desire for human flourishing • Attempts at global coordination • Strong signaling • Search problem & bandwidth limitations • Reality consensus • Value mesh & negotiation • Resource allocation • Network effects • Meta-coordination 1. Satoshi builds bitcoin to globally coordinate a financial system without a corrupt/extractive centralized layer. 2. Vitalik finds this idea enticing, he also wants to coordinate on a global scale for human flourishing, and decides to try to build a more generalizable powerful crypto model. The world computer of Ethereum. 3. Ethereum explodes in value, Vitalik becomes rich, and there is so much frothy capital being thrown around that he is gifted hundreds of millions of dollars by random crypto projects. 4. Vitalik knows he isn’t able to personally distribute the funds in the most positively impactful way, so he donates it to various organizations including the Future of Life Institute. 5. FLI spins out FLF when it realizes there aren’t enough high quality high leverage projects that can receive funding & scale. They made FLF to find gaps, design new orgs, headhunt founders, and support launches 6. FLF puts out a credible signal in the form a fellowship. 7. I pick up that signal through Scott Alexander and the 80,000 hours job board a. (it would be interesting to know how many other very qualified fellows could be brought into the fold but because of the signaling and searching problem it didn’t happen) 8. I credibly signal & communicate to FLF with my resume, application, and interview. 9. FLF accepts me into the fellowship. So because Satoshi built Bitcoin for human flourishing and global coordination, Vitalik built Ethereum, the world gave him a lot of money, he redistributed it to a credible organizations trying to do good, who has distributed some of that money to me to operationalize that into potentially impactful and scalable MVPs. If something is untrue about the chain of events, let me know, as this is only my personal understanding. I just find it interesting to think of the context. Basically I see it as proof that capital/resources is not a major bottleneck. Smart good people want humanity to flourish, people want to build tools to allow that to happen, attention/capital flows where it can, etc.
  └ Owen C: Nits on 3, 4, 5 that I think don't change the basic picture: • 3 -- I think Vitalik was given a lot of Shiba Inu coin as a ~joke when it was essentially worthless, and then it subsequently spiked to value his holding at billions • 4 -- guess there may also have been political considerations here : he may not have wanted to be seen to profit from this joke • 5 -- on spin out, I think FLF didn't really have this focus, and it was more centred around making new good orgs of whatever types (working on this came a bit later)
  └ [ME] Matt B: 3. Yeah, mostly agree, he was still somehow gifted hundreds of millions of dollars. Even if it was mostly a meme/joke, but it was also a way for projects to attempt to signal credibility, etc. 4. Maybe, I don't know him obviously, but it seems like he didn't want to personally benefit from the meme at other people's expense, and he thought he could maybe use the memetic force for good, like maybe the value wouldn't plummet as much if it was going to good things 5. ah interesting, yeah I guess I didn't actually know the original thought behind the FLF spin-out, I should edit that to make it more high level.
Luke H: @Matt Brooks given your all-in vibe of "I'm gonna give AGI as much data about me as I can possibly generate", shall we just like finetune a model right now on all the data you have and see how accurately it can simulate you?
  └ Luke H: maybe get GPT to generate like 1000 poll questions and just have you spend a few hours answering all of them and throw them into the mix
  └ [ME] Matt B: yeah, I would find that interesting Do you think finetune is better than just dropping in 500k of context into Gemini?
  └ Luke H: idk I feel like both have kinda different strengths, wouldn't necessarily be so hard to try both and compare them on a held-out test set
  └ Luke H: you could make an @ai-matt slackbot lol
  └ Luke H: and then people chat to it
  └ Luke H: and we can turing test it
  └ [ME] Matt B: lolll... create a breakout group where you guys type in the AI Matt channel and there is a 50% probability I reply or the bot replies. You guys have to guess which is which

--- #meta-fellowship-tools ---
  └ [ME] Matt B: I really want to make this happen, I just don't know exactly what I want to ask in the survey yet... are there pieces of data/info that would help unblock the largest amount of people? Or like help us coordinate at the end of this explore phase? @Luke Hewitt did you still want to create a survey to track trends/opinions at the start and end of the fellowship? @Kathleen Finlinson does FLF leadership already have an idea of what questions/topics you want to ask fellows? --- Potential questions: Project & Focus 1. The One-Liner: If you had to describe your primary project or area of exploration in a single sentence, what would it be? 2. The "Hair-on-Fire" Problem: Who is the specific user or group whose problem you are trying to solve? What is the "hair-on-fire" pain point that makes them desperate for a solution? 3. Core Hypothesis: What is the single most important hypothesis you are testing right now? Random question ideas: 1. What is the most valuable or surprising thing you've learned or realized in the last two weeks, related to AI for Human Reasoning? 2. What mistakes / misses do you see happening the most in the fellowship so far? 3. What is one open question you're wrestling with that you'd love to hear other fellows' perspectives on? 4. Which internal fellowship process (e.g., project discovery, scheduling, sharing feedback, resource allocation) do you think would be the most valuable to improve using our own AI-for-coordination tools?
  └ Luke H: I was imagining I would want to make a quantitative survey of maybe like 30-40 questions on likert scales (so that people could bash through all of them in 5-10 mins without it being a big burden)


===== Tue, Aug 12, 2025 =====

--- #distillation ---
[ME] Matt B: @Joshua Levy I tried to quickly hack together a deep research combiner with kash but I'm kind of failing (my "Combined Analysis" button to trigger the modal to open is hidden) https://github.com/MattB543/kash-matt-test/tree/matt-test I'm definitely interested in your https://github.com/jlevy/superanalyze If you're free tomorrow after 2 PM ET I'd love to hop on a call and discuss - or maybe you can present it at the lighting talk tomorrow I really want a Deep Research combiner + checker - for my own selfish information searching reasons [grin]

--- #eng ---
  └ Kathleen F: Love this thread. I personally use Claude extensively for personal advice in a high-context project. My system is much less sophisticated than Matt’s but maybe simpler to implement. • I started by having one-off conversations with Claude where I described particular problems I wanted advice about. • When these convos got too long, I asked Claude to summarize them and then used them as a starting prompt for a new convo. • I repeated this process until I had Claude-generated summaries about quite a bit of my life context, and I dumped this in a text file and attached it to a new Claude project. • I continually add new text files to this project as my life updates (generally by having Claude summarize conversations I’ve had in the project).
  └ [ME] Matt B: ah interesting, I like that flow too. Your system seems exactly as sophisticated as mine, lol! I just also take some surveys and dump that into my Obsidian so I can copy and paste it into prompts, etc. - so it's not only chats with LLMs I'm very interested in building out a very high quality and semi-automated flow to generate tons of context about people. Maybe I need to start with the AI Matt idea and see if it's actually useful in enough contexts

--- #epistemic-evals ---
[ME] Matt B: Interesting thread https://x.com/lefthanddraft/status/1954865556441739301?s=19 (https://x.com/lefthanddraft/status/1954865556441739301?s=19)

--- #general ---
Emma K: Recommendation systems contact: I talked to a recent MATS fellow last night about her research on recommendation systems aligned to genuine user desires. She’s looking for potential collaborators/cofounders for building something in that space applied to news or social media. I don’t think anyone here is thinking along those lines? But she asked me to pass on her email to anyone in the fellowship who’d want to chat (More generally will say I’m quite excited about improving information ecosystems by giving people AI digital media curators that represent their expressed preferences (by comparison to the status quo of everyone being at the mercy of psychologically-exploitative algorithms); I think there is some AI4HR potential there. I’m not actively exploring that myself rn, but would be interested to discuss if someone is)
  └ [ME] Matt B: I'm very interested in the idea of giving AI a ton more context about myself so that it knows my values, preferences, skills, context, etc. etc. which seems important for AI recommenders, assistants, negotiators, etc. Although, I haven't thought about recommendation systems at all, and don't have any experience in that space Can you pass my email on to her? <mailto:matthewrbrooks94@gmail.com|matthewrbrooks94@gmail.com>
  └ Emma K: Nice! Actually maybe better I post her contact here and people can reach out if of interest: <mailto:Addiefoote8@gmail.com|Addiefoote8@gmail.com> <tel:6513634377|6513634377> (text or signal)
  └ Ben G: Ivan Vendrov and Xiq would be interested in chatting w/ her!
  └ Paul F: I'm interested too! I'll be posting some stuff about "genuine desires" soon, but it's a question I want to pursue more and have been thinking about how to make more concrete, so this sounds like a good opportunity to explore.
  └ Herbie B: @Matt Brooks: see https://workshoplabs.ai/ that said, I expect this to just happen by default when the labs crack continual learning, which enables personalized AI agents
  └ [ME] Matt B: interesting, signing up for the waitlist, ty for sharing
  └ Ben W: icymi: https://forum.effectivealtruism.org/posts/xzjQvqDYahigHcwgQ/aligning-recommender-systems-as-cause-area

--- #lab-notes-matt ---
  └ Ben G: epistemic status: communed with the universe
  └ Ben G: related: https://www.narrativeark.xyz/p/epistemic-status-poetry-and-other
  └ [ME] Matt B: > epistemic status: communed with the universe lolll, meaning take these ideas with a heap of salt
  └ Kathleen F: “capital/resources is not a major bottleneck. Smart good people want humanity to flourish, people want to build tools to allow that to happen, attention/capital flows where it can, etc.” I love this! an important insight, I think
  └ Niki D: Speaking of, did you guys see the Daniel Dennett bot (https://arxiv.org/abs/2302.01339)?
  └ Niki D: It convinced me that the bar is possibly very low for making a bot good enough to convince people.
  └ [ME] Matt B: I wonder where "convince people" breaks down though. Like if you only ask it certain questions / topics I think it will nail it. But if you try to red team / break it, where does it break?
[ME] Matt B: Things I might think about / work on (input is very welcome) 1. Improving Offers / Asks bot - have it automatically comment on Slack messages that are asking for input/feedback/help 2. AI Matt Bot - give a whole bunch of context / examples about me to AI and see if it can pass a turing test in Slack during a breakout session 3. Notebook LM 2.0 - think and write more about what the MVP , what the ICP, counterfactual impact, and long term goals would be (@Alejandro Botas, @Ben Sklaroff and @Joshua Levy seemed interested) 4. Create a fellowship survey with @Luke Hewitt and @Kathleen Finlinson
  └ Alex B: > Improving Offers / Asks bot I think there's potential for this to add material value over the last couple of weeks of the explore phase and as folks decide what to focus on in the build phase. > AI Matt Bot I'm personally intellectually curious about this and would be excited for you to do it from that angle. I'm uncertain about the impact story, but I do think there could be useful learnings you could write up and that it wouldn't be totally frivolous.
  └ [ME] Matt B: > I think there's potential for this to add material value over the last couple of weeks of the explore phase and as folks decide what to focus on in the build phase. interesting framing..... it would be cool if it helped cohere clusters, project ideas, collaborators
  └ [ME] Matt B: @Alex Bleakley the cool thing about AI Matt Bot is that if I feel like it really does represent my values/preferences/whatever pretty well, we could plug it into DecisionMate or Pivitol and let it participate for me in these contexts. If I can pay $1 in LLM credits instead of spending 30 mins interacting with something physically, that's a huge win, even if it only represents 80% of my preferences (in certain use cases)
  └ [ME] Matt B: > I think there's potential for this to add material value over the last couple of weeks of the explore phase and as folks decide what to focus on in the build phase. @Kathleen Finlinson is explore phase supposed to be ending this Friday? Maybe the survey could be used to help people transition / coordinate from exploring to building?
  └ Kathleen F: This Friday is the end of week 3. Week 4 will be when we transition out of the explore phase (technically explore lasts through the end of week 4 but it’s a fuzzy boundary)


===== Wed, Aug 13, 2025 =====

--- #demos ---
[ME] Matt B: I'm re-using the Slack database I've built into a semantic search + summarize web app https://flf-fellowship-codex.pages.dev/ Password: 179a839f-750f-4c97-8dbc-bd5271ca7239 If you want to keep up to date on certain topics / projects / clusters you can search for it and then have AI summarize it
  └ Nuno S: Very nice

--- #distillation ---
[ME] Matt B: Call from 8/13 Breakout session: https://grain.com/share/recording/dd0c6965-0413-4a0f-b0c5-dbdb5f645bd8/zkFj03xlzcJKcRtdyHlUx6W0S7rLn22fKTFtuhwm (summary in thread)
  └ [ME] Matt B: 1. Project Exploration and Collaboration The group discussed building a "knowledge Ops platform" or "Notebook LM 2.0" with several key components: - A personal context layer that understands an individual's: - Worldview - Interests - Preferred sources - A knowledge operation layer for: - Extracting claims - Checking claims - Filtering citations - Combining documents[1] 2. Personal AI Agent Development Matt is working on creating a personal AI agent by aggregating data from: - Slack messages - Text messages - Twitter With the goal of making an AI that understands his worldview and personality[2] 3. Potential Product Ideas The group discussed the growing importance of: - Matching high-impact people to communities - Creating gated communities - Solving search problems as the online world becomes more complex[4] 4. Knowledge Repository Concepts Ben introduced the idea of "content gardening" - a semi-maintained list of resources that: - Individuals have agency over - Can be slowly maintained and grown - Remain high-quality over time - Don't become unmanageably large[3] 5. Technical Collaboration Joshua emphasized the potential value of agreeing on basic workspace conventions, such as: - Using consistent file formats (like YAML headers with Markdown) - Creating purpose-driven repositories - Enabling easier integration between different tools and actions[5] 6. Individual Focus Areas - Alejandro is working on finding datasets to measure: - Active claims - Claim consistency - Developing metrics for evaluating claims[6] - Ben is primarily focused on developing Pivotal, aiming to create an MVP for scheduling within a week[7] - Joshua is working on: - Assertion extraction - Creating basic actions - Developing a locally runnable REST API[8] The overarching theme was collaborative exploration of AI-powered tools for knowledge management, content analysis, and personal productivity.

--- #general ---
[ME] Matt B: FYI - it's really helpful if you record your breakout session or project catch-ups calls and share the link + the transcript in Slack like this (takes 2 mins) https://aiforhumanreasoning.slack.com/archives/C097Z6QB9QD/p1755107362646519 • If you put a transcript in your lab notes channel @TownCrier will pick it up • If you put a transcript in any public channel, my Fellowship Semantic Search (https://aiforhumanreasoning.slack.com/archives/C097X9QTMK6/p1755048000614019) will pick it up and it will be included in all future searches / summaries

--- #lab-notes-jamie ---
Jamie J: Notes on Projects Pushed Forward 8/12/25: • Writing a post-fact-checking prompt to abbreviate findings to fit character length (Community Notes AI) • (continuing) QA'ing 500 "lightning searched" (internal tool) claims to be summarized, grounded, and consolidated in a report. • Landed grant to offer bounties for epistemic tooling (will create docs and share with this cohort, perhaps we can collab on specs/needs to get common tools built out) via a convo with a donor • Lengthy meeting with potential pilot partner who wants to use AI deliberation/mediation tech industry-wide (will create docs and share with this cohort). Hoping to match-make/make intros from this cohort to potential external partner (notes (https://docs.google.com/document/d/1tcNdDw_56uzZ5pnBX6aG7hv--rIiwzL36RDvoqYdBcc/edit?tab=t.0))
  └ [ME] Matt B: Wow, that potential pilot with Daniel Kwan seems super cool. If it turns into something more serious that needs more fellowship support I'd be very happy to chip in I loved Everything Everywhere All At Once and thought in the past if we could get a powerful and believable "AI danger" movie, it could help push the global discussion out of the "deep future" and "Sci Fi" zone and into reality for deliberation The Day After and WarGames both scared Reagan and definitely made large positive impacts on the world
  └ Ben G: Congratulations on the epistemic tool grant and this partnership opportunity. +1 to Matt's notes that film seems super relevant for influencing global discussion/culture. Besides the folks you mentioned in the notes w/ DecisionMate @Alex Bleakley and Polis II @Colin Megill, also the cultural influence aspect reminds me of a conversation I had with @Blake Borgeson at one point, I think around Uptrust.

--- #lab-notes-matt ---
  └ Oly S: My pre-riff on this is a slide deck on coordination (https://docs.google.com/presentation/d/1RYG8yjsHzNLPzzKpWTsig0RkC9J3Z4Z1MgkCuaca-Nc/edit?usp=sharing) which I cut out of the retreat programming based on estimates of timing and appetite, but which I was wondering about sharing some other way. [ETA:] (Most of the discussion is in the slide notes; slide content are predominantly visual aids only.)
  └ Oly S: I super love the ambition and scale implied in this discussion! And I have a soft spot for systematising/taxonomic discussions.
  └ [ME] Matt B: Ah, very interesting! I just flipped through the slides and seems we are thinking similarly but slightly differently Like I agree with everything you're saying I just happened to approach it from a slightly different frame/angle I'll throw it all in Claude and see if there are any places we actually differ in thought
  └ Oly S: Yes, I agree we're basically using different words to lossily describe basically the same picture. I suspect, if an attempt at exhaustiveness is made (I haven't, and I don't think you have), the union of challenges/activities from our two takes would be better than either alone.
  └ [ME] Matt B: Yes, I agree, I think my take covers like 10% of the picture. Adding together our frames maybe takes that to like 15%, but I feel like an exhaustive understanding is basically out of reach currently because of how complex/deep/wide the idea space is Like you'd have to combine individual psychology with game theory with social dynamics with (continue for 50 more fields of study) - and then somehow distill all of that down into a clean/complete understandable picture I wonder if future AIs will be able to intuit stuff like this and that's why they'll be able to coordinate much better/faster (acausually?) --- Opus 4.1 says these are important areas one of us talks about that the other didn't mention: Your concepts that I'm missing: • Coordination ≠ alignment - enemies can successfully coordinate while remaining adversarial • Normplexes - self-reinforcing cultural systems that propagate and enforce coordination norms • Coalition discovery - recognizing that coordination is even possible is its own distinct challenge My concepts that you're missing: • Developmental mismatches - people at different cognitive stages have incompatible coordination operating systems • Signal decay at scale - trust degrades with each hop in a network • Time horizon conflicts - actors on different timescales (day trader vs climate scientist) can't coordinate • Internal coordination - alignment within a single person between conflicting drives/values
  └ Oly S: Claude sounds about right to me on that! (I reckon a nudge would surface expansion on those points and additional points.) From top of mind I also think I discussed some more activities in enactment/enforcement that I didn't see in your doc at a skim, and I expect you have additional points. It's all tasty tasty context

--- #small-group-decision-accelerator ---
[ME] Matt B: Random product ideas: 1. I think starting with a survey (async?) would be a good way to get initial data and ground out the different clusters / cruxes a. You could seed a survey with AI suggestions if you think the topic is broad enough but allow people to skip questions they think are not valuable b. You could allow people to add their own questions and then vote on them, so could be like 2 survey rounds. 1 is async before the call, you have the option to add questions, then if there are questions you didn't answer yet you can do that at the start of the sync session 2. I'm building a Matt-GPT copy of myself, it would be interesting to see how the agent answers surveys on my behalf a. would also be interesting if every interaction I had to do for decision mate could first flow to my agent and then I just approve/adjust the outputs of my agent i. could be async or sync 3. It would be cool to do an A/B test with the same people with Decision Mate and without on a task/topic that could be repeated a. to see the time difference, outcome difference, process difference, etc. b. idk what the task/topic could be though... like if the FLF leadership wanted to create two events during the fellowship. Coordinate on the event details with Decision Mate and try the second time in a Google Doc / Meeting call or something
  └ Alex B: Riffing on 3, it would be interesting to do an RCT (DecisionMate Vs video conferencing) with random groups of volunteers discussing the same topic, to see the time taken and also gather qualitative data like how they feel about the process, how they feel about their group mates, how they feel about the final statement.
  └ [ME] Matt B: yes, exactly


===== Thu, Aug 14, 2025 =====

--- #coordination-and-ai ---
[ME] Matt B: Meaning Alignment Institute is looking for alpha testers for their "meaning economy" social app: https://meaningalignment.substack.com/p/looking-for-testers-for-a-social Also, someone commented on their Substack saying he's building something similar: https://github.com/ekoori/TrustSphere

--- #distillation ---
Steven I: Here’s an example of fact checking an important government doc, Trump’s 140-page report – “A critical review of impacts of greenhouse gas emissions on the US climate.” The fact check was done by hand by dozens of climate scientists. It includes an interesting data visualization. “https://interactive.carbonbrief.org/doe-factcheck/index.html I can imagine several ways of improving it: 1) shades of red/orange to indicate severity of distortion, 2) change the size of the page to reflect the number of problematic claims, 3) they use white to indicate both “stated as accurate” and “no comments received” - I think they should be different colors. @Joshua Levy
  └ [ME] Matt B: very cool

--- #eng ---
[ME] Matt B: Had a good one on one call with Josh recording and transcript here (https://grain.com/share/recording/715336bb-84e2-42db-b2f9-bc106b03e7c2/r2dA7qAkdwD4MHQZ35Txvo5dYbux8eR30ikyfDOM) (summary in thread) My MVP idea: 1. Chunk and embed all Slack and Drive data into a clean format 2. Allow users to do semantic searches across this content & extract what they want 3. Enable users to drop in additional documents or links to expand the context 4. Provide a chat interface where users can interact with this aggregated information 5. Add cool AI add ons like find similar links given your goal/framing His comments and links about tools / kash: For quick RAG basic python template https://github.com/jlevy/simple-modern-uv and use llamaindex for indexing/ingesting https://docs.llamaindex.ai/en/stable/ For kash: markdownifydoc in kash-docs is good for converting all docs to markdown Uses markitdown and mammoth for docx and marker for pdf Clean html via readabilipy Kash chat format (https://github.com/jlevy/kash/blob/main/src/kash/utils/fileformats/chat_format.py) kash-docs (https://github.com/jlevy/kash-docs) has all the docs related actions Current state of kash repos (no promises on stability rn!): • kash is the main kash-shell and it includes all the basic framework + shell • kash-docs is a “kit” of extra actions and libraries • kash-media is a “kit” on top of kash-docs • always run kash-media for local/dev use if you want all actions • kash-experimental https://github.com/jlevy/kash-experimental has llama-index
  └ [ME] Matt B: Call Summary: 3. Fellowship and Project Discussion - Matt expressed uncertainty about his fellowship project and was curious about Joshua's work - Joshua explained his approach: - Not seeing himself as a researcher anymore - Wants to build products - Interested in open-source development - Skeptical of the traditional venture capital startup path 4. Technical Exploration - Matt described his MVP idea: - Improve semantic search - Pull information from Google Drive - Allow users to drop in documents or links - Enable chatting with the context - Joshua suggested tools like: - Llama Index - LangChain - Pydantic AI 5. Collaborative Tools and Infrastructure - Joshua explained his repository structure: - Main repo: cache (package and shell) - Separate repos for docs and media to manage dependencies - Trying to keep core functionality stable while allowing experimental features - They discussed E2B (a development platform): - Allows remote tool execution - Provides transient workspaces - Potential for web deployment - Flexible for different use cases 6. Closing - They discussed potentially syncing again next week - Joshua is working on an analysis tool - Aims to have something ready by the weekend
  └ Ben S: in case you haven't seen it, onyx is an open source version of glean and the code might be worth taking a look at (I haven't perused it myself but it sounds like they might have done something similar to what you're talking about): https://www.onyx.app/

--- #epistemic-evals ---
[ME] Matt B: Btw, @Paul de Font-Reaulx and @Alejandro Botas have you guys chatted about epistemic evals? I know Paul has a ton of philosophy/theory in his head (and in the Google doc) and Alejandro already built some first attempts at evals. Maybe Paul has suggestions on the process/prompts/data?
  └ Paul F: Yes! We chatted earlier this week and stay in touch!

--- #epistemics-and-ai ---
Alex B: Does anybody have explanations/hypotheses for why perplexity doesn't appear to be building and leveraging any kind of internal knowledge base? Why start from scratch for every query?
  └ [ME] Matt B: Quick thought: 95% of their users / queries use it as one-off Google search replacer. They don't care about building a knowledge base for themselves because they wouldn't know how to use it or even what that is But do you mean perplexity building it in the background to better serve future queries for that user? They probably are, just not showing you that
  └ Alex B: I mean building in the background to better serve future queries for all users. Instead of synthesizing a bunch of sources every time someone asks about topic X, store that synthesis and update and improve it over time. Do you think they're doing this?
  └ [ME] Matt B: Hmm... 1. Either they are dumb and aren't doing it but should be (eh, doubt it) 2. They tried it, and for some reason it doesn't outperform the quality of doing it from scratch every time (could be) 3. They're doing it in some sense and just not telling you (fairly likely) People would be more suspicious if they say like "We've already built a knowledge base about X" - they much prefer to see a bunch of "trusted sources" pop up on their screen
  └ Alex B: Maybe they are doing something, but if they were doing the thing I have in mind, the three answers below would share a lot of text. I agree (1) seems unlikely. But (2) also seems like... surely there is value here, no? Maybe it's prioritization? Maybe it's: (4) They think people prefer the idea that it's 100% based on live search and inference and the best way to make it look like it's 100% live search and inference is to just do that. Maybe this makes sense if they see google search as their primary competitor, so they want to provide a clear pareto improvement over google search, rather than reimagining information retrieval? I still feel like I'm missing something.
  └ Luke S: I think this is pretty hard to do well at scale, unless the queries are identical, in which case you can just cache (if you even care about saving the money on duplicate queries). It starts to shade into “automatically build/extend/refactor an ontology”, which shades into “automatically grow and maintain a codebase”
  └ Alex B: How do you feel about trying to manage this with the equivalent of Wikipedia's WP pages as the rules for how to manage the ontology?
  └ Luke S: As instructions about categories for LLMs, or something else?
  └ Alex B: Yes. I mean, Wikipedia manages the question of what pages should exist, what should they include etc. by having rules and guidelines, with humans interpreting those rules and guidelines case by case basis as they go about their business. I'm imagining LLMs interpreting such a set of rules. For example, if a page doesn't exist and the LLM wants to create a new page, it automatically reviews the relevant guidelines and makes a call. You'd want to be thoughtful to avoid noisy decisions creating churn, but that feels achievable. You can still have humans managing the rules and guidelines. Then, instead of duplicating work when talking about the same topic a million times, the same amount of inference is spent instead on improving the quality of the underlying knowledge base.
  └ Luke S: Collaboratively building a wiki might be a really cool experiment for AI Village. I’d expect it to succumb to entropy since current agents are too short-horizon to keep all the pages thoughtfully organized, even with guidance, but I might be wrong, and it might be an interesting baseline regardless. Our vision at Elicit (I wouldn’t quite call it a plan yet) has been to nail this at the level of individual/team projects (where the benefits of well-organized information are more tangible), then expand outward

--- #fellowship-water-cooler ---
Gordon B: IRL tech trees https://overcast.fm/+ABNn0ZqTOBk (https://overcast.fm/+ABNn0ZqTOBk)
  └ Gordon B: https://www.historicaltechtree.com (https://www.historicaltechtree.com)
  └ [ME] Matt B: Foresight has done a lot of work here as well: https://foresight.org/tech-tree/

--- #lab-notes-matt ---
[ME] Matt B: Okay updated list of small things I'm going to hack on: 1. Making offers and asks bot suggest matching opportunities for the people that opted in 2. Finish setting up / fine-tuning Matt-GPT a. Run it against the transcript of the DecisionMate demo I took part in and compare 3. Play around more with all of the fellowship context: a. Improve the semantic search platform by adding chunking / RAG for all of the tokens to let people pull out the info they need b. Create a red teaming / ideating rubric for people to brainstorm their different product ideas / paths 4. Think more about the broad "Notebook LM 2.0" idea a. See if I can build a simple yet useful platform where I dump in docs / ideas I like, dump in a ton of personal context about me, and use exa AI and web search LLMs to try and find interesting links / docs / communities I'd find a lot of value in

--- #small-group-decision-accelerator ---
[ME] Matt B: Btw @Luke Hewitt you've thought a lot about persuasion, right? Your expertise mentioned AI persuasion capability evals One of my worries about DecisionMate is that it could just persuade the weak-willed people in the group to go along with the people less suspectable to AI persuasion Or it could persuade everyone to go down a particular thought path, or agree to a simple lowest common denominator (that is counterfactually worse) @Alex Bleakley wouldn't persuasion evals fit into the explore/testing phase nicely?
  └ Alex B: I'm somewhat confident that we can address this by providing the right instructions to the AI to explicitly tell it that its job is not to persuade, but rather to help a group of humans get to their own decision. I'm not saying it's as simple as "just tell it not to" but I do feel like this can be mitigated with careful design and experimentation. I'm not prioritizing it for the explore phase because there are other things I'm less confident in that I want to de-risk first. Let me know if you think I'm thinking about this wrong.
  └ [ME] Matt B: hmm... yeah that makes sense But I feel like there is something adjacent here that isn't persuading but like the AI has certain personalities, preferences, etc. baked in from the RLHF and it might just naturally slip down certain paths more than others in a somewhat small/subtle way that isn't very obvious But because it's nudging 6 people 50 times each = 300 nudges in a decision, it could be pushing the idea space around more than you'd like But maybe if you're just super specific and strict in the prompt that it shouldn't input any of it's own preferences/ideas - but I guess I'm just worried there is implicit biases (shout out to epistemic evals) Anyway, you're right, not the largest concern or source of uncertainty
  └ Owen C: Some takes (not super carefully considered): 1. Seems important in the long-run if this kind of tech takes off to think about how to do well here and avoid adverse effects 2. Doesn't seem like a key hurdle to be overcome early in terms of "can you do something successful here" 3. Poses a Q of "does this concern loom large enough to not want to pursue this tech?" ◦ Mostly I think "no", in part because I think we're just going to get this tech at some point, and things like "how careful/responsible are the market leaders?" could really matter 4. Nudges are not axiomatically bad ◦ (some might be quite good; but this is a fraught area and it's worth being nervous about that line of thinking)
  └ Alex B: > But because it's nudging 6 people 50 times each = 300 nudges in a decision, it could be pushing the idea space around more than you'd like > > But maybe if you're just super specific and strict in the prompt that it shouldn't input any of it's own preferences/ideas - but I guess I'm just worried there is implicit biases (shout out to epistemic evals) I basically agree that this subtle influence is a concern and something that should be top of mind. I imagine studying this more closely in future. But I'm less concerned that this threatens the entire goal of the project to empower/improve human reasoning and decision making. (I'm focusing on things like subtle biases here as opposed to "dangerously misaligned AI with its own goals".)


===== Fri, Aug 15, 2025 =====

--- #distillation ---
Alejandro B: After a convo yesterday with @Joshua Levy @Ben Sklaroff @Matt Brooks i wanted to try to quantify coherence of some docs. • Main takeaways worth sharing it seems like you can find real 'coherence of claims' signal even with totally subjective LLM graded metrics. Within a doc or a set of docs. probably warrants some attention given that fact-checking is somewhat fraught
  └ Alejandro B: I made an 'substack analyzer' (which i totally doubt is the best use case) • extract all claims (like falsometer/Luke's paper he shared) from a doc or docs • then pairwise for all claims across all docs, grade if claim A how much more likely is claim B ◦ i used an oped from David Brooks in NYT about AI is going to just be so great vs A substack from owen about how we're entering 'the crucible'. If you create a coherence matrix Owen's claims correlate with owen's other claims being true and DavidBrooks's claims correlate with david brooks's other claims being true, and there's significant dissonance between their claims.
  └ Alejandro B: You can play with it here: https://ai4hoped.streamlit.app/ though the $ on the api key will run out so might be best to load the example from cache

--- #eng ---
  └ [ME] Matt B: Interesting, thanks, I hadn't heard of it (too many new damn things in this world.... ironically)

--- #epistemics-and-ai ---
Ben G: https://blog.cosmik.network/cosmik-grant-open-phil-astera
  └ Ben G: cc @Matt Brooks sensemaking
  └ Ben G: while v cool, I'm skeptical of this push to make Bluesky a thing

--- #lab-notes-matt ---
  └ Nuno S: > 1. Making offers and asks bot suggest matching opportunities for the people that opted in > I want to be opted into this? Who should I be talking with more?
  └ [ME] Matt B: Okay, I'll opt you in (and I still have to build the feature to have it actively look for matching opportunities) --- But in the meantime, I dropped all Slack messages and docs into Gemini and it said this regarding who you should talking with more: Nuno's profile is that of a pragmatic, technical builder focused on large-scale, automated information aggregation for global risk analysis (his Sentinel project). He is skilled in programming, forecasting, and impact estimation, with a strong interest in geopolitics and the practical application of AI tools. Here are the most promising, currently undiscovered collaborations for him: --- 1. Geopolitical Simulation & Scenario Planning Cluster Fellows: Alex van Grootel, Tamera Lanham, Gordon Brander. These fellows are building AI-driven engines for strategic foresight and tabletop exercises (TTX). • What Nuno should do: Provide the "Reality Engine": Integrate Sentinel's real-time global risk data feed to power their simulations, turning static scenarios into live models. Co-design the "Game Master" AI: Use his forecasting expertise to help build the core AI that determines simulation outcomes. Contribute his wargaming software: Merge his existing INLINECODE0 tool to accelerate their technical development. • Why: Nuno has the real-time data they need for their simulation engines. This makes their projects credible and gives Nuno a powerful way to test the consequences of the risks he tracks. --- 2. Distillation Toolbox & "Super Analyzer" Team Fellows: Joshua Levy, Matt Brooks, Ben S, Alejandro This group is building a modular, open-source toolbox for complex research and analysis. • What Nuno should do: Be the Flagship "Power User": Use Sentinel's high-stakes data pipeline to stress-test their modules for citation checking and claim extraction. Integrate his Tools: Contribute his CLI tools (e.g., INLINECODE1) to the shared toolbox to ensure interoperability. Drive the "Super Analyzer": Partner with Matt to combine outputs from multiple deep research AIs, using geopolitical analysis as the core use case. • Why: This grounds their tools in a real-world, high-stakes application. In return, Nuno upgrades Sentinel with advanced analysis modules he doesn't have to build himself. --- 3. Epistemic Virtue Evals Group Fellows: Luke Hewitt, Alejandro Botas, Ben West & Owen Cotton-Barratt. This group is creating novel evaluations to measure AI truthfulness, bias, and consistency. • What Nuno should do: Provide a Real-World Benchmark Dataset: Offer Sentinel's corpus of news—from reliable sources to state propaganda—to test their evals against adversarial data. Systematically Apply the "Falseometer": Integrate the tool from Ben and Owen to automatically score the reliability of his information sources. Test Summarization Virtue: Use Alejandro's evals to check his AI-generated summaries for bias and motivated reasoning. • Why:_ It connects their abstract evals to real-world propaganda and misinformation. This gives Nuno a rigorous, quantitative method to vet his sources and AI outputs, directly improving his risk-tracking mission.
  └ Nuno S: Ok this is amazing
  └ Alejandro B: on 3) asking models to assess internal coherence, contingency, external validation ('fact checking') over a set of claims yields some signal. So from the slackdump you could extract a set of N most salient/central claims on {pickatopic}. then use a coherence-o-meter/false-o-meter to measure those things
  └ Alejandro B: i.e. something like this https://ai4hoped.streamlit.app/ but on all the fellowship context


===== Sat, Aug 16, 2025 =====

--- #distillation ---
  └ [ME] Matt B: ohhh this is actually sick

--- #epistemic-evals ---
[ME] Matt B: https://eqbench.com/spiral-bench.html (https://eqbench.com/spiral-bench.html) Click expand details to see all categories
  └ [ME] Matt B: https://x.com/scaling01/status/1956371713949655328?s=19 (https://x.com/scaling01/status/1956371713949655328?s=19)

--- #epistemics-and-ai ---
  └ [ME] Matt B: oh interesting... yeah my current thought was the Bluesky was actively dying...? But maybe it's not? --- Yeah, -11% daily users over 3 months is not great... "Bluesky has 36m accounts created in total" - not really the right measure to be checking platform health https://bluefacts.app/bluesky-user-growth?t=3m --- That being said I love the frame of "Active knowledge creation over passive consumption" Ah, they are TPOT adjacent as well: "One direction we’re particularly excited about is a collaboration with Community Archive"

--- #lab-notes-matt ---
  └ [ME] Matt B: yeah..... that would be interesting, like which topics are most internally coherent? What if I give you access to my database and you pull whatever data you need? I have the Slack messages and the Drive docs chunked I'll DM you the Postgres string
  └ [ME] Matt B: I already have too many ideas to hack on, lollll, I can't add another to the plate just yet, but there is definitely something cool here and I'm interested

--- #sense-making ---
[ME] Matt B: I had a cool call with Rob today (video and transcript here (https://grain.com/share/recording/8b45830e-bd1d-4d8a-9ede-c6c7b709122f/tWlR9XvdEChg4EoVeg0OoV88EMKMboOSdikpZ2av)), I figured we should create a public sense making channel so other people can chime in and the bots can crawl our thoughts: TL;DR • Aligned to explore a multi-agent system that organizes Fellowship Slack + Docs into something navigable/useful. • Use existing infra (nightly Slack ingest + embeddings; pre-chunked/embedded Google Drive) as the data backbone. • Near-term: Rob will run some simple experiments on tagging/organization, Matt is going to try and finish Matt-GPT so he can try an agent-based crawling system Things we talked about • Infra already in place (From Matt/Nikki) ◦ Nightly “town crier” crawl of public Slack → Matt stores all messages, processes + embeds; Drive is also pre-chunked/embedded. ◦ V1 Fellowship Codex (https://aiforhumanreasoning.slack.com/archives/C097X9QTMK6/p1755048000614019) app does semantic search + summarization over Slack/Docs. • Rob’s related interests ◦ Building a TypeScript agent-based modeling framework; looking for the right use case. ◦ Wants to try multi-agent organization: orchestrator + specialist agents (tagging, novelty/patterns, sanitization/outlining) over Fellowship data. • “Matt GPT” direction ◦ Massive personal corpus (texts since 2017, Slack, tweets, preference docs). Unsure of best fine-tuning framing (threading, context windows). ◦ Idea: use a personal agent as a tool-augmented memory that can read Fellowship content and surface truly “Matt-like” signals; even run agent-to-agent long-form conversations (Matt-GPT [leftrightarrow] Rob-GPT). • MVP directions we riffed on ◦ Self-organizing wiki / Obsidian vault for the Fellowship: third-person, neutral “attn.-style” pages; heavy tagging, folder structure, cross-linking. ◦ Tagging & rubrics: schema for bottlenecks, coordination clusters, “AI unlocks,” asymmetry risks, etc., to power fast navigation and better sense-making. ◦ Brainstorm navigator: start from an interest → show diverging paths, people, fresh threads; improve “offers/asks” and missed-connections suggestions. ◦ Coordination/decision Slack bot: helps teams decide, logs decisions → milestones/tasks, tracks progress from Slack/GitHub, nudges based on org context. • Reality check - End product/value still fuzzy; want something concretely useful for fellows vs. a cool research toy. --- If anyone has ideas on what cool analysis, tools, outputs, etc. we could build with all of the fellowship data (or any large unstructured data), let us know!
[ME] Matt B: Adding some people in this channel because Gemini told me to (reasons in this thread, feel free to leave/mute this channel if not relevant)
  └ [ME] Matt B: High-level summary for each person who would be interested in the channel: • Gordon Brander: He is building "Deep Future," an AI-powered scenario planning tool designed to help users make sense of complex futures and improve strategic thinking. • Nuno Sempere: He builds and uses AI tools to parse millions of news articles and other signals to identify and make sense of emerging global risks. • Joshua Levy: He is leading the "Distillation Toolbox" project and building tools to automate the entire sense-making pipeline, from data extraction to publishing verified insights. • Matthew Brooks: He is a core contributor to the "Distillation Toolbox" and has built tools to scrape and semantically search all fellowship communications to surface valuable connections. • Jamie Joyce: Her organization, Society Library, is dedicated to creating tools like Deliberation Maps that make sense of large-scale, complex socio-political debates. • Steven Isley: He is building a tool to help government regulators make sense of thousands of public comments by automatically analyzing and synthesizing the feedback. • Alexander van Grootel: He is developing a simulation engine that runs thousands of geopolitical scenarios to help decision-makers make sense of complex, uncertain futures. • Herbie Bradley: He is focused on improving the "epistemic commons" by building tools like "Community Notes for Everything" and making information more digestible for AI agents. • Niki Dupuis: She builds meta-level sense-making tools for the fellowship itself, including an interactive project map and a bot that summarizes everyone's daily work. • Oly Sourbut: He consistently applies a systems-thinking approach to make sense of complex group dynamics, from internal fellowship data to broader political polarization. • Tamera Lanham: She is building a software platform for tabletop exercises, a key activity for making sense of complex scenarios and strategic interactions. • Kai Sandbrink: He uses Agent-Based Modeling to simulate complex social and economic systems, making sense of their emergent dynamics and potential outcomes. • Rob Gordon: He is a prolific builder of practical sense-making tools, including a generative agent modeling library and bots for improving Community Notes. • Nathan Young: He builds AI agents for forecasting and writing Community Notes, focusing on turning unstructured public data into valuable, synthesized insights.
[ME] Matt B: @Rob Gordon what if we used matt-gpt (and your agent simulation stuff) to simulate different Twitter users and see how their feed adjusts over time Like what if we built a simulated agent for each fellowship cluster, it builds a curated Twitter algorithm by liking and retweeting stuff that aligns with its preferences, etc.
  └ [ME] Matt B: https://x.com/elonmusk/status/1956580440136966203?s=19 (https://x.com/elonmusk/status/1956580440136966203?s=19) Like there is already a ton of great content on Twitter and they are likely going to put in a lot of effort of trying to improve the algo But if you build a good agent for you, it can scroll 10x more than you, and have more disciplined discernment


===== Sun, Aug 17, 2025 =====

--- #sense-making ---
[ME] Matt B: @Rob Gordon What if we used all of the Simpsons episodes transcripts to fine-tune/create simulated Homer, Marge, Lisa, and Bart agents You could come up with fun tests to explore multi agent dynamics (maybe @Kai Sandbrink would be interested in something like this?) • Agent to agent debates or something (what could Lisa convince Marge of, believably) • When a new Simpson episode comes out (so it's not in the training data), see if you can seed the multi-agent system with the start of the episode premise and see if it runs 100 times if the new IRL episode plays out ◦ I guess this is pretty hard/unlikely because of lot of story lines rely on new/small side characters ◦ Or just see if you can come up with fun/interesting/enjoyable episodes with an automated system ▪︎ insert new/interesting characters, insert crazy world events, or something • You could post some of the outcomes on Reddit or other fan forums to see if they might the outputs believable. Presumable there are Simpson-addicts that could tell if "Lisa could actually convince Marge of that, in-universe" • Risk of Disney getting pissed though... prob have to keep it on the downlow


===== Mon, Aug 18, 2025 =====

--- #epistemics-and-ai ---
[ME] Matt B: We had a breakout session today led by Alejandro regarding his internal coherence measurement app: https://ai4hoped.streamlit.app/ video / transcript here (https://grain.com/share/recording/457cdd94-c9ea-48b5-aa00-35c56a4ef3a7/gwGsZ9MzCYAngKAmY1gGPg9lYnQMItBybcDnMXal) (summary in the thread) Doc here (https://docs.google.com/document/d/1mp0eR4MqWXYClU6lZZXAEJ-BJRubpQu50JzEeyNeZe8/edit?tab=t.ada99tfae4j9)
  └ [ME] Matt B: Internal Coherence Tool Development and Applications • Alejandro Botas presented a Streamlit application designed to measure internal coherence of claims within and across documents. ◦ The tool utilizes a claim extractor to identify the 10 most salient claims from a given text. ◦ It performs generic fact-checking to assess truthiness or falsiness of claims. ◦ A novel feature measures internal coherence by calculating pairwise conditional probabilities between claims, estimating how the truth of one claim affects the likelihood of another. ◦ Initial tests demonstrated that claims within a single document exhibit higher internal coherence compared to claims across different documents. ◦ The visualization displays load-bearing claims (those significantly affecting the probability of other claims) through a diamond-shaped figure. ◦ The tool can aggregate coherence metrics by document, indicating the average load-bearing impact of claims within a document. ◦ Alejandro Botas hypothesizes this approach is underexplored compared to traditional fact-checking and has broader applicability. ◦ Acknowledged that while individual claim data quality might not be high, aggregating signals across large datasets is expected to yield more reliable insights. Discussion on Measuring Coherence and Epistemic Virtues • Owen Cotton-Barratt questioned the choice of conditional probabilities as the definitive measure for internal coherence. ◦ Alejandro Botas stated the choice was primarily due to ease of measurement, acknowledging it might "oversell" the concept of internal coherence. • Paul de Font-Reaulx views the tool as a measure of consistency between claims. ◦ Noted that high internal coherence is not always desirable (e.g., conspiracy theories). ◦ Suggested an interesting use case: measuring the coherence of a claim against an external, authoritative source. ◦ Mentioned exploring theorems like "Dutch books" to quantify coherence based on probabilities. • Joshua Levy is also working on claim extraction and proposed two levels of granularity: ◦ Top-level key claims for a document. ◦ More granular, specific claims for detailed consistency checking. ◦ Suggested pooling and comparing granular assertions across different blog posts or documents. ◦ Proposed mapping claims to document parts using embeddings and a UI/UX idea for mouse-over interactions to reveal associated assertions. • Owen Cotton-Barratt suggested exploring very granular claim extraction, potentially generating more claims than the original text. • Joshua Levy added that consistency can be viewed as a spectrum: ◦ Internal consistency within a document. ◦ Consistency of a document with its own citations. ◦ Consistency of a document with external citations found through searching. • Joshua Levy also highlighted other dimensions of "rigor" beyond logical consistency: ◦ Completeness of arguments (absence of gaps). ◦ Clarity and well-defined nature of the claims themselves. • Kathleen Finlinson proposed brainstorming other epistemic virtues to measure and prioritizing them, potentially creating two lists: underlying virtues and practically measurable aspects. ◦ Paul de Font-Reaulx plans to rework his document on epistemic virtues to cluster ideas based on discussions. Potential Product Ideas and Applications • LessWrong/EA Forum Integration: Matt Brooks suggested targeting platforms like LessWrong or EA Forum, where users prioritize internal coherence. ◦ The tool could analyze a user's posts to identify internal inconsistencies. • AI Editor: Alejandro Botas considered an AI editor for writers (Substack, op-ed, journalists) to review documents for coherence. • News and Science Analysis: The tool could be run daily over news or scientific publications to create a "secondhand news site" or analyze scientific rigor. • Personal Knowledge Database Integration: Ben Sklaroff suggested using a user's personal set of trusted links as a basis for fact-checking other content. ◦ This could function as a "fuzzy fact-checker" or an automated citation generator based on a user's trusted sources. ◦ It could also identify contradictory opinions to broaden a user's worldview. • Prediction Market Analysis: Kathleen Finlinson proposed using the tool to analyze commentary on prediction market websites, extracting claims and assessing if they logically support the predictions made. • Society Library Automation: Jamie Joyce expressed interest in integrating the tool into the Society Library's process for automating broad logical relationships (refuting or supporting arguments) across different positions, which is currently done manually. ◦ Jamie Joyce offered to test the tool with existing maps and provide feedback on refining prompts for hyper-targeted LLM performance.

--- #lab-notes-matt ---
[ME] Matt B: I'm trying to finish up my Matt-GPT fine-tune but it seems like there's a lot of little decisions I have to make, so any advice would be very appreciated! I've done some basic fine-tuning in the past but this use-case seems like it might be finicky Here are my current steps / ideas: 1. Ingest & normalize – Load texts/Slack, keep real user messages, resolve mentions/links, normalize whitespace, and filter to 2021+ 2. Threading / grouping – Detect Slack threads + inline one-turn pairs; group texts by day → thread (and Slack by channel → thread); sort chronologically a. So I'm creating one training example from each Slack/text thread per day (is this smart? Any other ways to do it? Idk how to ensure a two message single turn would be guaranteed to be semantically linked) 3. Conversation building – Map Matt → assistant, anyone else → user; merge consecutive same-role turns; ensure conversations end with assistant 4. Noise & shaping – Drop OTP/alerts/emoji reactions/links-only, dedupe repeats, and prefer user-starts with assistant-ends a. Question: How should I handle training examples that the assistant would start first. Like a Slack thread that I started? Add a placeholder initial input message from the user? 5. Convert to fine-tuning training format 6. Final QC – Validate JSONL (parsable, roles valid, non-empty content, Unicode clean, token/length caps, no near-duplicate examples) @Luke Hewitt I know you mentioned fine-tuning models before. And @Jamie Joyce is there any way you can ask your dev if he has input? I remember you saying you train a bunch of fine-tuned models for different parts of your stack.
  └ [ME] Matt B: @Ben West @Ben Sklaroff @Alejandro Botas @Martin Listwan Gemini think you 4 might be able to give me useful fine-tuning advice as well
  └ [ME] Matt B: Oh, also I want to give the final LLM a tool to let it fetch my preferences Like if a conversation is about religion, use the tool to find and pull in the file for my religious preferences. That part is easy, but if I fine-tune on like 5,000 text + Slack messages, will that effect the tool use? Or would I try to somehow make examples of the tool use for fine-tuning? Also, I think I want to give the final LLM like 20k tokens at the start of each conversation (my high level preferences, personality, etc.) once I have the fine-tuned version Do I need to include this for each fine-tune example? (would be too expensive I think). Or should I put like a 2k token higher-level summary / distilled version of my personal overview for each example? Or is it not needed at all?
  └ Ben S: I haven't fine-tuned LLMs before -- Ivan from Midjourney would probably be a good person to talk to about this
  └ Luke H: Ya I have indeed fine-tuned various LLMs and happy to help. A few initial thoughts? > Question: How should I handle training examples that the assistant would start first. Like a Slack thread that I started? Add a placeholder initial input message from the user? This won't be a very consequential decision. What you suggest sounds fine, just do something something consistent (like "[assistant sends first message]") (Depending on how you're finetuning it might also work to just not include any initial user message) > Conversation building – Map Matt → assistant, anyone else → user; merge consecutive same-role turns; ensure conversations end with assistant I think this doesn't matter to you, but not that this doesn't give you a model of whether Matt responds or not? > Oh, also I want to give the final LLM a tool to let it fetch my preferences Hmmm, worth discussing this in more depth. Things get much more complicated if what you want to do is (reinforcement) finetuning to optimize the tool use itself. But you probably don't need to do that?
  └ [ME] Matt B: @Ben Sklaroff Great idea, someone mentioned Midjourney created a CEO-copy fine-tuned agent. I messaged Ivan on twitter but idk if he sees that. Do you happen to have a way to contact him? This is the DM I sent him: > Hey Ivan! I'm part of the FLF Fellowship - "AI for Human Reasoning" > > I'm trying to build an agent fine-tuned on my messages + tons of context around my preferences/personality to see if it can be a partial drop-in agent for: > - ai-assisted negotiation/decision-making/mediation > - agent based simulations/debates > - recommender system interface > - etc. > I believe someone said in the fellowship that Midjourney created an LLM to think/talk like the CEO, and was wondering if you had any recommendations on repos/blogs/papers that I should look into for inspiration > I have my text messages, Slack messages, emails, twitter data, etc. I can use
  └ Ben S: I'll message him on signal
  └ [ME] Matt B: @Luke Hewitt thanks for your thoughts. Do you happen to be free tomorrow at 3:30 PM ET? (and not sick enough to hop on a call?) Rob and I scheduled a chat to talk about agents/simulations/sense-making stuff and if you joined the call I'm sure we could pepper you with fine-tuning questions, lol
  └ Luke H: > Also, I think I want to give the final LLM like 20k tokens at the start of each conversation (my high level preferences, personality, etc.) once I have the fine-tuned version Do I need to include this for each fine-tune example? (would be too expensive I think). Or should I put like a 2k token higher-level summary / distilled version of my personal overview for each example? Or is it not needed at all? I think sticking a very brief summary of everything into the system prompt for each conversation sounds good. You could also (instead of tool use) just use gemini (or your favorite vector search or whatever) to pull out relevant chunks of the 20k context after each user message and feed that in (and do the same at test time)
  └ Ben W: My main suggestion is to have an eval that tells you how good whatever intervention you are taking is. This could be an rlhf style thing where you just say which of two outputs is better. But otherwise it can be hard to tell if the thing you are doing is actually working And to that point, how confident are you that you need to fine tune? In context learning is often pretty good, particularly if you are just talking about slack threads that are a couple hundred tokens long
  └ [ME] Matt B: > have an eval that tells you how good whatever intervention you are taking is okay, @Ben West - noted, I didn't want to do/think about that yet, but it does seem very important > how confident are you that you need to fine tune Are you saying for every interaction just try to give the LLM the relevant data it needs from my personal context to respond? Meaning give the agent chunks of texts, chunks of Slack messages, chunks of personality docs, etc? --- I was thinking fine-tuning on like 5k messages would somehow give the model some more tacit knowledge about how I tend to reply to things (and how I think?) that couldn't be captured in the more formal information I'm giving it. But maybe that's just not true? 5k messages doesn't actually sound like that much now, and probably 80% are relatively low value...
  └ Luke H: [100] to having an actual eval Also re: value of finetuning - I do kinda agree that in-context learning is a strong and very easy starting point here, and so very plausibly it's worth just doing that first and evaluating it. But, there are some things I'd expect it to be worse at than a finetuned model (e.g. talking about contentious issues like politics/vaccines/whatever where the model has been safety-tuned not to say bad things). Also just depending on the volume, you might have more text than you'd realistically want to throw into the system prompt every time.
  └ Martin C: I don't think I have all of the context as to what Matt-GPT is but I'd definitely +1 on having an eval. If I personally had to build this myself, I would build the eval first and then try to do prompt fine tuning before actually fine tuning the LLM (So using something like DSPY which can double as an eval). From my perspective, it's sometimes hard to judge if a fine-tuned LLM is better on vibes alone.
  └ Martin C: Although, the above is just my generic advice since I dont quite understand what we're trying to do
  └ Ben W: > Are you saying for every interaction just try to give the LLM the relevant data it needs from my personal context to respond Yeah just like in the system prompt be like "try to match the tone of these slack messages: {message 1} {message 2} {message 3}".
  └ Ben W: Also I'm not sure what you mean by "relevant data" but if you are trying to teach new facts (rather than a tone/style thing) then fine tuning usually doesn't work that well (because the updates to the weights are too constrained). Usually people ideally do ICL, and if that doesn't work then RAG.
  └ [ME] Matt B: No, not new facts. More like "In this type of thread, Matt generally replies with this nuanced tone/type of reply, etc." But now that I think about it more, I think that "nuanced tone" thing likely doesn't matter much for the use cases I want to try to use my Matt-GPT for (automated participation in DecisionMate, etc.) I think I'll play with ICL + RAG first. Probably test out DSPY (never tried) to help me iterate and evaluate Thank you all for your input, I really do appreciate it [pray] [pray] [pray]

--- #pivotal-group ---
[ME] Matt B: lol, misaligned
[ME] Matt B: (still very happy to test anything, and opt-in for bot messages)

--- #sense-making ---
Rob G: Haha, I love the Simpsons idea. I wonder if it’s possible to fine-tune directly from dialogue (or tweets) like that, I’ve never tried it! Could be fun to play around with. Over the weekend I was actually thinking about a tool that could help fellows choose concrete projects by leaning on the FLF Codex (maybe search via an AP routeI?). Feels like it could be a way to connect the agent/facilitation stuff back to the fellowship questions more directly. Let's find a time this week to catch up about this stuff!
  └ [ME] Matt B: I honestly have no idea what the best practices are for fine-tuning... (I'm asking for help with Matt-GPT in my lab notes channel here (https://aiforhumanreasoning.slack.com/archives/C0984LYL5T8/p1755527266820909), lol) I slightly improved the FLF Codex over the weekend. It's still pretty crappy and I think most people aren't going to get much value out of it. But if I expose all of the data / flows via an API (and you help me improve the code and/or prompts), we could probably do something cool > Feels like it could be a way to connect the agent/facilitation stuff back to the fellowship questions more directly Yeah, I think so too. I also briefly started on the rubric idea > Let's find a time this week to catch up about this stuff! Are you free around 3:30 PM ET on Tuesday? I know that's pretty soon but I'm at a lake house with the family so I'm trying to group my calls together
  └ Rob G: 3:30 works [+1]

--- #small-group-decision-accelerator ---
  └ Luke H: Ya Interesting. I guess I agree that persuasion might not be the first order failure mode here, but in terms of validation it does seem like it would be very important to provide evidence that the tool leads groups to come to "good" decisions (e.g the same decision they would have come to otherwise over a longer period of time, which is related to what we're doing in <#C098NB4KQNR|> )


===== Tue, Aug 19, 2025 =====

--- #coordination-and-ai ---
Oly S: Looks like the Meaning Alignment Institute (https://meaningalignment.substack.com/p/introducing-full-stack-alignment) folks might be interested in a visiting talk/discussion. I assume from a bit of Slack interest that there'd be appetite for that if it worked out? My guess is a 10-20 min talk from them followed by questions and open-ended discussion would be the best format. I'll moot that with them and we can always adjust. @Matt Brooks @Tamera Lanham @Niki Dupuis @Gordon Brander @Paul de Font-Reaulx @Ben Goldhaber, others
  └ Gordon B: Meaning Alignment Institute giving a talk at FLF, or FLF fellows giving talks at Meaning Alignment Institute?
  └ Oly S: The first one!
  └ Gordon B: Very interested in both, but even more interested in the first one at the moment, because I won't have to prepare a talk [laughing]
  └ Paul F: Yea totally down for this
  └ [ME] Matt B: I would also love to attend. I would be very curious to understand: • Their ideas about their counterfactual impact (will it give me ideas for how to frame my own impact?) • The roadblocks/pitfalls they have seen or run into themselves that FLF fellows are likely to also hit • Which ideas/areas do they think are most underserved/under researched and have the most potential for large value • etc.
  └ Oly S: Fab. Any other prompts for me to pass on?
  └ Paul F: No, I'd personally be interested in hearing about the details of their full-stack alignment ideas, but can also follow up with them myself if it's not of equal interest to others
  └ [ME] Matt B: Other prompt ideas: • What are your key learnings on bridging ambitious, long-term goals with the immediate need to build products that users will actually adopt and get value from? • Do you have any thoughts on asymmetry risk with projects in this space? • We are all building tools that aim to guide user decisions. What have you learned about the design challenge of steering users toward "wiser" outcomes without undermining their sense of agency or creating frustrating user experiences? • What is the single most important unsolved problem or missing "coordination primitive" that you believe a small, agile team from our fellowship could make meaningful progress on in the coming months? --- But I want them to decide how they best think they can help us. With ideas, suggestions, historic examples, etc. etc.
  └ Gordon B: I would love to hear them speak to the Market Intermediaries idea. Curious how this might be compared to the bloc phenomena in young democracies, and also how we might avoid the feedback failures of central planning (another normatively driven response to markets that didn't exactly work out) https://meaningalignment.substack.com/p/market-intermediaries-a-post-agi
  └ [ME] Matt B: Might make sense to have a 40 min talk with 20 mins of Q&A? If there's enough demand and topics
  └ Tamera L: I’m interested in anything they want to talk about haha. Just wanted to express my general excitement to hear a talk from them
  └ Oly S: Grand! I'll bring a summary of these prompts to them and see when they can do. Balancing talk vs Q&A/discussion is always hard. I tend to favour Q&A for 'getting to the point', but some framing talk is usually needed to set the scene. Q&A can shade into basically 'them talk more' if that's the way the questions point! Or into freer discussion, if that's looking promising.
  └ Oly S: Gordon > the bloc phenomena in young democracies Do you mean like... things get sectarian, rigid, polarised? I'm not sure what this is.
  └ Gordon B: I don't have a great name for this, but Fukuyama discusses it in Political Order and Political Decay. "State of nature" political order mostly grows around kin selection. Modern states have various mechanisms for restructuring this. However, young democracies often find themselves in a halfway state. Votes tend to move in blocs along tribal/sectarian/ethnic lines, rather than for/against discrete issues. Electoral wins are co-opted to disburse handouts to the ingroup. At the extreme this amounts to effectively buying votes. Tammany Hall is a historical example of this phenomena in the US, but there are many others. https://en.wikipedia.org/wiki/PoliticalOrderandPoliticalDecay
  └ Gordon B: Given a market intermediary is a sort of AI-mediated negotiating bloc, I wonder whether it would be susceptible to the same issues, and if so, how to mitigate.

--- #epistemic-evals ---
  └ Kathleen F: very interesting find!
  └ [ME] Matt B: I need Matt-GPT to be crawling twitter 24/7 to surface more finds like this...

--- #fellowship-water-cooler ---
[ME] Matt B: Anyone getting API Error (Request timed out.) for Claude Code? I'm on Windows Started happening yesterday a bunch. It's intermittent Maybe it's not actually an API error, but a tool error? https://x.com/joedevon/status/1957568848280056070
  └ Alex B: I just ran into this. Not using any MCPs. > ⎿ API Error (Request timed out.) · Retrying in 1 seconds… (attempt 1/10) > ⎿ API Error (Request timed out.) · Retrying in 1 seconds… (attempt 2/10) > ⎿ API Error (Request timed out.) · Retrying in 2 seconds… (attempt 3/10) > ⎿ API Error (Request timed out.) · Retrying in 5 seconds… (attempt 4/10) > ⎿ API Error (Request timed out.) · Retrying in 8 seconds… (attempt 5/10) > ⎿ API Error (Request timed out.) · Retrying in 17 seconds… (attempt 6/10) > ⎿ API Error (Request timed out.) · Retrying in 39 seconds… (attempt 7/10) > ⎿ API Error (Request timed out.) · Retrying in 33 seconds… (attempt 8/10) > ⎿ API Error (Request timed out.) · Retrying in 36 seconds… (attempt 9/10) > ⎿ API Error (Request timed out.) · Retrying in 39 seconds… (attempt 10/10) > ⎿ API Error: Connection error.
  └ [ME] Matt B: Someone on the GitHub issue said to try npm install -g @anthropic-ai/claude-code@1.0.80 So I did that I restarted my comp And I switched to Sonnet (I think Opus was hitting this more) I also have some huge files in this repo, so maybe the tool-use was failing? I told Claude to specifically not open any files in "raw_data" folder. To only read the first 200 lines if necessary I'm still hitting some API timeouts but much much fewer now. And it's not getting totally frozen/stuck anymore
  └ Alex B: I was getting the error above for every request. I restarted my machine and at least my first request worked. Will update if I obtain other insights.

--- #lab-notes-matt ---
[ME] Matt B: Had a good chat with Timothy today (summary in thread)
  └ [ME] Matt B: Fellowship Experience and Challenges • Expresses gratitude and appreciation for the intellectual space and opportunities within the fellowship. • Feels nervousness about transitioning from the "explore" to the "build" phase, citing the vastness of possible ideas and potential pitfalls. • Struggles to distill complex, nebulous ideas into actionable, operational projects. • Worries about the "counterfactual impact" and "distribution plan" for project ideas, identifying these as the most challenging and unknown hurdles. ◦ Believes the ability to code and implement AI tools is not a concern. ◦ Needs to believe the project's story and its potential for real-world impact and traction. Collaboration and Teammates • Considers teammates important and valuable, potentially indispensable for a project's success. • Expresses nervousness about long-term co-founder commitments based on limited interaction (e.g., 5 hours of discussion) within the fellowship. ◦ Notes that in a prior startup, a co-founder was indispensable, but they had known each other for many months as coworkers. • Believes the ideal scenario is to iterate with a person during the fellowship and then scale the project together, potentially seeking funding. • Acknowledges that funders require clarity on the organization's structure and commitment level before providing funds. • Suggests that finding teammates for the fellowship should be viewed primarily for the fellowship's duration, with the understanding that paths might diverge post-fellowship. • Recognizes that due to the fellowship's size (33 people) and focused fields, not every person may find a perfect long-term co-founder match. • Emphasizes the importance of not overthinking the "perfect fit" for a co-founder, given potential time constraints. Proposed Improvements for Transition to Build Phase • Structured Ideation and Commitment: • ◦ Suggests breakout groups to help fellows cohere around ideas and commit to a 2-week MVP. ▪︎ This could facilitate quicker commitment without feeling locked into a long-term project. • • Proposes an alternative: a 1 to 1.5-week MVP, potentially working on two MVPs simultaneously (half-time on each). ◦ This approach allows for continued exploration while bounding time and assessing team dynamics. • Tools for Project Selection: ◦ ▪︎ Is collaborating with Rob on rubrics and a "habermas machine" to help fellows self-interrogate project ideas. ◦ The tool aims to guide brainstorming by asking about clusters, use cases, users, pain points, and impact. ◦ The tool will prompt users and offer options to help refine ideas. • Working Groups and Micro-Groups: • ◦ Suggests working groups clustered by topic area or tool type. ◦ These clusters should lead to micro-groups (2-4 people) working on 1-week MVP sprints. ◦ Believes this hands-on collaboration helps assess personal work styles, communication, and priorities, which is crucial for co-founder potential. ◦ Proposes a "mini demo day" after these sprints to provide a semi-formal end to the "explore" phase and generate excitement. Project Ideas ◦ Generalized Knowledge Management Platform: ▪︎ Discussed with Ben S, Alejandro, and Josh. ◦ Simple version: Users drop 10 links, AI analyzes, buckets, embeds, and finds 10 more links, or provides insights about the user or links. ◦ Complex version: Could evolve into a group Obsidian-like system with agents automatically tending to information, finding truthful content, and identifying worldviews. ◦ Could lead to automated matching or an agent representing a user's views for negotiation or deliberation. ◦ Acknowledges the idea is currently too vague due to many possibilities. • Super Personal Context Builder / "Matt GPT": ◦ Has already built a hacky version for personal use, pulling text, Slack messages, and personality tests to create a corpus of personal preferences, ideas, and values. ◦ Believes this could be valuable for matching, decision-making, deliberation, negotiation, coordination, communication, and searching. ◦ Is currently working on "Matt GPT," a version of himself with all his information that can speak on his behalf. ◦ Potential applications include plugging into Decision Mate for debate or summarizing social media feeds. ◦ Unsure about the real-world impact story for this project. Next Steps • Matt will: ◦ Write down considerations for each project idea, including distribution, counterfactual impact, and teammate compatibility. ◦ Try out Rob's "habermas machine" tool once it is live. ◦ Consider implementing a 1-week MVP sprint with a small group (e.g., Ben S, Alejandro, Josh) for the generalized knowledge management platform idea. ◦ Post the call link and transcript in the lab notes channel.

--- #sense-making ---
[ME] Matt B: Rob and I had another (great) chat today Recording, transcript, and detailed summary here (https://grain.com/share/recording/7b5a33d5-43dd-4ba1-ac7b-270ff2d031e3/PFbxVOzy2YK8RUukNlVFo2UK17DaSoDZZUQhPcaV) • Rob’s project-chooser tool: CLI “personal Habermas machine” that walks users through dimensions (target user, epistemic goal, hair-on-fire problem, signal gathering, connection pathway, technical shape). Plans to integrate the Fellowship Codex vector search; might become a web app. • Matt’s evaluation flow: Four rubrics (coordination layer fit, bottlenecks addressed, failure modes/“why goodness doesn’t spread,” and “abundant intelligence” leverage). Process = auto-gather context → 30-min AI pre-brief → 1-hr human call → feed transcript back to AI for a structured report. ◦ Matt's chat with Claude (https://claude.ai/share/b3de3ce0-70c0-4e31-bad1-8f22b6379d18) about the rubrics, project analysis flow, etc. • Collab thread: Matt will share rubrics; Rob will try plugging them into the chooser. Use Matt’s search API (and possibly a new “extended search” endpoint) to fetch hits plus nearby Slack/thread and doc context for better summaries. • Rob's Product idea: Multi-agent “reverse DecisionMate” / collective red-teaming where specialized agents (e.g., distribution) critique user statements; could run on live text or call transcripts. • Fellowship angle: Run Rob’s tool across fellows to map interests/bottlenecks; optional export to CSV and opt-in shared DB for clustering and auto-suggested breakouts. Leadership (Timothy/Ben/Kathleen) likely interested during the explore→build transition. • MattGBT update: No fine-tuning; instead heavy context + vector pulls (texts/Slack/docs) to simulate Matt. Separate outreach to Xiq/Community Archive; Decisions & next steps • Rob: integrate vector search, get the CLI online, send link/code to Matt; meet Timothy tomorrow. • Matt: share rubrics + Claude chat; try adding an “extended search” API that returns surrounding context • Both: test with a few fellows; consider looping in leadership after initial feedback; reconnect later this week.


===== Wed, Aug 20, 2025 =====

--- #ea-forum-sensemaking-mvp ---
[ME] Matt B: Had a great call with Rob, Alejandro, Alex, Blake, and Kathleen about doing a ~1 week MVP sprint (recording and transcript here (https://grain.com/share/recording/ac152b3c-ead0-46e0-8c5d-3acb2dd51d68/CZJm056X7dvDt6MqjE9xorA68EaOZTWYATPpZTkE)) (rough doc here (https://docs.google.com/document/d/1cpStkDQa2QsK9AwV1ekuGvq5Ai9hAbo5cFQYzV5UTK4/edit?usp=sharing)) I created a new channel to coordinate on the project, anyone is free to join the channel and put their thoughts or even help out! <#C09C3T2R1UG|> The rough idea is: 1. I'll scrape all posts and comments from the EA forum starting Jan 1 2025 2. We'll agree on an initial smallish subset of posts to analyze a. this is TBD - open to suggestions 3. We all hack around on the data to find interesting stuff a. any and all type of analysis, but generally in the sensemaking / epistemic eval area 4. We build a website / leaderboard / EA forum post to announce our findings/process/thinking 5. We nerd snipe EA into giving us better ideas for future work (please add more details / info in the thread if I missed something or misunderstood something) --- I added Paul, Luke, and Ben S (feel free to ignore / leave) • Paul because we're thinking about your doc on epistemic categories to see if we can come up with interest analysis • Luke because we might use your claim extraction tool from your paper / you might be interested • Ben S because you've been interested in distillation / sensemaking stuff
[ME] Matt B: --- Alejandro had a great domain name when we were considering the Less Wrong forum: areyoulesswrong.com (http://areyoulesswrong.com) I thought of whoislesswrong.com (http://whoislesswrong.com) if we were doing a more leaderboard type thing But now that we're doing the EA Forum we need to find another unique domain (or we can just stick to an EA Forum post if we want)
Kathleen F: Thanks Matt! I have some thoughts on the subset of posts to start analyzing [thread]
  └ Kathleen F: I would probably start off with the N highest-rated posts (they’re probably higher quality which will make for more interesting analysis)
  └ Kathleen F: It could potentially be interesting to look at the N highest-rated and M lowest-rated posts for contrast. Low-rated posts might be making controversial claims, or not be very clear in what they’re claiming, or their claims aren’t well-supported. This could make them hard to analyze but could be interesting
  └ [ME] Matt B: Okay cool! You guys can sort by score ascending and descending and take like the top and bottom 100 automatically (is 100 the right amount? More or less?) Then I'll also randomly choose 100 posts and put random_set=true on those 100 rows if we want to also play around with the same random set
Alejandro B: some claim extractors from fellows: • https://github.com/Xodarap/falseometer (Ben W) • prompts in 4.4.4 https://github.com/kobihackenburg/scaling-conversational-AI/blob/main/Supplementary%20Materials.pdf (LukeH) • https://github.com/abotas/oped/blob/main/claim_extractor.py Josh might have one too?
  └ [ME] Matt B: @Joshua Levy do you have a very easy way for people to test your claim extractor? (if you have one) I have yet to test out your https://e2b.dev, unfortunately

--- #epistemic-evals ---
  └ Lukas F: dang i was surprised to see claude (sonnet) almost at the bottom, but then i clicked through to some chats, and, uh, sonnet, yikes

--- #fellowship-water-cooler ---
[ME] Matt B: Good @slimepriestess Thread: > "the majority of the current issues facing the world are ultimately not technological and even in the places they are, the technological hurdles are ultimately solvable with clear roadmaps and short timelines. most of the pressing issues are political and economic." https://x.com/slimepriestess/status/1958245892336853177
  └ Ben S: good except the "structural change" she's proposing seems to be like slightly better neoliberalism (e.g. andrew yang), rather than anything that really challenges existing power structures
  └ [ME] Matt B: Fair enough - do you have a concise way to propose your suggested re-framing? Like how the US founding fathers challenged the UK's existing power structure? What forces do you see brewing now that can viably (and positively) challenge existing power structures?
  └ Ben S: On the national political level, ditching corporate centrism + performative wokeness for real left movements that speak to the working class such as Zohran, Bernie, Jeremy Corbyn, Mélenchon, Lula, etc
  └ Ben S: at the grassroots level, building solidarity across classes of laborers to fight for common interest. from an AI safety lens, this might look like organizing ML researchers to fight for a set of concrete demands, e.g. no autonomous weapons, no predictive policing, etc
  └ [ME] Matt B: > organizing ML researchers to fight for a set of concrete demands I like this idea - especially because they have a lot of power/influence now and that power/influence might rapidly decay as ML because automated I think this is related to the FLF project idea of Critical Number: a threshold signature revealing platform (with the possibility of having a credible commitment component. https://jobs.lever.co/futureof-life/9334cb97-3928-47a4-88e6-c10472b8a6f1

--- #sense-making ---
[ME] Matt B: I scheduled a brainstorming / ideating / MVP planning call at 4:30 ET for this general area of thinking I'll record the call, so if you can't make it, no problem. I invited people that showed interest in the Google Doc, but no pressure to join at all https://meet.google.com/phz-yyge-spu
[ME] Matt B: Had a great call with Rob, Alejandro, Alex, Blake, and Kathleen about doing a ~1 week MVP sprint (recording and transcript here (https://grain.com/share/recording/ac152b3c-ead0-46e0-8c5d-3acb2dd51d68/CZJm056X7dvDt6MqjE9xorA68EaOZTWYATPpZTkE)) (rough doc here (https://docs.google.com/document/d/1cpStkDQa2QsK9AwV1ekuGvq5Ai9hAbo5cFQYzV5UTK4/edit?usp=sharing)) I created a new channel to coordinate on the project, anyone is free to join the channel and put their thoughts or even help out! <#C09C3T2R1UG|> The rough idea is: 1. I'll scrape all posts and comments from the EA forum starting Jan 1 2025 2. We'll agree on an initial smallish subset of posts to analyze a. this is TBD - open to suggestions 3. We all hack around on the data to find interesting stuff a. any and all type of analysis, but generally in the sensemaking / epistemic eval area 4. We build a website / leaderboard / EA forum post to announce our findings/process/thinking 5. We nerd snipe EA into giving us better ideas for future work


===== Thu, Aug 21, 2025 =====

--- #ea-forum-sensemaking-mvp ---
  └ [ME] Matt B: @Jamie Joyce are you able to expose your claim extractor as an API we can use/test?
  └ [ME] Matt B: An interesting side-experiment / eval would be cool to try like 3 claim extractors (with 2 or 3 LLMs each?) and see how they differ? Which is best? Also could be tedious
  └ Jamie J: Sure, but we would prefer drafting agreements about its use (standard practice for us). Would that be ok for you?Also, head down a little bit in rewriting prompts and building a pipeline for an extensive report atm.

</flf_fellowship_slack>